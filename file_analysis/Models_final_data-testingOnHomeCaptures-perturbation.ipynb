{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import keras \n",
    "\n",
    "# import numpy\n",
    "# import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFromMerged=True\n",
    "loadFromIndexes= False\n",
    "Mapper='S'\n",
    "IgnoreEmpty= True\n",
    "FoldID =\"1\"\n",
    "Epoch_count=100\n",
    "Batch_size=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data the old way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data= [] \n",
    "# y_data= [] \n",
    "\n",
    "\n",
    "# with open( '../files/txt/seq_mapping_large.txt' ) as f:\n",
    "#     x_data =   f.readlines()\n",
    "\n",
    "# with open( '../files/txt/command_mapping_large.txt' ) as f:\n",
    "#     y_data = f.readlines()\n",
    "    \n",
    "    \n",
    "# x_data =[ np.array([ int(y) for y in x.strip().split( ' ') ])   for x in  x_data ] \n",
    "# y_data =[ x.strip().split(' ') for x in  y_data ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load The Data The New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mapps the input records to a integer array for the input\n",
    "def mapping_x( inp, includeDirection = False , TrimAt= 15 ):\n",
    "    if includeDirection:\n",
    "        return np.array([ int(x[\"packet_length\"]) * (1 if x['packet_source']=='hub' else -1)  for x in inp ][:15])\n",
    "    else:\n",
    "        return np.array([ int(x[\"packet_length\"])  for x in inp ][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_y_service(inp):\n",
    "    return np.array(  list(set([x[\"event\"] for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_service_event(inp):\n",
    "    return np.array(  list(set([ \"%s-%s\"%( x[\"event\"] ,x[\"val\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_device_service(inp):\n",
    "    return np.array(  list(set([ \"%s & %s\"%( x[\"device\"] ,x[\"event\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_full(inp):\n",
    "    return np.array(  list(set([ \"%s & %s & %s\"%( x[\"device\"] ,x[\"event\"], x['val'] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cleans the data removing emply nodes and turning the nodes into sarrays by calling the mapping function \n",
    "def clean_data( x_data, y_data , removeempty=True, Mapping='S'):\n",
    "    cleans = [] \n",
    "    cleans = (sorted([ x for x in y_data if (removeempty and len(y_data[x]) > 0) or not removeempty  ] ))\n",
    "    \n",
    "    ret_x  = [x_data[x] for x in cleans]\n",
    "    ret_y  = [y_data[x] for x in cleans] \n",
    "    \n",
    "    print( len(y_data), len(cleans) )\n",
    "    \n",
    "    ret_x  = [ mapping_x(x) for x in ret_x ] \n",
    "    ret_y_s = [ mapping_y_service(y) for y in ret_y ]\n",
    "    if Mapping=='S':\n",
    "        ret_y  = [ mapping_y_service(y) for y in ret_y ]\n",
    "    elif Mapping=='SE':\n",
    "        ret_y  = [ mapping_y_service_event(y) for y in ret_y ]\n",
    "    elif Mapping=='DS':\n",
    "        ret_y  = [ mapping_y_device_service(y) for y in ret_y ]\n",
    "    elif Mapping=='F':\n",
    "        ret_y  = [ mapping_y_full(y) for y in ret_y ]\n",
    "    return ret_x, ret_y, ret_y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in load from merged\n",
      "58958 57867\n",
      "loading from test files\n",
      "found files :  4\n",
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57867, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= []\n",
    "y= []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "y_test_service= []\n",
    "\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "test_names = []\n",
    "\n",
    "add_to_trainig = [0,2]\n",
    "\n",
    "if loadFromMerged:\n",
    "    print(\"in load from merged\")\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_hub_segments_final.json'  ) as f:\n",
    "        y_data = json.load(f)\n",
    "\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_pcap_segments_final.json'  ) as f:\n",
    "        x_data = json.load(f)\n",
    "        \n",
    "#     with open(  '../files/train/merged/hub_segments_2.json'  ) as f:\n",
    "#         y_data = json.load(f)\n",
    "\n",
    "#     with open(  '../files/train/merged/pcap_segments_2.json'  ) as f:\n",
    "#         x_data = json.load(f)\n",
    "  \n",
    "    if len( y_data ) != len(x_data) :\n",
    "        print( pick )\n",
    "        \n",
    "    \n",
    "    x_train,y_train, y_train_service= clean_data( x_data, y_data, IgnoreEmpty , Mapping=Mapper )\n",
    "    \n",
    "    #     continue\n",
    "#     if loadFromIndexes:\n",
    "#         print(\"load from indexes\")\n",
    "#         with open(\"../files/train/merged/items_2_test-train_indexes.json\")  as f:\n",
    "#             index_info = json.load(f)\n",
    "\n",
    "\n",
    "#         for i in index_info[FoldID][\"test\"]:\n",
    "#             x_test[str(i)]=(x_data[str(i)] )\n",
    "#             y_test[str(i)]=(y_data[str(i)] )\n",
    "\n",
    "#         for i in index_info[FoldID][\"train\"]:\n",
    "#             x_train[str(i)]=(  x_data[str(i)] )\n",
    "#             y_train[str(i)]=(  y_data[str(i)] )\n",
    "        \n",
    "#         x_test_t,y_test_t= clean_data( x_test, y_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#         x_test.append(x_test_t)\n",
    "#         y_test.append(y_test_t)\n",
    "    #     else :\n",
    "    print(\"loading from test files\")\n",
    "    test_files = sorted(glob.glob( '../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/home*.json' ))\n",
    "    print( \"found files : \" , len(test_files) )\n",
    "    for pick  in test_files:\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/test/test_homes/final_upload/usecases/hub_segments_final_final/', fname) ) as f:\n",
    "            y_data_test = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/', fname) ) as f:\n",
    "            x_data_test = json.load(f)\n",
    "\n",
    "\n",
    "        t_x,t_y, t_z= clean_data( x_data_test, y_data_test, False , Mapping=Mapper )\n",
    "\n",
    "#         if test_files.index(pick) in add_to_trainig:\n",
    "#             x_test_t,y_test_t, y_test_service_t= clean_data( x_data_test, y_data_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#             x_train.extend(x_test_t)\n",
    "#             y_train.extend(y_test_t)\n",
    "#             y_train_service.extend(y_test_service_t)\n",
    "\n",
    "\n",
    "        x_test.append(t_x)\n",
    "        y_test.append(t_y)\n",
    "        y_test_service.append(t_z)\n",
    "            \n",
    "#     x_test = x_data[ index_info[\"1\"][\"test\"]  ]\n",
    "#     y_test = y_data[ index_info[\"1\"][\"test\"]  ]\n",
    "    \n",
    "#     x_train = x_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     y_train = y_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     x.extend(t_x)\n",
    "#     y.extend(t_y)\n",
    "else:\n",
    "    for pick in sorted(glob.glob( '../files/train/hub_segments/*.json' )):\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/hub_segments/', fname) ) as f:\n",
    "            y_data = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/pcap_segments/', fname) ) as f:\n",
    "            x_data = json.load(f)\n",
    "\n",
    "        if len( y_data ) != len(x_data) :\n",
    "            print( pick )\n",
    "            continue\n",
    "\n",
    "        t_x,t_y= clean_data( x_data, y_data, True )\n",
    "\n",
    "        x.extend( t_x)\n",
    "        y.extend(t_y)\n",
    "\n",
    "x= np.array(x)\n",
    "y= np.array(y)\n",
    "\n",
    "# x_train = np.append( x_train, x_test[0] , axis=0)\n",
    "# x_train = np.append( x_train, x_test[2] , axis=0)\n",
    "\n",
    "# y_train = np.append( y_train, y_test[0] , axis=0)\n",
    "# y_train = np.append( y_train, y_test[2] , axis=0)\n",
    "\n",
    "\n",
    "len(x_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def purturb( x_inp , y_inp  ,percentage= 0.5, ignore_empty=False, overlap_count = 1 ):\n",
    "    \n",
    "    if ignore_empty :\n",
    "        indexes= [ i for i in range(len(y_inp)) if  len( x_inp[i] )>overlap_count and  'none' not in  y_inp[i]  and 'unknown'  not in y_inp[i] ]\n",
    "    else :\n",
    "        indexes=  [ i for i in range(len(y_inp)) if  len( x_inp[i] )>overlap_count ]\n",
    "    \n",
    "    assert len(indexes)> overlap_count\n",
    "    \n",
    "    pickable_len =int( len(indexes) * percentage)\n",
    "    if pickable_len %2 != 0 :\n",
    "        pickable_len+=1\n",
    "    \n",
    "    indexes =  random.sample(indexes, pickable_len)\n",
    "    \n",
    "    set_1 = np.array( random.sample( indexes, int(pickable_len/2) ), dtype=int )\n",
    "    indexes = np.array(indexes, dtype=int)\n",
    "    set_2 = np.array(  indexes[ ~np.isin(indexes, set_1) ], dtype=int )\n",
    "    \n",
    "    x_new = list(x_inp) \n",
    "    y_new = list(y_inp)\n",
    "    for i in range(len(set_1)):\n",
    "        a= x_inp[set_1[i]]\n",
    "        b= x_inp[set_2[i]]\n",
    "        a_y =list( y_inp[set_1[i]])\n",
    "        b_y = list(y_inp[set_2[i]])\n",
    "        \n",
    "        ret = [ a[:-1* overlap_count] , b[:overlap_count] ,a[-1*overlap_count:],b[overlap_count:]]\n",
    "        ret  = [  i if type(i)==np.ndarray else np.array([i]) for i in ret ]      \n",
    "        ret =np.array(list(np.concatenate(ret)))\n",
    "#         print(a,b, ret)\n",
    "\n",
    "        x_new.append( ret)\n",
    "        y_new.append( list(set(b_y + a_y))) \n",
    "\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = purturb( x_train, y_train, ignore_empty=True,overlap_count=2 )\n",
    "x_train, y_train = purturb( x_train, y_train, ignore_empty=True,overlap_count=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acceleration',\n",
       " 'activity',\n",
       " 'battery',\n",
       " 'button',\n",
       " 'colorTemperature',\n",
       " 'contact',\n",
       " 'level',\n",
       " 'lock',\n",
       " 'motion',\n",
       " 'ping',\n",
       " 'status',\n",
       " 'switch',\n",
       " 'temperature',\n",
       " 'threeAxis',\n",
       " 'unknown',\n",
       " 'water']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(  np.concatenate( y_train  )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'acceleration'), (1, 'activity'), (2, 'battery'), (3, 'button'), (4, 'colorTemperature'), (5, 'contact'), (6, 'level'), (7, 'lock'), (8, 'motion'), (9, 'ping'), (10, 'status'), (11, 'switch'), (12, 'temperature'), (13, 'threeAxis'), (14, 'unknown'), (15, 'water')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'acceleration'),\n",
       " (1, 'activity'),\n",
       " (2, 'battery'),\n",
       " (3, 'button'),\n",
       " (4, 'colorTemperature'),\n",
       " (5, 'contact'),\n",
       " (6, 'level'),\n",
       " (7, 'lock'),\n",
       " (8, 'motion'),\n",
       " (9, 'ping'),\n",
       " (10, 'status'),\n",
       " (11, 'switch'),\n",
       " (12, 'temperature'),\n",
       " (13, 'threeAxis'),\n",
       " (14, 'unknown'),\n",
       " (15, 'water')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(list(np.unique(  np.concatenate( y_train  ))))\n",
    "print([ (i , classes[i]) for i in range( len(classes) ) ])\n",
    "\n",
    "service_classes = sorted(list(np.unique(  np.concatenate( y_train_service  ))))\n",
    "[ (i , service_classes[i]) for i in range( len(service_classes) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ (i , classes[i]) for i in range( len(classes) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean_event( inp, return_clean= True  ):\n",
    "    if return_clean:\n",
    "        return  'no_logs' not in inp and 'lock-unlocked' not in inp and 'on/off-XXX' not in inp and 'raw-XXX' not in inp and 'read_attr_-_raw-XXX' not in inp\n",
    "    else:\n",
    "        return  'lock-locked' in inp or 'lock-unlocked'  in inp or 'on/off-XXX' in inp or  'raw-XXX' in inp  or 'read_attr_-_raw-XXX' in inp \n",
    "    \n",
    "def is_clean_service( inp, return_clean= True  ):\n",
    "    \n",
    "    if return_clean:\n",
    "        return  'no_logs' not in inp and 'unknown' not in inp and 'read_attr_-_raw' not in inp #and 'ping' not in inp \n",
    "    else:\n",
    "        return  'no_logs' in inp or  'unknown' in inp  or 'read_attr_-_raw' in inp #or 'ping' in inp \n",
    "#     return  \"contact-open\" not in inp and 'contact-closed' not in inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toKeep = [ i for i in range(len(y_train)) if is_clean_event( y_train[i]) ] if Mapper=='SE' else [ i for i in range(len(y_train)) if is_clean_service( y_train[i]) ]\n",
    "x_train= [ x_train[i] for i in toKeep ]\n",
    "y_train= [ y_train[i] for i in toKeep ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(x_test)):\n",
    "    toChange= [ i for i in range(len(y_test[j])) if is_clean_service( y_test[j][i], False) ]\n",
    "    y_test[j] = [ (y_test[j][i] if i not in toChange else np.array( ['none'])) for i in range(len(y_test[j])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes.remove('read_attr_-_raw-XXX')\n",
    "# classes.remove('on/off-XXX')\n",
    "# classes.remove('raw-XXX')\n",
    "# classes.remove('lock-unlocked')\n",
    "# classes.remove('lock-locked')\n",
    "\n",
    "\n",
    "# classes.remove('read_attr_-_raw')\n",
    "# classes.remove('on/off')\n",
    "# classes.remove('raw')\n",
    "classes.remove('unknown')\n",
    "\n",
    "# classes.remove('lock')\n",
    "# # classes.remove('lock')\n",
    "\n",
    "\n",
    "# classes.remove('switch-on')\n",
    "\n",
    "\n",
    "\n",
    "service_classes= [\"\",\"\",\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_raw( x_data,y_data, dim_size = 128, zero_pad = False, normalize = False ,classes=None, twoD= False ):\n",
    "#  y data \n",
    "    if classes is None:\n",
    "        classes  = sorted(list(np.unique(  np.concatenate( y_data  ))))\n",
    "    else :\n",
    "        classes = sorted(classes)\n",
    "    y_data_categorical = []  \n",
    "\n",
    "    for x in y_data:\n",
    "        temp = np.zeros( len(classes) )\n",
    "        for y in x : \n",
    "            if y in classes:\n",
    "                temp[ classes.index( y ) ] = 1\n",
    "        y_data_categorical.append( temp )\n",
    "    y_data_categorical = np.vstack(y_data_categorical)\n",
    "\n",
    "#     x_data = np.array( x_data) / 1500.0\n",
    "    \n",
    "    x_data_temp = [] \n",
    "    \n",
    "    if not zero_pad:\n",
    "        if twoD:\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size**2 - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size**2:\n",
    "                    temp.append( 0 )\n",
    "\n",
    "                x_data_temp.append(np.array( temp).reshape(dim_size,dim_size))\n",
    "\n",
    "\n",
    "            x_data_temp = np.array( x_data_temp )\n",
    "            x_data_temp=x_data_temp.reshape(x_data_temp.shape+(1,))\n",
    "        else: \n",
    "            temp = [] \n",
    "            lst = list(x)\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size:\n",
    "                    temp.append( 0 )\n",
    "                \n",
    "                x_data_temp.append(np.array( temp))\n",
    "            \n",
    "    else :\n",
    "        x_data_temp = sequence.pad_sequences(x_data, maxlen=dim_size)\n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        x_data_temp = np.array( x_data_temp) / 1500.0\n",
    "    else :\n",
    "        x_data_temp = np.array(x_data_temp)\n",
    "    \n",
    "    \n",
    "    return x_data_temp ,y_data_categorical , classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recall_shit( inp ):\n",
    "    tp = inp[1][1]\n",
    "    tn = inp[0][0]\n",
    "    fp = inp[0][1] \n",
    "    fn = inp[1][0]\n",
    "    \n",
    "    acc = (tp+tn)*1.0 / ( tp+tn+fp+fn)*1.0\n",
    "    recall = tp*1.0/ ( tp+fn ) *1.0\n",
    "    prec = tp*1.0 / ( tp+fp )*1.0\n",
    "    \n",
    "#     F= 2.0*( prec* recall )/ (prec+recall)\n",
    "    F= 2.0*( tp)/ (2*tp + fp + fn)\n",
    "    \n",
    "    return acc, recall, prec, F\n",
    "\n",
    "def acc_match( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    " \n",
    "    return (len( [ x  for x  in  [np.sum(np.abs( true[i]- pred[i] )) for i in range(len(true))] if x  == 0]))*1.0 / len(true)\n",
    "\n",
    "\n",
    "# def acc_none_zero ( true, pred ):\n",
    "    \n",
    "\n",
    "def acc_match_wierd( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    "    level = 6 \n",
    "    switch = 11\n",
    "    threeAxis=13\n",
    "    accel = 0 \n",
    "    status=10\n",
    "    contact=5\n",
    "    \n",
    "    counter  = 0 \n",
    "    for i in range( len (true) ):\n",
    "        if np.sum(np.abs( true[i]- pred[i] ))==0 :\n",
    "            counter+=1\n",
    "        else : \n",
    "            t_rec = np.array(list( pred[i]))\n",
    "            \n",
    "            if true[i][level]==1 and true[i][switch]==1 and t_rec[level]==1 :\n",
    "                t_rec[switch]=1\n",
    "            \n",
    "            if true[i][threeAxis]==1 and true[i][accel]==1 and t_rec[threeAxis]==1:\n",
    "                t_rec[accel] =1\n",
    "            \n",
    "            if true[i][status]==1 and true[i][contact]==1 and t_rec[status]==1:\n",
    "                t_rec[contact]=1\n",
    "#             print(t_rec , true[i])    \n",
    "            if np.sum(np.abs( true[i]- t_rec ))==0 :\n",
    "                counter+=1   \n",
    "            \n",
    "             \n",
    "            \n",
    "    \n",
    "    return counter*1.0 / len(true)\n",
    "\n",
    "\n",
    "def print_info(y_test, pred , classes , confidance=0.5 ):\n",
    "    \n",
    "    counts = np.sum( y_test.astype(int) , axis=0)\n",
    "    \n",
    "    pred[pred>=confidance] = 1\n",
    "    pred[pred<confidance] = 0\n",
    "    \n",
    "#     acc_wierd  =acc_match_wierd(y_test, pred)\n",
    "    \n",
    "    conf= multilabel_confusion_matrix( y_test , pred.astype(int), labels= range(len(classes)))\n",
    "    accs = [make_recall_shit(x) for x in conf]\n",
    "    print( \"%30s  %8s   %8s  %8s  %8s %8s %16s\"  %( \"Class\",\"Accuracy\", \"Recall\",\"Precision\",\"F Score\" , \"Count\", \"TP/TN/FP/FN\"))\n",
    "    print( \"------------------------------------------------------------------------\" )\n",
    "    \n",
    "    for index in range(len(classes)):\n",
    "        tp = conf[index][1][1]\n",
    "        tn = conf[index][0][0]\n",
    "        fp = conf[index][0][1] \n",
    "        fn = conf[index][1][0]\n",
    "        print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %d/%d/%d/%d\"  %\n",
    "             (classes[index],\n",
    "              accs[index][0],\n",
    "              accs[index][1],\n",
    "              accs[index][2],\n",
    "              accs[index][3],\n",
    "              counts[index],\n",
    "                  tp ,\n",
    "                tn ,\n",
    "                fp ,\n",
    "                fn ))\n",
    "    n_zeros_true = len([ x  for x  in  [np.sum(np.abs( y_test[i] )) for i in range(len(y_test))] if x  == 0]  )\n",
    "    n_zeros_pred = len([ x  for x  in  [np.sum(np.abs( pred[i] )) for i in range(len(pred))] if x  == 0]  )\n",
    "    \n",
    "    accs = np.nan_to_num(accs)\n",
    "    \n",
    "    print (\"------------------------------------------------------------------------\")\n",
    "    print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %d/%d/%d/%d\"  %\n",
    "             (\"AVERAGES\",\n",
    "              np.average( accs, axis=0)[0],\n",
    "              np.average( accs, axis=0)[1],\n",
    "              np.average( accs, axis=0)[2],\n",
    "              np.average( accs, axis=0)[3],\n",
    "              len(y_test),\n",
    "                  0 ,\n",
    "                0,\n",
    "                0 ,\n",
    "                0 ))\n",
    "    \n",
    "    print ( \"Exact Match ACC : %.5f \" % acc_match( y_test, pred )  )\n",
    "#     print ( \"Wierd Exact Match ACC : %.5f\" % acc_wierd)\n",
    "    print ( \"Total Records : %d \" % len(y_test)  )\n",
    "    print ( \"Total ZXeros in True : %d (%.3f)%%\" % (n_zeros_true ,  n_zeros_true * 1.0/ len(y_test)  ))\n",
    "    print ( \"Total ZXeros in Test : %d (%.3f)%%\" % (n_zeros_pred ,  n_zeros_pred * 1.0/ len(y_test)  ) )\n",
    "    print ('=============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_readable_results ( inp , classes , conffidance=True):\n",
    "    ret = [] \n",
    "    inp =inp.astype(int)\n",
    "    for xx in range(len(inp)) :\n",
    "        u = inp[xx]\n",
    "        temp = []\n",
    "        for j in range(len(u)) : \n",
    "            if u[j] >0:\n",
    "                temp.append(classes[j])\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def makeReadable( model , data, gt, path , classes, x, confidance=0.7):\n",
    "    pred_temp = model.predict(data)\n",
    "    print_info(gt, pred_temp, classes , confidance=confidance)\n",
    "    print( len(classes ), len( pred_temp[0] ) )\n",
    "    xcc= make_readable_results(pred_temp , classes)\n",
    "    y_gt = make_readable_results( gt, classes )\n",
    "    temp_dic = {} \n",
    "    for pick in range(len(xcc)): \n",
    "        temp_dic[ pick +1 ] =  { 'seq': str(data[pick]),\n",
    "                               'pred': xcc[pick],\n",
    "                                'true':y_gt[pick]\n",
    "                               }   \n",
    "\n",
    "    with open(path , 'w') as f:\n",
    "        json.dump(temp_dic , f, indent=4)\n",
    "\n",
    "\n",
    "# def makeReadable( model , data, gt, path , classes, confidance=0.7, x):\n",
    "#     pred_temp = model.predict( data)\n",
    "#     print_info(gt, pred_temp, classes , confidance=confidance)\n",
    "#     xcc= make_readable_results( pred_temp , classes )\n",
    "#     temp_dic = {} \n",
    "#     for pick in range(len(xcc)): \n",
    "#         temp_dic[ pick +1 ] = xcc[pick]  \n",
    "\n",
    "#     with open(path , 'w') as f:\n",
    "#         json.dump(temp_dic , f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcualte per class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest baseline calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size= 50\n",
    "x_random_forest_train,y_random_forest_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "# x_random_forest_test,y_random_forest_test, _ = pre_process_raw( x_test[0], y_test[0] , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "rf_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=True, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "# x,y, classes = pre_process_raw( x_data, y_data , dim_size, zero_pad=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "t_hist = clf.fit(x_random_forest_train, y_random_forest_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(clf.feature_importances_)\n",
    "\n",
    "# print(clf.predict([[0, 0, 0, 0]]))\n",
    "# from sklearn import metrics\n",
    "# scores = cross_val_score(clf, x_random_forest_train, y_random_forest_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     1.000      0.500     0.750     0.600         6 3/32062/1/3\n",
      "                      activity     0.999      0.729     0.854     0.787        48 35/32015/6/13\n",
      "                       battery     1.000        nan     0.000     0.000         0 0/32067/2/0\n",
      "                        button     1.000      0.857     0.857     0.857         7 6/32061/1/1\n",
      "              colorTemperature     1.000        nan       nan       nan         0 0/32069/0/0\n",
      "                       contact     0.999      0.718     0.862     0.783        78 56/31982/9/22\n",
      "                         level     0.995      0.815     0.128     0.221        27 22/31892/150/5\n",
      "                          lock     0.999      0.786     0.229     0.355        14 11/32018/37/3\n",
      "                        motion     0.971      0.183     0.824     0.299      1099 201/30927/43/898\n",
      "                          ping     0.997      0.997     0.991     0.994      6975 6951/25034/60/24\n",
      "                        status     1.000      0.920     0.920     0.920        50 46/32015/4/4\n",
      "                        switch     1.000      0.957     0.957     0.957        23 22/32045/1/1\n",
      "                   temperature     0.955      0.095     0.702     0.168      1512 144/30496/61/1368\n",
      "                     threeAxis     0.999      0.625     0.250     0.357         8 5/32046/15/3\n",
      "                       unknown     0.898      0.984     0.883     0.931     22337 21985/6806/2926/352\n",
      "                         water     1.000        nan     0.000     0.000         0 0/32066/3/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.988      0.573     0.575     0.514     32069 0/0/0/0\n",
      "Exact Match ACC : 0.89345 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 811 (0.025)%\n",
      "Total ZXeros in Test : 191 (0.006)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.998      0.281     0.842     0.421        57 16/19908/3/41\n",
      "                      activity     0.999      0.684     0.684     0.684        19 13/19943/6/6\n",
      "                       battery     0.999      0.000     0.000     0.000         7 0/19958/3/7\n",
      "                        button     0.999      0.000     0.000     0.000        14 0/19953/1/14\n",
      "              colorTemperature     1.000      0.667     1.000     0.800         6 4/19962/0/2\n",
      "                       contact     0.997      0.665     0.921     0.772       176 117/19782/10/59\n",
      "                         level     0.986      0.704     0.064     0.117        27 19/19661/280/8\n",
      "                          lock     0.997      0.771     0.380     0.509        35 27/19889/44/8\n",
      "                        motion     0.981      0.194     0.483     0.277       371 72/19520/77/299\n",
      "                          ping     0.997      0.994     0.992     0.993      4842 4814/15086/40/28\n",
      "                        status     0.998      0.723     0.851     0.782       119 86/19834/15/33\n",
      "                        switch     0.999      0.619     0.867     0.722        21 13/19945/2/8\n",
      "                   temperature     0.938      0.054     0.328     0.093      1168 63/18671/129/1105\n",
      "                     threeAxis     0.998      0.492     0.775     0.602        63 31/19896/9/32\n",
      "                       unknown     0.883      0.949     0.884     0.915     13305 12631/5005/1658/674\n",
      "                         water     1.000        nan     0.000     0.000         0 0/19964/4/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.986      0.487     0.567     0.480     19968 0/0/0/0\n",
      "Exact Match ACC : 0.86684 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 452 (0.023)%\n",
      "Total ZXeros in Test : 418 (0.021)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_os_final_reduced.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.998      0.200     1.000     0.333        25 5/9084/0/20\n",
      "                      activity     0.998      0.381     0.800     0.516        21 8/9086/2/13\n",
      "                       battery     1.000      0.000       nan     0.000         4 0/9105/0/4\n",
      "                        button     1.000      0.000     0.000     0.000         3 0/9105/1/3\n",
      "              colorTemperature     1.000        nan       nan       nan         0 0/9109/0/0\n",
      "                       contact     0.996      0.615     0.923     0.738        78 48/9027/4/30\n",
      "                         level     0.986      0.750     0.066     0.122        12 9/8970/127/3\n",
      "                          lock     0.997      0.545     0.400     0.462        22 12/9069/18/10\n",
      "                        motion     0.976      0.170     0.507     0.254       218 37/8855/36/181\n",
      "                          ping     0.996      0.993     0.989     0.991      2237 2222/6847/25/15\n",
      "                        status     0.996      0.580     0.674     0.624        50 29/9045/14/21\n",
      "                        switch     1.000      0.500     0.750     0.600         6 3/9102/1/3\n",
      "                   temperature     0.929      0.044     0.373     0.079       630 28/8432/47/602\n",
      "                     threeAxis     0.998      0.480     0.857     0.615        25 12/9082/2/13\n",
      "                       unknown     0.874      0.953     0.867     0.908      5948 5671/2290/871/277\n",
      "                         water     1.000        nan     0.000     0.000         0 0/9108/1/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.984      0.388     0.513     0.390      9109 0/0/0/0\n",
      "Exact Match ACC : 0.85717 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 168 (0.018)%\n",
      "Total ZXeros in Test : 167 (0.018)%\n",
      "=============================================================================\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.995      0.300     0.857     0.444        40 12/6362/2/28\n",
      "                      activity     1.000        nan       nan       nan         0 0/6404/0/0\n",
      "                       battery     1.000      0.000       nan     0.000         2 0/6402/0/2\n",
      "                        button     0.991      0.050     0.750     0.094        60 3/6343/1/57\n",
      "              colorTemperature     1.000      0.000       nan     0.000         2 0/6402/0/2\n",
      "                       contact     0.987      0.514     0.989     0.677       175 90/6228/1/85\n",
      "                         level     0.993      0.515     0.708     0.596        66 34/6324/14/32\n",
      "                          lock     0.999        nan     0.000     0.000         0 0/6400/4/0\n",
      "                        motion     0.981      0.269     0.696     0.388       145 39/6242/17/106\n",
      "                          ping     0.998      0.997     0.996     0.997      2307 2300/4088/9/7\n",
      "                        status     0.996      0.767     0.988     0.863       103 79/6300/1/24\n",
      "                        switch     0.997      0.417     0.667     0.513        24 10/6375/5/14\n",
      "                   temperature     0.967      0.064     0.361     0.109       203 13/6178/23/190\n",
      "                     threeAxis     0.996      0.587     0.818     0.684        46 27/6352/6/19\n",
      "                       unknown     0.927      0.983     0.896     0.938      3558 3499/2439/407/59\n",
      "                         water     1.000        nan       nan       nan         0 0/6404/0/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.989      0.341     0.545     0.394      6404 0/0/0/0\n",
      "Exact Match ACC : 0.90240 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 80 (0.012)%\n",
      "Total ZXeros in Test : 103 (0.016)%\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_tests[i][0])\n",
    "    print_info( rf_tests[i][1], rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makeReadable( data=rf_tests[0][0], model=clf, classes=classes, confidance=0.7,gt=rf_tests[0][1], path='sk_home_out.json' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(y_random_forest_test, rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(y_random_forest_test, rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ( \"mean : %f \\nstd: %f\\nmax:%f\" %( scores.mean(), scores.std(), scores.max()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnts  = np.unique(np.array([ len(x) for x  in x_train ]), return_counts=True)\n",
    "# # np.sort( cnts[1] )\n",
    "# cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dim_size =15\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False,classes=classes)\n",
    "_, y_s_lstm_processed_train ,_ =  pre_process_raw( x_train, y_train_service , dim_size, zero_pad=False, normalize=False,classes=service_classes)\n",
    "# x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test_2 , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "lstm_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "lstm_tests_services  = [ pre_process_raw( x_test[i], y_test_service[i] , dim_size, zero_pad=False, normalize=False, classes=service_classes) for i in range(len(x_test)) ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([290,  99, 412,  99, 290,  99, 412,  99, 290,  99, 412,  99,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lstm_prossed_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    }
   ],
   "source": [
    "for i in range( len(lstm_tests) ):\n",
    "    print( len( lstm_tests[i][0] ), len( lstm_tests_services[i][0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lstm_prossed_test2 = np.expand_dims(x_lstm_prossed_test,axis=1)\n",
    "# x_lstm_prossed_train2 =np.expand_dims(x_lstm_prossed_train,axis=1)\n",
    "\n",
    "for tt  in range( len(lstm_tests) ):\n",
    "    lstm_tests[tt]= (lstm_tests[tt][0].reshape(len(lstm_tests[tt][0]),dim_size,1) , lstm_tests[tt][1],lstm_tests_services[tt][1] )\n",
    "# x_lstm_prossed_test2 = x_lstm_prossed_test.reshape(len(x_lstm_prossed_test),dim_size,1)\n",
    "x_lstm_prossed_train2 =x_lstm_prossed_train.reshape(len(x_lstm_prossed_train),dim_size,1)\n",
    "\n",
    "# y_lstm_prossed_test2 = y_lstm_prossed_test.reshape(len(y_lstm_prossed_test),len(classes),1)\n",
    "# y_lstm_prossed_train2 =y_lstm_prossed_train.reshape(len(y_lstm_prossed_train),len(classes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60562.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_s_lstm_processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 15, 1)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)               (None, 15, 128)       512         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)               (None, 15, 128)       512         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNor (None, 15, 128)       512         conv1d_53[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)               (None, 15, 128)       384         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNor (None, 15, 128)       512         conv1d_49[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 15, 128)       0           batch_normalization_27[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNor (None, 15, 128)       512         conv1d_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 15, 128)       0           batch_normalization_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)            (None, 15, 128)       0           activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 15, 128)       0           batch_normalization_26[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)             (None, 15, 128)       0           activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)               (None, 15, 128)       49280       dropout_105[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)             (None, 15, 128)       0           activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)               (None, 15, 128)       49280       dropout_91[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)               (None, 15, 128)       49280       conv1d_54[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)               (None, 15, 128)       32896       dropout_96[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                   (None, 15, 100)       91600       conv1d_50[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNor (None, 15, 128)       512         conv1d_55[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                   (None, 15, 100)       40800       input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                   (None, 15, 100)       91600       conv1d_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_72 (Dense)                 (None, 15, 128)       12928       lstm_15[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 15, 128)       0           batch_normalization_28[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_80 (Dense)                 (None, 15, 128)       12928       lstm_17[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_76 (Dense)                 (None, 15, 128)       12928       lstm_16[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)             (None, 15, 128)       0           dense_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)            (None, 15, 128)       0           activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)            (None, 15, 128)       0           dense_80[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)             (None, 15, 128)       0           dense_76[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_73 (Dense)                 (None, 15, 128)       16512       dropout_92[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)               (None, 15, 128)       49280       dropout_106[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_81 (Dense)                 (None, 15, 128)       16512       dropout_101[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_77 (Dense)                 (None, 15, 128)       16512       dropout_97[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)             (None, 15, 128)       0           dense_73[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)               (None, 15, 128)       49280       conv1d_56[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)            (None, 15, 128)       0           dense_81[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)             (None, 15, 128)       0           dense_77[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (None, 15, 128)       16512       dropout_93[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNor (None, 15, 128)       512         conv1d_57[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_82 (Dense)                 (None, 15, 128)       16512       dropout_102[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_78 (Dense)                 (None, 15, 128)       16512       dropout_98[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)             (None, 15, 128)       0           dense_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 15, 128)       0           batch_normalization_29[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)            (None, 15, 128)       0           dense_82[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)             (None, 15, 128)       0           dense_78[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)             (None, 1920)          0           dropout_94[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)            (None, 15, 128)       0           activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)             (None, 1920)          0           dropout_103[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)             (None, 1920)          0           dropout_99[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_75 (Dense)                 (None, 128)           245888      flatten_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)               (None, 15, 128)       49280       dropout_107[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_83 (Dense)                 (None, 128)           245888      flatten_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_79 (Dense)                 (None, 128)           245888      flatten_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)             (None, 128)           0           dense_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)             (None, 1920)          0           conv1d_58[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)            (None, 128)           0           dense_83[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)            (None, 128)           0           dense_79[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "mergerguy (Concatenate)          (None, 2304)          0           dropout_95[0][0]                 \n",
      "                                                                   flatten_23[0][0]                 \n",
      "                                                                   dropout_104[0][0]                \n",
      "                                                                   dropout_100[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_84 (Dense)                 (None, 128)           295040      mergerguy[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)            (None, 128)           0           dense_84[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_85 (Dense)                 (None, 128)           16512       dropout_108[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)            (None, 128)           0           dense_85[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_86 (Dense)                 (None, 128)           16512       dropout_109[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "to_service1 (Dense)              (None, 130)           16770       dense_86[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "to_service2 (Dense)              (None, 130)           17030       to_service1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "service_output (Dense)           (None, 16)            2096        to_service2[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 1,796,024\n",
      "Trainable params: 1,794,744\n",
      "Non-trainable params: 1,280\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "65266/65266 [==============================] - 6s - loss: 46.8733 - fp_rate: 4.6743 - f1_perRow: 0.1077 - f1_perClass: 0.2969 - acc: 0.4831     \n",
      "Epoch 2/100\n",
      "65266/65266 [==============================] - 3s - loss: 33.0607 - fp_rate: 1.9123 - f1_perRow: 0.1561 - f1_perClass: 0.4923 - acc: 0.4900     \n",
      "Epoch 3/100\n",
      "65266/65266 [==============================] - 3s - loss: 30.4007 - fp_rate: 1.5741 - f1_perRow: 0.1911 - f1_perClass: 0.5279 - acc: 0.4140     \n",
      "Epoch 4/100\n",
      "65266/65266 [==============================] - 3s - loss: 27.2212 - fp_rate: 1.1292 - f1_perRow: 0.2269 - f1_perClass: 0.5969 - acc: 0.4139     \n",
      "Epoch 5/100\n",
      "65266/65266 [==============================] - 3s - loss: 26.1174 - fp_rate: 1.1373 - f1_perRow: 0.2545 - f1_perClass: 0.6117 - acc: 0.4139     \n",
      "Epoch 6/100\n",
      "65266/65266 [==============================] - 3s - loss: 25.4940 - fp_rate: 1.1354 - f1_perRow: 0.2683 - f1_perClass: 0.6180 - acc: 0.4224     \n",
      "Epoch 7/100\n",
      "65266/65266 [==============================] - 3s - loss: 24.9677 - fp_rate: 1.1167 - f1_perRow: 0.2887 - f1_perClass: 0.6250 - acc: 0.4455     \n",
      "Epoch 8/100\n",
      "65266/65266 [==============================] - 3s - loss: 24.6571 - fp_rate: 1.0980 - f1_perRow: 0.2994 - f1_perClass: 0.6290 - acc: 0.4658     \n",
      "Epoch 9/100\n",
      "65266/65266 [==============================] - 3s - loss: 24.4175 - fp_rate: 1.0794 - f1_perRow: 0.3063 - f1_perClass: 0.6319 - acc: 0.4865     \n",
      "Epoch 10/100\n",
      "65266/65266 [==============================] - 3s - loss: 24.1259 - fp_rate: 1.0645 - f1_perRow: 0.3139 - f1_perClass: 0.6338 - acc: 0.5238     \n",
      "Epoch 11/100\n",
      "65266/65266 [==============================] - 3s - loss: 23.9148 - fp_rate: 1.0466 - f1_perRow: 0.3175 - f1_perClass: 0.6344 - acc: 0.6064     \n",
      "Epoch 12/100\n",
      "65266/65266 [==============================] - 3s - loss: 23.7195 - fp_rate: 1.0320 - f1_perRow: 0.3198 - f1_perClass: 0.6347 - acc: 0.6510     \n",
      "Epoch 13/100\n",
      "65266/65266 [==============================] - 3s - loss: 23.5722 - fp_rate: 1.0096 - f1_perRow: 0.3203 - f1_perClass: 0.6339 - acc: 0.6644     \n",
      "Epoch 14/100\n",
      "65266/65266 [==============================] - 3s - loss: 22.6061 - fp_rate: 0.7627 - f1_perRow: 0.3138 - f1_perClass: 0.6440 - acc: 0.6708     \n",
      "Epoch 15/100\n",
      "65266/65266 [==============================] - 3s - loss: 21.3165 - fp_rate: 0.6402 - f1_perRow: 0.3088 - f1_perClass: 0.6762 - acc: 0.6889     \n",
      "Epoch 16/100\n",
      "65266/65266 [==============================] - 3s - loss: 19.0602 - fp_rate: 0.4851 - f1_perRow: 0.3078 - f1_perClass: 0.6863 - acc: 0.7482     \n",
      "Epoch 17/100\n",
      "65266/65266 [==============================] - 3s - loss: 17.5138 - fp_rate: 0.4004 - f1_perRow: 0.3116 - f1_perClass: 0.6967 - acc: 0.7838     \n",
      "Epoch 18/100\n",
      "65266/65266 [==============================] - 3s - loss: 16.7357 - fp_rate: 0.3658 - f1_perRow: 0.3204 - f1_perClass: 0.7287 - acc: 0.7984     \n",
      "Epoch 19/100\n",
      "65266/65266 [==============================] - 3s - loss: 16.3241 - fp_rate: 0.3585 - f1_perRow: 0.3388 - f1_perClass: 0.7527 - acc: 0.8042     \n",
      "Epoch 20/100\n",
      "65266/65266 [==============================] - 3s - loss: 16.1231 - fp_rate: 0.3643 - f1_perRow: 0.3510 - f1_perClass: 0.7597 - acc: 0.8052     \n",
      "Epoch 21/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.8989 - fp_rate: 0.3615 - f1_perRow: 0.3510 - f1_perClass: 0.7567 - acc: 0.8077     \n",
      "Epoch 22/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.6314 - fp_rate: 0.3449 - f1_perRow: 0.3544 - f1_perClass: 0.7512 - acc: 0.8103     \n",
      "Epoch 23/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.5768 - fp_rate: 0.3367 - f1_perRow: 0.3506 - f1_perClass: 0.7273 - acc: 0.8108     \n",
      "Epoch 24/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.4747 - fp_rate: 0.3288 - f1_perRow: 0.3471 - f1_perClass: 0.7156 - acc: 0.8107     \n",
      "Epoch 25/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.3002 - fp_rate: 0.3188 - f1_perRow: 0.3532 - f1_perClass: 0.7330 - acc: 0.8135     \n",
      "Epoch 26/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.2231 - fp_rate: 0.3287 - f1_perRow: 0.3608 - f1_perClass: 0.7578 - acc: 0.8133     \n",
      "Epoch 27/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.9861 - fp_rate: 0.3203 - f1_perRow: 0.3610 - f1_perClass: 0.7459 - acc: 0.8145     \n",
      "Epoch 28/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.9995 - fp_rate: 0.3191 - f1_perRow: 0.3605 - f1_perClass: 0.7448 - acc: 0.8168     \n",
      "Epoch 29/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.2256 - fp_rate: 0.3241 - f1_perRow: 0.3588 - f1_perClass: 0.7460 - acc: 0.8156     \n",
      "Epoch 30/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.0314 - fp_rate: 0.3135 - f1_perRow: 0.3531 - f1_perClass: 0.7449 - acc: 0.8155     \n",
      "Epoch 31/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.9158 - fp_rate: 0.3167 - f1_perRow: 0.3609 - f1_perClass: 0.7377 - acc: 0.8150     \n",
      "Epoch 32/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.7960 - fp_rate: 0.3191 - f1_perRow: 0.3704 - f1_perClass: 0.7557 - acc: 0.8178     \n",
      "Epoch 33/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.7047 - fp_rate: 0.3212 - f1_perRow: 0.3726 - f1_perClass: 0.7638 - acc: 0.8161     \n",
      "Epoch 34/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.7810 - fp_rate: 0.3190 - f1_perRow: 0.3720 - f1_perClass: 0.7531 - acc: 0.8173     \n",
      "Epoch 35/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.4961 - fp_rate: 0.3083 - f1_perRow: 0.3767 - f1_perClass: 0.7649 - acc: 0.8186     \n",
      "Epoch 36/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.9447 - fp_rate: 0.3152 - f1_perRow: 0.3717 - f1_perClass: 0.7504 - acc: 0.8169     \n",
      "Epoch 37/100\n",
      "65266/65266 [==============================] - 3s - loss: 15.0187 - fp_rate: 0.3186 - f1_perRow: 0.3628 - f1_perClass: 0.7463 - acc: 0.8173     \n",
      "Epoch 38/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.7464 - fp_rate: 0.3209 - f1_perRow: 0.3701 - f1_perClass: 0.7603 - acc: 0.8184     \n",
      "Epoch 39/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.5416 - fp_rate: 0.3116 - f1_perRow: 0.3818 - f1_perClass: 0.7544 - acc: 0.8176     \n",
      "Epoch 40/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.4977 - fp_rate: 0.3209 - f1_perRow: 0.3827 - f1_perClass: 0.7568 - acc: 0.8165     \n",
      "Epoch 41/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.3545 - fp_rate: 0.3004 - f1_perRow: 0.3845 - f1_perClass: 0.7467 - acc: 0.8199     \n",
      "Epoch 42/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.3038 - fp_rate: 0.3070 - f1_perRow: 0.3882 - f1_perClass: 0.7555 - acc: 0.8184     \n",
      "Epoch 43/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.6340 - fp_rate: 0.3067 - f1_perRow: 0.3868 - f1_perClass: 0.7629 - acc: 0.8177     \n",
      "Epoch 44/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.3802 - fp_rate: 0.3058 - f1_perRow: 0.3823 - f1_perClass: 0.7486 - acc: 0.8202     \n",
      "Epoch 45/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.2214 - fp_rate: 0.2918 - f1_perRow: 0.3806 - f1_perClass: 0.7418 - acc: 0.8193     \n",
      "Epoch 46/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.1899 - fp_rate: 0.3024 - f1_perRow: 0.3960 - f1_perClass: 0.7662 - acc: 0.8207     \n",
      "Epoch 47/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.2359 - fp_rate: 0.2997 - f1_perRow: 0.3934 - f1_perClass: 0.7601 - acc: 0.8193     \n",
      "Epoch 48/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.3590 - fp_rate: 0.3070 - f1_perRow: 0.3924 - f1_perClass: 0.7718 - acc: 0.8195     \n",
      "Epoch 49/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.2043 - fp_rate: 0.3031 - f1_perRow: 0.3899 - f1_perClass: 0.7546 - acc: 0.8208     \n",
      "Epoch 50/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.1508 - fp_rate: 0.2869 - f1_perRow: 0.3880 - f1_perClass: 0.7622 - acc: 0.8224     \n",
      "Epoch 51/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.0571 - fp_rate: 0.3000 - f1_perRow: 0.3968 - f1_perClass: 0.7529 - acc: 0.8218     \n",
      "Epoch 52/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.9497 - fp_rate: 0.2957 - f1_perRow: 0.3997 - f1_perClass: 0.7573 - acc: 0.8190     \n",
      "Epoch 53/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.0080 - fp_rate: 0.3001 - f1_perRow: 0.4069 - f1_perClass: 0.7691 - acc: 0.8225     \n",
      "Epoch 54/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.2837 - fp_rate: 0.3058 - f1_perRow: 0.3979 - f1_perClass: 0.7644 - acc: 0.8212     \n",
      "Epoch 55/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.0337 - fp_rate: 0.2986 - f1_perRow: 0.3968 - f1_perClass: 0.7631 - acc: 0.8221     \n",
      "Epoch 56/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.8562 - fp_rate: 0.2924 - f1_perRow: 0.3999 - f1_perClass: 0.7647 - acc: 0.8237     \n",
      "Epoch 57/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.8318 - fp_rate: 0.2831 - f1_perRow: 0.3996 - f1_perClass: 0.7563 - acc: 0.8233     \n",
      "Epoch 58/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.0423 - fp_rate: 0.2973 - f1_perRow: 0.4075 - f1_perClass: 0.7640 - acc: 0.8237     \n",
      "Epoch 59/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.9018 - fp_rate: 0.2924 - f1_perRow: 0.4062 - f1_perClass: 0.7720 - acc: 0.8220     \n",
      "Epoch 60/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7801 - fp_rate: 0.2914 - f1_perRow: 0.4144 - f1_perClass: 0.7962 - acc: 0.8276     \n",
      "Epoch 61/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7412 - fp_rate: 0.2878 - f1_perRow: 0.4128 - f1_perClass: 0.7759 - acc: 0.8247     \n",
      "Epoch 62/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7861 - fp_rate: 0.2958 - f1_perRow: 0.4088 - f1_perClass: 0.7809 - acc: 0.8244     \n",
      "Epoch 63/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.8305 - fp_rate: 0.2976 - f1_perRow: 0.4109 - f1_perClass: 0.7755 - acc: 0.8259     \n",
      "Epoch 64/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.9274 - fp_rate: 0.3061 - f1_perRow: 0.4055 - f1_perClass: 0.7842 - acc: 0.8247     \n",
      "Epoch 65/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7942 - fp_rate: 0.2967 - f1_perRow: 0.4112 - f1_perClass: 0.7706 - acc: 0.8235     \n",
      "Epoch 66/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7120 - fp_rate: 0.2940 - f1_perRow: 0.4191 - f1_perClass: 0.7796 - acc: 0.8223     \n",
      "Epoch 67/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.6089 - fp_rate: 0.2870 - f1_perRow: 0.4259 - f1_perClass: 0.7925 - acc: 0.8242     \n",
      "Epoch 68/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7362 - fp_rate: 0.2971 - f1_perRow: 0.4150 - f1_perClass: 0.7778 - acc: 0.8226     \n",
      "Epoch 69/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5859 - fp_rate: 0.2864 - f1_perRow: 0.4187 - f1_perClass: 0.7887 - acc: 0.8240     \n",
      "Epoch 70/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5028 - fp_rate: 0.2941 - f1_perRow: 0.4252 - f1_perClass: 0.7838 - acc: 0.8248     \n",
      "Epoch 71/100\n",
      "65266/65266 [==============================] - 3s - loss: 14.3169 - fp_rate: 0.2987 - f1_perRow: 0.4072 - f1_perClass: 0.7561 - acc: 0.8253     \n",
      "Epoch 72/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.9181 - fp_rate: 0.2961 - f1_perRow: 0.4061 - f1_perClass: 0.7933 - acc: 0.8242     \n",
      "Epoch 73/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.6959 - fp_rate: 0.2758 - f1_perRow: 0.4073 - f1_perClass: 0.7749 - acc: 0.8245     \n",
      "Epoch 74/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5654 - fp_rate: 0.2838 - f1_perRow: 0.4229 - f1_perClass: 0.7861 - acc: 0.8270     \n",
      "Epoch 75/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5330 - fp_rate: 0.2936 - f1_perRow: 0.4217 - f1_perClass: 0.7865 - acc: 0.8223     \n",
      "Epoch 76/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7051 - fp_rate: 0.2969 - f1_perRow: 0.4320 - f1_perClass: 0.7990 - acc: 0.8249     \n",
      "Epoch 77/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.6758 - fp_rate: 0.2946 - f1_perRow: 0.4228 - f1_perClass: 0.8016 - acc: 0.8233     \n",
      "Epoch 78/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5926 - fp_rate: 0.2916 - f1_perRow: 0.4204 - f1_perClass: 0.7891 - acc: 0.8261     \n",
      "Epoch 79/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.6526 - fp_rate: 0.2875 - f1_perRow: 0.4231 - f1_perClass: 0.8024 - acc: 0.8270     \n",
      "Epoch 80/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.8017 - fp_rate: 0.2876 - f1_perRow: 0.4224 - f1_perClass: 0.8005 - acc: 0.8250     \n",
      "Epoch 81/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.7127 - fp_rate: 0.2991 - f1_perRow: 0.4210 - f1_perClass: 0.7908 - acc: 0.8271     \n",
      "Epoch 82/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5814 - fp_rate: 0.2892 - f1_perRow: 0.4249 - f1_perClass: 0.8002 - acc: 0.8264     \n",
      "Epoch 83/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.4737 - fp_rate: 0.2800 - f1_perRow: 0.4291 - f1_perClass: 0.7850 - acc: 0.8286     \n",
      "Epoch 84/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.3033 - fp_rate: 0.2744 - f1_perRow: 0.4366 - f1_perClass: 0.8024 - acc: 0.8256     \n",
      "Epoch 85/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.2123 - fp_rate: 0.2753 - f1_perRow: 0.4399 - f1_perClass: 0.7928 - acc: 0.8264     \n",
      "Epoch 86/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.1070 - fp_rate: 0.2833 - f1_perRow: 0.4422 - f1_perClass: 0.7980 - acc: 0.8261     \n",
      "Epoch 87/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.2359 - fp_rate: 0.2795 - f1_perRow: 0.4432 - f1_perClass: 0.8018 - acc: 0.8267     \n",
      "Epoch 88/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.2335 - fp_rate: 0.2830 - f1_perRow: 0.4393 - f1_perClass: 0.8064 - acc: 0.8271     \n",
      "Epoch 89/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.2568 - fp_rate: 0.2901 - f1_perRow: 0.4417 - f1_perClass: 0.8037 - acc: 0.8280     \n",
      "Epoch 90/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.5791 - fp_rate: 0.2936 - f1_perRow: 0.4360 - f1_perClass: 0.7872 - acc: 0.8264     \n",
      "Epoch 91/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.4301 - fp_rate: 0.2802 - f1_perRow: 0.4331 - f1_perClass: 0.8078 - acc: 0.8298     \n",
      "Epoch 92/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.3800 - fp_rate: 0.2883 - f1_perRow: 0.4350 - f1_perClass: 0.7966 - acc: 0.8257     \n",
      "Epoch 93/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.3323 - fp_rate: 0.2866 - f1_perRow: 0.4392 - f1_perClass: 0.8002 - acc: 0.8296     \n",
      "Epoch 94/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.1061 - fp_rate: 0.2708 - f1_perRow: 0.4398 - f1_perClass: 0.8006 - acc: 0.8272     \n",
      "Epoch 95/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.4637 - fp_rate: 0.2951 - f1_perRow: 0.4402 - f1_perClass: 0.8029 - acc: 0.8272     \n",
      "Epoch 96/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.1804 - fp_rate: 0.2777 - f1_perRow: 0.4451 - f1_perClass: 0.8056 - acc: 0.8255     \n",
      "Epoch 97/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.1570 - fp_rate: 0.2784 - f1_perRow: 0.4480 - f1_perClass: 0.8163 - acc: 0.8281     \n",
      "Epoch 98/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.2945 - fp_rate: 0.2936 - f1_perRow: 0.4488 - f1_perClass: 0.8064 - acc: 0.8280     \n",
      "Epoch 99/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.2573 - fp_rate: 0.2765 - f1_perRow: 0.4416 - f1_perClass: 0.8084 - acc: 0.8302     \n",
      "Epoch 100/100\n",
      "65266/65266 [==============================] - 3s - loss: 13.0514 - fp_rate: 0.2645 - f1_perRow: 0.4531 - f1_perClass: 0.8182 - acc: 0.8304     \n"
     ]
    }
   ],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fn_loss ( y_true, y_pred ):\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    loss = fp\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return 1.0 - K.mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "def fn_rate ( y_true, y_pred ):\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    loss = fn\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return  K.mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "def fp_loss ( y_true, y_pred ):\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    loss = fp\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return  K.mean(loss)\n",
    "\n",
    "\n",
    "def fp_loss_perRow ( y_true, y_pred ):\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    loss = fp\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return  K.sum(loss)\n",
    "\n",
    "\n",
    "\n",
    "def fp_rate ( y_true, y_pred ):\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    loss = fp\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return  K.mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "bi_d_1= Dropout( 0.1 )(bi_d_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_1 = Dropout(0.1)(lstm_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,2,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,2,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "bi_d_1= Dropout( 0.1 )(bi_d_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_1 = Dropout(0.1)(lstm_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "bi_d_raw_1 = Dropout(0.1)(bi_d_raw_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_1= Dropout(0.1)(lstm_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [dout_2, fl_out_cnn,dout_3,dout_2_2] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_1= Dropout(0.1)(dens_out_1)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_2=Dropout(0.1)(dens_out_2)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": fp_loss, \n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 25,\n",
    "    \"service_output\": 20\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[fp_rate, f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=11500, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fp_loss_perRow ( y_true, y_pred ):\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    loss = fp\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return  K.sum(loss)\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": fp_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "            \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=1e-5  ), metrics=[f1_perRow,f1_perClass,'acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "65266/65266 [==============================] - 6s - loss: 11.6022 - f1_perRow: 0.5678 - f1_perClass: 0.8576 - acc: 0.8461     \n",
      "Epoch 2/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2201 - f1_perRow: 0.5786 - f1_perClass: 0.8604 - acc: 0.8472     \n",
      "Epoch 3/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3462 - f1_perRow: 0.5785 - f1_perClass: 0.8591 - acc: 0.8471     \n",
      "Epoch 4/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5365 - f1_perRow: 0.5712 - f1_perClass: 0.8576 - acc: 0.8463     \n",
      "Epoch 5/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2033 - f1_perRow: 0.5863 - f1_perClass: 0.8602 - acc: 0.8463     \n",
      "Epoch 6/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4254 - f1_perRow: 0.5715 - f1_perClass: 0.8582 - acc: 0.8461     \n",
      "Epoch 7/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2435 - f1_perRow: 0.5784 - f1_perClass: 0.8598 - acc: 0.8459     \n",
      "Epoch 8/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3526 - f1_perRow: 0.5752 - f1_perClass: 0.8591 - acc: 0.8467     \n",
      "Epoch 9/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2876 - f1_perRow: 0.5808 - f1_perClass: 0.8594 - acc: 0.8460     \n",
      "Epoch 10/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3338 - f1_perRow: 0.5813 - f1_perClass: 0.8594 - acc: 0.8465     \n",
      "Epoch 11/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2906 - f1_perRow: 0.5739 - f1_perClass: 0.8596 - acc: 0.8468     \n",
      "Epoch 12/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2755 - f1_perRow: 0.5851 - f1_perClass: 0.8595 - acc: 0.8467     \n",
      "Epoch 13/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3945 - f1_perRow: 0.5802 - f1_perClass: 0.8589 - acc: 0.8472     \n",
      "Epoch 14/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2608 - f1_perRow: 0.5855 - f1_perClass: 0.8600 - acc: 0.8461     \n",
      "Epoch 15/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2316 - f1_perRow: 0.5859 - f1_perClass: 0.8602 - acc: 0.8465     \n",
      "Epoch 16/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2849 - f1_perRow: 0.5819 - f1_perClass: 0.8595 - acc: 0.8466     \n",
      "Epoch 17/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6891 - f1_perRow: 0.5655 - f1_perClass: 0.8566 - acc: 0.8462     \n",
      "Epoch 18/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2508 - f1_perRow: 0.5781 - f1_perClass: 0.8598 - acc: 0.8466     \n",
      "Epoch 19/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4680 - f1_perRow: 0.5734 - f1_perClass: 0.8581 - acc: 0.8470     \n",
      "Epoch 20/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3576 - f1_perRow: 0.5745 - f1_perClass: 0.8588 - acc: 0.8463     \n",
      "Epoch 21/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5036 - f1_perRow: 0.5763 - f1_perClass: 0.8574 - acc: 0.8470     \n",
      "Epoch 22/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3002 - f1_perRow: 0.5765 - f1_perClass: 0.8590 - acc: 0.8468     \n",
      "Epoch 23/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6379 - f1_perRow: 0.5738 - f1_perClass: 0.8561 - acc: 0.8460     \n",
      "Epoch 24/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4969 - f1_perRow: 0.5728 - f1_perClass: 0.8577 - acc: 0.8464     \n",
      "Epoch 25/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5389 - f1_perRow: 0.5717 - f1_perClass: 0.8576 - acc: 0.8465     \n",
      "Epoch 26/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3995 - f1_perRow: 0.5788 - f1_perClass: 0.8584 - acc: 0.8473     \n",
      "Epoch 27/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4097 - f1_perRow: 0.5765 - f1_perClass: 0.8584 - acc: 0.8466     \n",
      "Epoch 28/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.8626 - f1_perRow: 0.5640 - f1_perClass: 0.8547 - acc: 0.8464     \n",
      "Epoch 29/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1410 - f1_perRow: 0.5859 - f1_perClass: 0.8603 - acc: 0.8461     \n",
      "Epoch 30/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2588 - f1_perRow: 0.5854 - f1_perClass: 0.8595 - acc: 0.8467     \n",
      "Epoch 31/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3206 - f1_perRow: 0.5779 - f1_perClass: 0.8589 - acc: 0.8468     \n",
      "Epoch 32/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4669 - f1_perRow: 0.5769 - f1_perClass: 0.8581 - acc: 0.8468     \n",
      "Epoch 33/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3058 - f1_perRow: 0.5863 - f1_perClass: 0.8590 - acc: 0.8466     \n",
      "Epoch 34/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2124 - f1_perRow: 0.5817 - f1_perClass: 0.8601 - acc: 0.8462     \n",
      "Epoch 35/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.7656 - f1_perRow: 0.5707 - f1_perClass: 0.8561 - acc: 0.8472     \n",
      "Epoch 36/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2015 - f1_perRow: 0.5889 - f1_perClass: 0.8605 - acc: 0.8461     \n",
      "Epoch 37/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6807 - f1_perRow: 0.5669 - f1_perClass: 0.8568 - acc: 0.8457     \n",
      "Epoch 38/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3726 - f1_perRow: 0.5769 - f1_perClass: 0.8587 - acc: 0.8465     \n",
      "Epoch 39/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2197 - f1_perRow: 0.5789 - f1_perClass: 0.8601 - acc: 0.8472     \n",
      "Epoch 40/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3391 - f1_perRow: 0.5742 - f1_perClass: 0.8591 - acc: 0.8464     \n",
      "Epoch 41/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3345 - f1_perRow: 0.5824 - f1_perClass: 0.8593 - acc: 0.8475     \n",
      "Epoch 42/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2235 - f1_perRow: 0.5834 - f1_perClass: 0.8600 - acc: 0.8469     \n",
      "Epoch 43/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2784 - f1_perRow: 0.5793 - f1_perClass: 0.8596 - acc: 0.8476     \n",
      "Epoch 44/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2002 - f1_perRow: 0.5818 - f1_perClass: 0.8601 - acc: 0.8460     \n",
      "Epoch 45/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5321 - f1_perRow: 0.5781 - f1_perClass: 0.8575 - acc: 0.8462     \n",
      "Epoch 46/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4162 - f1_perRow: 0.5743 - f1_perClass: 0.8588 - acc: 0.8468     \n",
      "Epoch 47/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3512 - f1_perRow: 0.5792 - f1_perClass: 0.8591 - acc: 0.8465     \n",
      "Epoch 48/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5713 - f1_perRow: 0.5745 - f1_perClass: 0.8570 - acc: 0.8454     \n",
      "Epoch 49/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4010 - f1_perRow: 0.5808 - f1_perClass: 0.8584 - acc: 0.8464     \n",
      "Epoch 50/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3599 - f1_perRow: 0.5845 - f1_perClass: 0.8587 - acc: 0.8460     \n",
      "Epoch 51/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6307 - f1_perRow: 0.5731 - f1_perClass: 0.8566 - acc: 0.8464     \n",
      "Epoch 52/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3517 - f1_perRow: 0.5804 - f1_perClass: 0.8586 - acc: 0.8459     \n",
      "Epoch 53/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3006 - f1_perRow: 0.5753 - f1_perClass: 0.8590 - acc: 0.8461     \n",
      "Epoch 54/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1662 - f1_perRow: 0.5829 - f1_perClass: 0.8602 - acc: 0.8466     \n",
      "Epoch 55/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1581 - f1_perRow: 0.5777 - f1_perClass: 0.8601 - acc: 0.8456     \n",
      "Epoch 56/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3080 - f1_perRow: 0.5799 - f1_perClass: 0.8593 - acc: 0.8464     \n",
      "Epoch 57/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1956 - f1_perRow: 0.5787 - f1_perClass: 0.8605 - acc: 0.8461     \n",
      "Epoch 58/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3003 - f1_perRow: 0.5780 - f1_perClass: 0.8597 - acc: 0.8474     \n",
      "Epoch 59/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 11.3160 - f1_perRow: 0.5862 - f1_perClass: 0.8596 - acc: 0.8467     \n",
      "Epoch 60/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1736 - f1_perRow: 0.5866 - f1_perClass: 0.8606 - acc: 0.8468     \n",
      "Epoch 61/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1865 - f1_perRow: 0.5855 - f1_perClass: 0.8605 - acc: 0.8472     \n",
      "Epoch 62/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2233 - f1_perRow: 0.5845 - f1_perClass: 0.8602 - acc: 0.8468     \n",
      "Epoch 63/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5007 - f1_perRow: 0.5693 - f1_perClass: 0.8579 - acc: 0.8460     \n",
      "Epoch 64/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2869 - f1_perRow: 0.5773 - f1_perClass: 0.8597 - acc: 0.8464     \n",
      "Epoch 65/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2467 - f1_perRow: 0.5856 - f1_perClass: 0.8600 - acc: 0.8467     \n",
      "Epoch 66/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3402 - f1_perRow: 0.5760 - f1_perClass: 0.8591 - acc: 0.8468     \n",
      "Epoch 67/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2511 - f1_perRow: 0.5748 - f1_perClass: 0.8597 - acc: 0.8476     \n",
      "Epoch 68/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2998 - f1_perRow: 0.5790 - f1_perClass: 0.8594 - acc: 0.8469     \n",
      "Epoch 69/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4051 - f1_perRow: 0.5746 - f1_perClass: 0.8585 - acc: 0.8471     \n",
      "Epoch 70/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2348 - f1_perRow: 0.5770 - f1_perClass: 0.8596 - acc: 0.8478     \n",
      "Epoch 71/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2126 - f1_perRow: 0.5848 - f1_perClass: 0.8599 - acc: 0.8474     \n",
      "Epoch 72/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3478 - f1_perRow: 0.5838 - f1_perClass: 0.8588 - acc: 0.8470     \n",
      "Epoch 73/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3203 - f1_perRow: 0.5766 - f1_perClass: 0.8592 - acc: 0.8469     \n",
      "Epoch 74/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3507 - f1_perRow: 0.5783 - f1_perClass: 0.8592 - acc: 0.8474     \n",
      "Epoch 75/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2211 - f1_perRow: 0.5903 - f1_perClass: 0.8603 - acc: 0.8474     \n",
      "Epoch 76/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1489 - f1_perRow: 0.5817 - f1_perClass: 0.8609 - acc: 0.8474     \n",
      "Epoch 77/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1767 - f1_perRow: 0.5912 - f1_perClass: 0.8605 - acc: 0.8465     \n",
      "Epoch 78/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5069 - f1_perRow: 0.5702 - f1_perClass: 0.8580 - acc: 0.8466     \n",
      "Epoch 79/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3141 - f1_perRow: 0.5808 - f1_perClass: 0.8597 - acc: 0.8475     \n",
      "Epoch 80/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2835 - f1_perRow: 0.5813 - f1_perClass: 0.8598 - acc: 0.8475     \n",
      "Epoch 81/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3553 - f1_perRow: 0.5784 - f1_perClass: 0.8586 - acc: 0.8463     \n",
      "Epoch 82/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6413 - f1_perRow: 0.5678 - f1_perClass: 0.8567 - acc: 0.8466     \n",
      "Epoch 83/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2731 - f1_perRow: 0.5837 - f1_perClass: 0.8599 - acc: 0.8473     \n",
      "Epoch 84/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3453 - f1_perRow: 0.5819 - f1_perClass: 0.8591 - acc: 0.8464     \n",
      "Epoch 85/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4181 - f1_perRow: 0.5770 - f1_perClass: 0.8586 - acc: 0.8456     \n",
      "Epoch 86/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.7289 - f1_perRow: 0.5657 - f1_perClass: 0.8564 - acc: 0.8459     \n",
      "Epoch 87/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3494 - f1_perRow: 0.5802 - f1_perClass: 0.8593 - acc: 0.8464     \n",
      "Epoch 88/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2082 - f1_perRow: 0.5802 - f1_perClass: 0.8603 - acc: 0.8468     \n",
      "Epoch 89/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2263 - f1_perRow: 0.5836 - f1_perClass: 0.8600 - acc: 0.8468     \n",
      "Epoch 90/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2736 - f1_perRow: 0.5832 - f1_perClass: 0.8600 - acc: 0.8467     \n",
      "Epoch 91/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2611 - f1_perRow: 0.5875 - f1_perClass: 0.8600 - acc: 0.8468     \n",
      "Epoch 92/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2231 - f1_perRow: 0.5810 - f1_perClass: 0.8607 - acc: 0.8471     \n",
      "Epoch 93/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4543 - f1_perRow: 0.5823 - f1_perClass: 0.8584 - acc: 0.8461     \n",
      "Epoch 94/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2468 - f1_perRow: 0.5800 - f1_perClass: 0.8599 - acc: 0.8477     \n",
      "Epoch 95/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5381 - f1_perRow: 0.5812 - f1_perClass: 0.8576 - acc: 0.8463     \n",
      "Epoch 96/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2336 - f1_perRow: 0.5775 - f1_perClass: 0.8603 - acc: 0.8480     \n",
      "Epoch 97/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2995 - f1_perRow: 0.5850 - f1_perClass: 0.8597 - acc: 0.8475     \n",
      "Epoch 98/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3873 - f1_perRow: 0.5769 - f1_perClass: 0.8587 - acc: 0.8464     \n",
      "Epoch 99/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4005 - f1_perRow: 0.5746 - f1_perClass: 0.8585 - acc: 0.8473     \n",
      "Epoch 100/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6747 - f1_perRow: 0.5679 - f1_perClass: 0.8570 - acc: 0.8467     \n",
      "Epoch 101/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2995 - f1_perRow: 0.5822 - f1_perClass: 0.8597 - acc: 0.8467     \n",
      "Epoch 102/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3103 - f1_perRow: 0.5833 - f1_perClass: 0.8593 - acc: 0.8470     \n",
      "Epoch 103/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4801 - f1_perRow: 0.5723 - f1_perClass: 0.8587 - acc: 0.8469     \n",
      "Epoch 104/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6601 - f1_perRow: 0.5711 - f1_perClass: 0.8567 - acc: 0.8466     \n",
      "Epoch 105/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1283 - f1_perRow: 0.5910 - f1_perClass: 0.8606 - acc: 0.8474     \n",
      "Epoch 106/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3692 - f1_perRow: 0.5801 - f1_perClass: 0.8590 - acc: 0.8467     \n",
      "Epoch 107/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3845 - f1_perRow: 0.5840 - f1_perClass: 0.8590 - acc: 0.8472     \n",
      "Epoch 108/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3014 - f1_perRow: 0.5850 - f1_perClass: 0.8596 - acc: 0.8468     \n",
      "Epoch 109/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3000 - f1_perRow: 0.5866 - f1_perClass: 0.8595 - acc: 0.8466     \n",
      "Epoch 110/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3346 - f1_perRow: 0.5820 - f1_perClass: 0.8586 - acc: 0.8462     \n",
      "Epoch 111/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6441 - f1_perRow: 0.5700 - f1_perClass: 0.8559 - acc: 0.8468     \n",
      "Epoch 112/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1857 - f1_perRow: 0.5833 - f1_perClass: 0.8601 - acc: 0.8473     \n",
      "Epoch 113/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1271 - f1_perRow: 0.5855 - f1_perClass: 0.8604 - acc: 0.8472     \n",
      "Epoch 114/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4213 - f1_perRow: 0.5812 - f1_perClass: 0.8585 - acc: 0.8475     \n",
      "Epoch 115/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3401 - f1_perRow: 0.5771 - f1_perClass: 0.8590 - acc: 0.8470     \n",
      "Epoch 116/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2388 - f1_perRow: 0.5847 - f1_perClass: 0.8601 - acc: 0.8470     \n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 11.4826 - f1_perRow: 0.5755 - f1_perClass: 0.8580 - acc: 0.8465     \n",
      "Epoch 118/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1616 - f1_perRow: 0.5846 - f1_perClass: 0.8607 - acc: 0.8469     \n",
      "Epoch 119/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3410 - f1_perRow: 0.5810 - f1_perClass: 0.8595 - acc: 0.8469     \n",
      "Epoch 120/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2857 - f1_perRow: 0.5804 - f1_perClass: 0.8599 - acc: 0.8466     \n",
      "Epoch 121/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1310 - f1_perRow: 0.5860 - f1_perClass: 0.8609 - acc: 0.8464     \n",
      "Epoch 122/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2573 - f1_perRow: 0.5828 - f1_perClass: 0.8601 - acc: 0.8478     \n",
      "Epoch 123/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1093 - f1_perRow: 0.5834 - f1_perClass: 0.8611 - acc: 0.8483     \n",
      "Epoch 124/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1768 - f1_perRow: 0.5847 - f1_perClass: 0.8606 - acc: 0.8476     \n",
      "Epoch 125/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5925 - f1_perRow: 0.5763 - f1_perClass: 0.8574 - acc: 0.8483     \n",
      "Epoch 126/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1545 - f1_perRow: 0.5850 - f1_perClass: 0.8608 - acc: 0.8477     \n",
      "Epoch 127/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5006 - f1_perRow: 0.5760 - f1_perClass: 0.8579 - acc: 0.8469     \n",
      "Epoch 128/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1285 - f1_perRow: 0.5945 - f1_perClass: 0.8609 - acc: 0.8471     \n",
      "Epoch 129/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5488 - f1_perRow: 0.5679 - f1_perClass: 0.8571 - acc: 0.8461     \n",
      "Epoch 130/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1417 - f1_perRow: 0.5913 - f1_perClass: 0.8608 - acc: 0.8467     \n",
      "Epoch 131/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4151 - f1_perRow: 0.5810 - f1_perClass: 0.8588 - acc: 0.8471     \n",
      "Epoch 132/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2219 - f1_perRow: 0.5851 - f1_perClass: 0.8604 - acc: 0.8478     \n",
      "Epoch 133/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3980 - f1_perRow: 0.5830 - f1_perClass: 0.8588 - acc: 0.8468     \n",
      "Epoch 134/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4189 - f1_perRow: 0.5847 - f1_perClass: 0.8588 - acc: 0.8472     \n",
      "Epoch 135/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1382 - f1_perRow: 0.5894 - f1_perClass: 0.8608 - acc: 0.8476     \n",
      "Epoch 136/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4580 - f1_perRow: 0.5782 - f1_perClass: 0.8583 - acc: 0.8467     \n",
      "Epoch 137/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6472 - f1_perRow: 0.5752 - f1_perClass: 0.8567 - acc: 0.8473     \n",
      "Epoch 138/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1239 - f1_perRow: 0.5894 - f1_perClass: 0.8608 - acc: 0.8474     \n",
      "Epoch 139/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1866 - f1_perRow: 0.5813 - f1_perClass: 0.8603 - acc: 0.8479     \n",
      "Epoch 140/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2194 - f1_perRow: 0.5817 - f1_perClass: 0.8598 - acc: 0.8472     \n",
      "Epoch 141/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3944 - f1_perRow: 0.5817 - f1_perClass: 0.8586 - acc: 0.8467     \n",
      "Epoch 142/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2411 - f1_perRow: 0.5796 - f1_perClass: 0.8602 - acc: 0.8477     \n",
      "Epoch 143/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2277 - f1_perRow: 0.5848 - f1_perClass: 0.8604 - acc: 0.8476     \n",
      "Epoch 144/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3837 - f1_perRow: 0.5829 - f1_perClass: 0.8586 - acc: 0.8477     \n",
      "Epoch 145/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4510 - f1_perRow: 0.5726 - f1_perClass: 0.8580 - acc: 0.8473     \n",
      "Epoch 146/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5783 - f1_perRow: 0.5712 - f1_perClass: 0.8572 - acc: 0.8464     \n",
      "Epoch 147/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3157 - f1_perRow: 0.5832 - f1_perClass: 0.8593 - acc: 0.8468     \n",
      "Epoch 148/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3823 - f1_perRow: 0.5857 - f1_perClass: 0.8591 - acc: 0.8479     \n",
      "Epoch 149/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3669 - f1_perRow: 0.5839 - f1_perClass: 0.8591 - acc: 0.8472     \n",
      "Epoch 150/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2722 - f1_perRow: 0.5750 - f1_perClass: 0.8598 - acc: 0.8474     \n",
      "Epoch 151/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.7034 - f1_perRow: 0.5743 - f1_perClass: 0.8563 - acc: 0.8476     \n",
      "Epoch 152/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1764 - f1_perRow: 0.5835 - f1_perClass: 0.8604 - acc: 0.8474     \n",
      "Epoch 153/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3317 - f1_perRow: 0.5768 - f1_perClass: 0.8595 - acc: 0.8471     \n",
      "Epoch 154/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1290 - f1_perRow: 0.5885 - f1_perClass: 0.8608 - acc: 0.8473     \n",
      "Epoch 155/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4842 - f1_perRow: 0.5787 - f1_perClass: 0.8583 - acc: 0.8473     \n",
      "Epoch 156/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4132 - f1_perRow: 0.5764 - f1_perClass: 0.8586 - acc: 0.8474     \n",
      "Epoch 157/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4214 - f1_perRow: 0.5766 - f1_perClass: 0.8586 - acc: 0.8472     \n",
      "Epoch 158/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.7729 - f1_perRow: 0.5717 - f1_perClass: 0.8557 - acc: 0.8473     \n",
      "Epoch 159/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2613 - f1_perRow: 0.5844 - f1_perClass: 0.8596 - acc: 0.8463     \n",
      "Epoch 160/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2580 - f1_perRow: 0.5830 - f1_perClass: 0.8598 - acc: 0.8470     \n",
      "Epoch 161/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4542 - f1_perRow: 0.5810 - f1_perClass: 0.8584 - acc: 0.8474     \n",
      "Epoch 162/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3911 - f1_perRow: 0.5714 - f1_perClass: 0.8588 - acc: 0.8466     \n",
      "Epoch 163/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6551 - f1_perRow: 0.5741 - f1_perClass: 0.8570 - acc: 0.8469     \n",
      "Epoch 164/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2903 - f1_perRow: 0.5849 - f1_perClass: 0.8597 - acc: 0.8469     \n",
      "Epoch 165/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1005 - f1_perRow: 0.5887 - f1_perClass: 0.8612 - acc: 0.8482     \n",
      "Epoch 166/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2207 - f1_perRow: 0.5797 - f1_perClass: 0.8602 - acc: 0.8475     \n",
      "Epoch 167/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4764 - f1_perRow: 0.5783 - f1_perClass: 0.8581 - acc: 0.8467     \n",
      "Epoch 168/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2379 - f1_perRow: 0.5785 - f1_perClass: 0.8601 - acc: 0.8471     \n",
      "Epoch 169/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1800 - f1_perRow: 0.5934 - f1_perClass: 0.8605 - acc: 0.8478     \n",
      "Epoch 170/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2935 - f1_perRow: 0.5806 - f1_perClass: 0.8600 - acc: 0.8480     \n",
      "Epoch 171/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.0919 - f1_perRow: 0.5885 - f1_perClass: 0.8612 - acc: 0.8475     \n",
      "Epoch 172/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3055 - f1_perRow: 0.5818 - f1_perClass: 0.8596 - acc: 0.8466     \n",
      "Epoch 173/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2545 - f1_perRow: 0.5764 - f1_perClass: 0.8599 - acc: 0.8475     \n",
      "Epoch 174/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3192 - f1_perRow: 0.5756 - f1_perClass: 0.8592 - acc: 0.8473     \n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 11.4461 - f1_perRow: 0.5749 - f1_perClass: 0.8588 - acc: 0.8471     \n",
      "Epoch 176/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2843 - f1_perRow: 0.5766 - f1_perClass: 0.8599 - acc: 0.8472     \n",
      "Epoch 177/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3639 - f1_perRow: 0.5820 - f1_perClass: 0.8593 - acc: 0.8477     \n",
      "Epoch 178/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3022 - f1_perRow: 0.5838 - f1_perClass: 0.8597 - acc: 0.8478     \n",
      "Epoch 179/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3261 - f1_perRow: 0.5848 - f1_perClass: 0.8598 - acc: 0.8483     \n",
      "Epoch 180/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3033 - f1_perRow: 0.5778 - f1_perClass: 0.8597 - acc: 0.8469     \n",
      "Epoch 181/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2629 - f1_perRow: 0.5845 - f1_perClass: 0.8597 - acc: 0.8474     \n",
      "Epoch 182/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2340 - f1_perRow: 0.5765 - f1_perClass: 0.8601 - acc: 0.8474     \n",
      "Epoch 183/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3610 - f1_perRow: 0.5862 - f1_perClass: 0.8594 - acc: 0.8477     \n",
      "Epoch 184/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5518 - f1_perRow: 0.5783 - f1_perClass: 0.8575 - acc: 0.8479     \n",
      "Epoch 185/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2178 - f1_perRow: 0.5753 - f1_perClass: 0.8603 - acc: 0.8477     \n",
      "Epoch 186/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5009 - f1_perRow: 0.5780 - f1_perClass: 0.8577 - acc: 0.8475     \n",
      "Epoch 187/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3625 - f1_perRow: 0.5837 - f1_perClass: 0.8590 - acc: 0.8472     \n",
      "Epoch 188/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6686 - f1_perRow: 0.5684 - f1_perClass: 0.8567 - acc: 0.8467     \n",
      "Epoch 189/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1666 - f1_perRow: 0.5878 - f1_perClass: 0.8604 - acc: 0.8485     \n",
      "Epoch 190/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1195 - f1_perRow: 0.5892 - f1_perClass: 0.8607 - acc: 0.8477     \n",
      "Epoch 191/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2347 - f1_perRow: 0.5840 - f1_perClass: 0.8601 - acc: 0.8477     \n",
      "Epoch 192/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1461 - f1_perRow: 0.5822 - f1_perClass: 0.8608 - acc: 0.8475     \n",
      "Epoch 193/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3521 - f1_perRow: 0.5788 - f1_perClass: 0.8597 - acc: 0.8472     \n",
      "Epoch 194/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2136 - f1_perRow: 0.5885 - f1_perClass: 0.8609 - acc: 0.8480     \n",
      "Epoch 195/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2933 - f1_perRow: 0.5846 - f1_perClass: 0.8596 - acc: 0.8481     \n",
      "Epoch 196/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1974 - f1_perRow: 0.5846 - f1_perClass: 0.8608 - acc: 0.8478     \n",
      "Epoch 197/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4113 - f1_perRow: 0.5807 - f1_perClass: 0.8592 - acc: 0.8480     \n",
      "Epoch 198/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2527 - f1_perRow: 0.5913 - f1_perClass: 0.8606 - acc: 0.8468     \n",
      "Epoch 199/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2085 - f1_perRow: 0.5882 - f1_perClass: 0.8609 - acc: 0.8473     \n",
      "Epoch 200/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2232 - f1_perRow: 0.5806 - f1_perClass: 0.8607 - acc: 0.8470     \n",
      "Epoch 201/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5676 - f1_perRow: 0.5817 - f1_perClass: 0.8576 - acc: 0.8476     \n",
      "Epoch 202/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1880 - f1_perRow: 0.5866 - f1_perClass: 0.8610 - acc: 0.8480     \n",
      "Epoch 203/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5407 - f1_perRow: 0.5760 - f1_perClass: 0.8576 - acc: 0.8465     \n",
      "Epoch 204/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2862 - f1_perRow: 0.5867 - f1_perClass: 0.8600 - acc: 0.8471     \n",
      "Epoch 205/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3484 - f1_perRow: 0.5782 - f1_perClass: 0.8593 - acc: 0.8472     \n",
      "Epoch 206/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1185 - f1_perRow: 0.5904 - f1_perClass: 0.8615 - acc: 0.8468     \n",
      "Epoch 207/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3090 - f1_perRow: 0.5780 - f1_perClass: 0.8601 - acc: 0.8470     \n",
      "Epoch 208/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3843 - f1_perRow: 0.5815 - f1_perClass: 0.8597 - acc: 0.8476     \n",
      "Epoch 209/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5501 - f1_perRow: 0.5731 - f1_perClass: 0.8581 - acc: 0.8470     \n",
      "Epoch 210/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3995 - f1_perRow: 0.5803 - f1_perClass: 0.8591 - acc: 0.8479     \n",
      "Epoch 211/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4644 - f1_perRow: 0.5714 - f1_perClass: 0.8586 - acc: 0.8474     \n",
      "Epoch 212/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2502 - f1_perRow: 0.5879 - f1_perClass: 0.8603 - acc: 0.8478     \n",
      "Epoch 213/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6282 - f1_perRow: 0.5762 - f1_perClass: 0.8575 - acc: 0.8471     \n",
      "Epoch 214/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4285 - f1_perRow: 0.5796 - f1_perClass: 0.8592 - acc: 0.8478     \n",
      "Epoch 215/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4010 - f1_perRow: 0.5814 - f1_perClass: 0.8592 - acc: 0.8481     \n",
      "Epoch 216/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3349 - f1_perRow: 0.5746 - f1_perClass: 0.8593 - acc: 0.8470     \n",
      "Epoch 217/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4638 - f1_perRow: 0.5828 - f1_perClass: 0.8588 - acc: 0.8470     \n",
      "Epoch 218/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5256 - f1_perRow: 0.5745 - f1_perClass: 0.8581 - acc: 0.8473     \n",
      "Epoch 219/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4969 - f1_perRow: 0.5713 - f1_perClass: 0.8586 - acc: 0.8476     \n",
      "Epoch 220/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3130 - f1_perRow: 0.5838 - f1_perClass: 0.8599 - acc: 0.8480     \n",
      "Epoch 221/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3019 - f1_perRow: 0.5801 - f1_perClass: 0.8599 - acc: 0.8473     \n",
      "Epoch 222/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2909 - f1_perRow: 0.5810 - f1_perClass: 0.8601 - acc: 0.8475     \n",
      "Epoch 223/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2975 - f1_perRow: 0.5813 - f1_perClass: 0.8600 - acc: 0.8478     \n",
      "Epoch 224/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3255 - f1_perRow: 0.5876 - f1_perClass: 0.8603 - acc: 0.8473     \n",
      "Epoch 225/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3570 - f1_perRow: 0.5821 - f1_perClass: 0.8599 - acc: 0.8472     \n",
      "Epoch 226/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.7500 - f1_perRow: 0.5739 - f1_perClass: 0.8564 - acc: 0.8469     \n",
      "Epoch 227/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4405 - f1_perRow: 0.5776 - f1_perClass: 0.8587 - acc: 0.8480     \n",
      "Epoch 228/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2362 - f1_perRow: 0.5779 - f1_perClass: 0.8598 - acc: 0.8478     \n",
      "Epoch 229/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4896 - f1_perRow: 0.5725 - f1_perClass: 0.8576 - acc: 0.8478     \n",
      "Epoch 230/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3461 - f1_perRow: 0.5731 - f1_perClass: 0.8589 - acc: 0.8480     \n",
      "Epoch 231/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2527 - f1_perRow: 0.5832 - f1_perClass: 0.8596 - acc: 0.8461     \n",
      "Epoch 232/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2895 - f1_perRow: 0.5797 - f1_perClass: 0.8597 - acc: 0.8479     \n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 11.3906 - f1_perRow: 0.5813 - f1_perClass: 0.8590 - acc: 0.8474     \n",
      "Epoch 234/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2835 - f1_perRow: 0.5805 - f1_perClass: 0.8599 - acc: 0.8479     \n",
      "Epoch 235/300\n",
      "65266/65266 [==============================] - 3s - loss: 12.1028 - f1_perRow: 0.5612 - f1_perClass: 0.8545 - acc: 0.8467     \n",
      "Epoch 236/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4233 - f1_perRow: 0.5841 - f1_perClass: 0.8590 - acc: 0.8479     \n",
      "Epoch 237/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3429 - f1_perRow: 0.5815 - f1_perClass: 0.8591 - acc: 0.8471     \n",
      "Epoch 238/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2397 - f1_perRow: 0.5781 - f1_perClass: 0.8602 - acc: 0.8479     \n",
      "Epoch 239/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1851 - f1_perRow: 0.5896 - f1_perClass: 0.8607 - acc: 0.8475     \n",
      "Epoch 240/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6537 - f1_perRow: 0.5701 - f1_perClass: 0.8573 - acc: 0.8473     \n",
      "Epoch 241/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4824 - f1_perRow: 0.5793 - f1_perClass: 0.8586 - acc: 0.8474     \n",
      "Epoch 242/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4038 - f1_perRow: 0.5737 - f1_perClass: 0.8590 - acc: 0.8471     \n",
      "Epoch 243/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3398 - f1_perRow: 0.5820 - f1_perClass: 0.8594 - acc: 0.8476     \n",
      "Epoch 244/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.8859 - f1_perRow: 0.5609 - f1_perClass: 0.8557 - acc: 0.8464     \n",
      "Epoch 245/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1705 - f1_perRow: 0.5881 - f1_perClass: 0.8605 - acc: 0.8477     \n",
      "Epoch 246/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2072 - f1_perRow: 0.5813 - f1_perClass: 0.8601 - acc: 0.8480     \n",
      "Epoch 247/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2856 - f1_perRow: 0.5894 - f1_perClass: 0.8598 - acc: 0.8474     \n",
      "Epoch 248/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3632 - f1_perRow: 0.5836 - f1_perClass: 0.8592 - acc: 0.8471     \n",
      "Epoch 249/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1719 - f1_perRow: 0.5827 - f1_perClass: 0.8608 - acc: 0.8473     \n",
      "Epoch 250/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4863 - f1_perRow: 0.5748 - f1_perClass: 0.8582 - acc: 0.8465     \n",
      "Epoch 251/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4591 - f1_perRow: 0.5675 - f1_perClass: 0.8585 - acc: 0.8479     \n",
      "Epoch 252/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3919 - f1_perRow: 0.5755 - f1_perClass: 0.8591 - acc: 0.8471     \n",
      "Epoch 253/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3673 - f1_perRow: 0.5770 - f1_perClass: 0.8593 - acc: 0.8470     \n",
      "Epoch 254/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3751 - f1_perRow: 0.5767 - f1_perClass: 0.8591 - acc: 0.8474     \n",
      "Epoch 255/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2902 - f1_perRow: 0.5837 - f1_perClass: 0.8597 - acc: 0.8479     \n",
      "Epoch 256/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3123 - f1_perRow: 0.5826 - f1_perClass: 0.8596 - acc: 0.8476     \n",
      "Epoch 257/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5141 - f1_perRow: 0.5804 - f1_perClass: 0.8587 - acc: 0.8481     \n",
      "Epoch 258/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4390 - f1_perRow: 0.5747 - f1_perClass: 0.8589 - acc: 0.8471     \n",
      "Epoch 259/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2030 - f1_perRow: 0.5806 - f1_perClass: 0.8605 - acc: 0.8469     \n",
      "Epoch 260/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3166 - f1_perRow: 0.5799 - f1_perClass: 0.8593 - acc: 0.8480     \n",
      "Epoch 261/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3089 - f1_perRow: 0.5850 - f1_perClass: 0.8594 - acc: 0.8478     \n",
      "Epoch 262/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1813 - f1_perRow: 0.5834 - f1_perClass: 0.8603 - acc: 0.8473     \n",
      "Epoch 263/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2617 - f1_perRow: 0.5853 - f1_perClass: 0.8599 - acc: 0.8478     \n",
      "Epoch 264/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2473 - f1_perRow: 0.5876 - f1_perClass: 0.8602 - acc: 0.8486     \n",
      "Epoch 265/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2268 - f1_perRow: 0.5788 - f1_perClass: 0.8604 - acc: 0.8475     \n",
      "Epoch 266/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3658 - f1_perRow: 0.5785 - f1_perClass: 0.8597 - acc: 0.8475     \n",
      "Epoch 267/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3660 - f1_perRow: 0.5795 - f1_perClass: 0.8595 - acc: 0.8473     \n",
      "Epoch 268/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3741 - f1_perRow: 0.5853 - f1_perClass: 0.8593 - acc: 0.8474     \n",
      "Epoch 269/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4221 - f1_perRow: 0.5771 - f1_perClass: 0.8590 - acc: 0.8469     \n",
      "Epoch 270/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1843 - f1_perRow: 0.5806 - f1_perClass: 0.8607 - acc: 0.8481     \n",
      "Epoch 271/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1538 - f1_perRow: 0.5861 - f1_perClass: 0.8609 - acc: 0.8484     \n",
      "Epoch 272/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.0614 - f1_perRow: 0.5861 - f1_perClass: 0.8618 - acc: 0.8484     \n",
      "Epoch 273/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3629 - f1_perRow: 0.5807 - f1_perClass: 0.8597 - acc: 0.8479     \n",
      "Epoch 274/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2114 - f1_perRow: 0.5838 - f1_perClass: 0.8605 - acc: 0.8483     \n",
      "Epoch 275/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5368 - f1_perRow: 0.5729 - f1_perClass: 0.8581 - acc: 0.8477     \n",
      "Epoch 276/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2300 - f1_perRow: 0.5860 - f1_perClass: 0.8605 - acc: 0.8483     \n",
      "Epoch 277/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3138 - f1_perRow: 0.5821 - f1_perClass: 0.8597 - acc: 0.8479     \n",
      "Epoch 278/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1101 - f1_perRow: 0.5872 - f1_perClass: 0.8615 - acc: 0.8478     \n",
      "Epoch 279/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2658 - f1_perRow: 0.5875 - f1_perClass: 0.8606 - acc: 0.8483     \n",
      "Epoch 280/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4277 - f1_perRow: 0.5740 - f1_perClass: 0.8590 - acc: 0.8473     \n",
      "Epoch 281/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5155 - f1_perRow: 0.5743 - f1_perClass: 0.8584 - acc: 0.8478     \n",
      "Epoch 282/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3515 - f1_perRow: 0.5848 - f1_perClass: 0.8597 - acc: 0.8471     \n",
      "Epoch 283/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2721 - f1_perRow: 0.5826 - f1_perClass: 0.8603 - acc: 0.8476     \n",
      "Epoch 284/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.5721 - f1_perRow: 0.5785 - f1_perClass: 0.8581 - acc: 0.8470     \n",
      "Epoch 285/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3111 - f1_perRow: 0.5842 - f1_perClass: 0.8604 - acc: 0.8484     \n",
      "Epoch 286/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3198 - f1_perRow: 0.5771 - f1_perClass: 0.8602 - acc: 0.8474     \n",
      "Epoch 287/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1743 - f1_perRow: 0.5893 - f1_perClass: 0.8610 - acc: 0.8472     \n",
      "Epoch 288/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1911 - f1_perRow: 0.5809 - f1_perClass: 0.8608 - acc: 0.8472     \n",
      "Epoch 289/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4649 - f1_perRow: 0.5578 - f1_perClass: 0.8582 - acc: 0.8474     \n",
      "Epoch 290/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.1229 - f1_perRow: 0.5864 - f1_perClass: 0.8611 - acc: 0.8486     \n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 11.3208 - f1_perRow: 0.5708 - f1_perClass: 0.8598 - acc: 0.8468     \n",
      "Epoch 292/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4094 - f1_perRow: 0.5800 - f1_perClass: 0.8593 - acc: 0.8472     \n",
      "Epoch 293/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3501 - f1_perRow: 0.5807 - f1_perClass: 0.8600 - acc: 0.8479     \n",
      "Epoch 294/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4239 - f1_perRow: 0.5808 - f1_perClass: 0.8588 - acc: 0.8470     \n",
      "Epoch 295/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.3322 - f1_perRow: 0.5757 - f1_perClass: 0.8600 - acc: 0.8480     \n",
      "Epoch 296/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4332 - f1_perRow: 0.5789 - f1_perClass: 0.8591 - acc: 0.8472     \n",
      "Epoch 297/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.6220 - f1_perRow: 0.5723 - f1_perClass: 0.8578 - acc: 0.8471     \n",
      "Epoch 298/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.4485 - f1_perRow: 0.5843 - f1_perClass: 0.8588 - acc: 0.8473     \n",
      "Epoch 299/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2113 - f1_perRow: 0.5830 - f1_perClass: 0.8605 - acc: 0.8477     \n",
      "Epoch 300/300\n",
      "65266/65266 [==============================] - 3s - loss: 11.2878 - f1_perRow: 0.5769 - f1_perClass: 0.8601 - acc: 0.8481     \n"
     ]
    }
   ],
   "source": [
    "# model2.compile(loss=losses,loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=[fp_rate,f1_perRow,f1_perClass,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=11500, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3300\n",
      "65266/65266 [==============================] - 7s - loss: 0.5572 - f1_perRow: 0.5908 - f1_perClass: 0.8610 - acc: 0.8483     \n",
      "Epoch 2/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5691 - f1_perRow: 0.5773 - f1_perClass: 0.8590 - acc: 0.8483     \n",
      "Epoch 3/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5863 - f1_perClass: 0.8604 - acc: 0.8482     \n",
      "Epoch 4/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5688 - f1_perRow: 0.5873 - f1_perClass: 0.8595 - acc: 0.8478     \n",
      "Epoch 5/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5887 - f1_perClass: 0.8602 - acc: 0.8484     \n",
      "Epoch 6/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5886 - f1_perRow: 0.5705 - f1_perClass: 0.8564 - acc: 0.8482     \n",
      "Epoch 7/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5617 - f1_perRow: 0.5813 - f1_perClass: 0.8606 - acc: 0.8483     \n",
      "Epoch 8/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5571 - f1_perRow: 0.5842 - f1_perClass: 0.8611 - acc: 0.8482     \n",
      "Epoch 9/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5623 - f1_perRow: 0.5801 - f1_perClass: 0.8604 - acc: 0.8483     \n",
      "Epoch 10/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5587 - f1_perRow: 0.5837 - f1_perClass: 0.8607 - acc: 0.8477     \n",
      "Epoch 11/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5651 - f1_perRow: 0.5894 - f1_perClass: 0.8600 - acc: 0.8479     \n",
      "Epoch 12/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5753 - f1_perRow: 0.5752 - f1_perClass: 0.8582 - acc: 0.8480     \n",
      "Epoch 13/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5630 - f1_perRow: 0.5820 - f1_perClass: 0.8603 - acc: 0.8481     \n",
      "Epoch 14/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5591 - f1_perRow: 0.5871 - f1_perClass: 0.8607 - acc: 0.8479     \n",
      "Epoch 15/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5715 - f1_perRow: 0.5763 - f1_perClass: 0.8592 - acc: 0.8478     \n",
      "Epoch 16/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5669 - f1_perRow: 0.5817 - f1_perClass: 0.8596 - acc: 0.8475     \n",
      "Epoch 17/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5864 - f1_perClass: 0.8603 - acc: 0.8480     \n",
      "Epoch 18/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5550 - f1_perRow: 0.5905 - f1_perClass: 0.8615 - acc: 0.8484     \n",
      "Epoch 19/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5594 - f1_perRow: 0.5839 - f1_perClass: 0.8606 - acc: 0.8477     \n",
      "Epoch 20/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5719 - f1_perRow: 0.5746 - f1_perClass: 0.8592 - acc: 0.8476     \n",
      "Epoch 21/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5775 - f1_perRow: 0.5760 - f1_perClass: 0.8579 - acc: 0.8475     \n",
      "Epoch 22/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5601 - f1_perRow: 0.5901 - f1_perClass: 0.8606 - acc: 0.8476     \n",
      "Epoch 23/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5641 - f1_perRow: 0.5812 - f1_perClass: 0.8600 - acc: 0.8480     \n",
      "Epoch 24/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5785 - f1_perRow: 0.5729 - f1_perClass: 0.8575 - acc: 0.8466     \n",
      "Epoch 25/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5674 - f1_perRow: 0.5778 - f1_perClass: 0.8592 - acc: 0.8474     \n",
      "Epoch 26/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5690 - f1_perRow: 0.5781 - f1_perClass: 0.8591 - acc: 0.8472     \n",
      "Epoch 27/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5715 - f1_perRow: 0.5779 - f1_perClass: 0.8587 - acc: 0.8476     \n",
      "Epoch 28/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5587 - f1_perRow: 0.5890 - f1_perClass: 0.8607 - acc: 0.8474     \n",
      "Epoch 29/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5652 - f1_perRow: 0.5811 - f1_perClass: 0.8597 - acc: 0.8489     \n",
      "Epoch 30/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5589 - f1_perRow: 0.5895 - f1_perClass: 0.8607 - acc: 0.8485     \n",
      "Epoch 31/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5585 - f1_perRow: 0.5881 - f1_perClass: 0.8606 - acc: 0.8475     \n",
      "Epoch 32/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5714 - f1_perRow: 0.5700 - f1_perClass: 0.8589 - acc: 0.8475     \n",
      "Epoch 33/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5580 - f1_perRow: 0.5822 - f1_perClass: 0.8612 - acc: 0.8487     \n",
      "Epoch 34/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5695 - f1_perRow: 0.5818 - f1_perClass: 0.8593 - acc: 0.8487     \n",
      "Epoch 35/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5669 - f1_perRow: 0.5725 - f1_perClass: 0.8594 - acc: 0.8474     \n",
      "Epoch 36/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5709 - f1_perRow: 0.5810 - f1_perClass: 0.8591 - acc: 0.8488     \n",
      "Epoch 37/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5614 - f1_perRow: 0.5905 - f1_perClass: 0.8602 - acc: 0.8481     \n",
      "Epoch 38/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5596 - f1_perRow: 0.5827 - f1_perClass: 0.8607 - acc: 0.8488     \n",
      "Epoch 39/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5643 - f1_perRow: 0.5830 - f1_perClass: 0.8599 - acc: 0.8475     \n",
      "Epoch 40/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5674 - f1_perRow: 0.5827 - f1_perClass: 0.8597 - acc: 0.8486     \n",
      "Epoch 41/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5653 - f1_perRow: 0.5862 - f1_perClass: 0.8599 - acc: 0.8477     \n",
      "Epoch 42/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5879 - f1_perClass: 0.8604 - acc: 0.8485     \n",
      "Epoch 43/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5640 - f1_perRow: 0.5931 - f1_perClass: 0.8600 - acc: 0.8480     \n",
      "Epoch 44/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5708 - f1_perRow: 0.5745 - f1_perClass: 0.8591 - acc: 0.8480     \n",
      "Epoch 45/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5567 - f1_perRow: 0.5905 - f1_perClass: 0.8612 - acc: 0.8479     \n",
      "Epoch 46/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5651 - f1_perRow: 0.5797 - f1_perClass: 0.8600 - acc: 0.8489     \n",
      "Epoch 47/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5728 - f1_perRow: 0.5815 - f1_perClass: 0.8587 - acc: 0.8470     \n",
      "Epoch 48/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5567 - f1_perRow: 0.5863 - f1_perClass: 0.8611 - acc: 0.8475     \n",
      "Epoch 49/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5668 - f1_perRow: 0.5855 - f1_perClass: 0.8594 - acc: 0.8479     \n",
      "Epoch 50/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5822 - f1_perClass: 0.8606 - acc: 0.8472     \n",
      "Epoch 51/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5584 - f1_perRow: 0.5895 - f1_perClass: 0.8608 - acc: 0.8487     \n",
      "Epoch 52/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5573 - f1_perRow: 0.5825 - f1_perClass: 0.8610 - acc: 0.8474     \n",
      "Epoch 53/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5569 - f1_perRow: 0.5936 - f1_perClass: 0.8613 - acc: 0.8482     \n",
      "Epoch 54/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5766 - f1_perRow: 0.5769 - f1_perClass: 0.8582 - acc: 0.8477     \n",
      "Epoch 55/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5666 - f1_perRow: 0.5765 - f1_perClass: 0.8595 - acc: 0.8484     \n",
      "Epoch 56/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5584 - f1_perRow: 0.5868 - f1_perClass: 0.8607 - acc: 0.8485     \n",
      "Epoch 57/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5600 - f1_perRow: 0.5858 - f1_perClass: 0.8606 - acc: 0.8481     \n",
      "Epoch 58/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5782 - f1_perRow: 0.5764 - f1_perClass: 0.8575 - acc: 0.8475     \n",
      "Epoch 59/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5776 - f1_perRow: 0.5699 - f1_perClass: 0.8578 - acc: 0.8475     \n",
      "Epoch 60/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5701 - f1_perRow: 0.5768 - f1_perClass: 0.8594 - acc: 0.8487     \n",
      "Epoch 61/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5750 - f1_perRow: 0.5767 - f1_perClass: 0.8585 - acc: 0.8476     \n",
      "Epoch 62/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5845 - f1_perClass: 0.8603 - acc: 0.8478     \n",
      "Epoch 63/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5688 - f1_perRow: 0.5841 - f1_perClass: 0.8591 - acc: 0.8476     \n",
      "Epoch 64/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5666 - f1_perRow: 0.5803 - f1_perClass: 0.8597 - acc: 0.8466     \n",
      "Epoch 65/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5709 - f1_perRow: 0.5796 - f1_perClass: 0.8591 - acc: 0.8479     \n",
      "Epoch 66/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5626 - f1_perRow: 0.5857 - f1_perClass: 0.8602 - acc: 0.8484     \n",
      "Epoch 67/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5594 - f1_perRow: 0.5844 - f1_perClass: 0.8605 - acc: 0.8483     \n",
      "Epoch 68/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5656 - f1_perRow: 0.5854 - f1_perClass: 0.8598 - acc: 0.8485     \n",
      "Epoch 69/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5622 - f1_perRow: 0.5853 - f1_perClass: 0.8601 - acc: 0.8484     \n",
      "Epoch 70/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5557 - f1_perRow: 0.5825 - f1_perClass: 0.8610 - acc: 0.8480     \n",
      "Epoch 71/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5757 - f1_perRow: 0.5790 - f1_perClass: 0.8583 - acc: 0.8475     \n",
      "Epoch 72/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5683 - f1_perRow: 0.5795 - f1_perClass: 0.8590 - acc: 0.8474     \n",
      "Epoch 73/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5683 - f1_perRow: 0.5852 - f1_perClass: 0.8592 - acc: 0.8479     \n",
      "Epoch 74/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5749 - f1_perRow: 0.5746 - f1_perClass: 0.8583 - acc: 0.8474     \n",
      "Epoch 75/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5677 - f1_perRow: 0.5767 - f1_perClass: 0.8595 - acc: 0.8480     \n",
      "Epoch 76/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5649 - f1_perRow: 0.5846 - f1_perClass: 0.8601 - acc: 0.8479     \n",
      "Epoch 77/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5640 - f1_perRow: 0.5852 - f1_perClass: 0.8599 - acc: 0.8477     \n",
      "Epoch 78/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5600 - f1_perRow: 0.5906 - f1_perClass: 0.8607 - acc: 0.8481     \n",
      "Epoch 79/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5608 - f1_perRow: 0.5878 - f1_perClass: 0.8603 - acc: 0.8475     \n",
      "Epoch 80/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5781 - f1_perRow: 0.5773 - f1_perClass: 0.8578 - acc: 0.8480     \n",
      "Epoch 81/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5624 - f1_perRow: 0.5816 - f1_perClass: 0.8602 - acc: 0.8480     \n",
      "Epoch 82/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5641 - f1_perRow: 0.5897 - f1_perClass: 0.8601 - acc: 0.8477     \n",
      "Epoch 83/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5571 - f1_perRow: 0.5889 - f1_perClass: 0.8610 - acc: 0.8479     \n",
      "Epoch 84/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5758 - f1_perRow: 0.5757 - f1_perClass: 0.8585 - acc: 0.8474     \n",
      "Epoch 85/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5709 - f1_perRow: 0.5738 - f1_perClass: 0.8584 - acc: 0.8474     \n",
      "Epoch 86/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5608 - f1_perRow: 0.5877 - f1_perClass: 0.8606 - acc: 0.8484     \n",
      "Epoch 87/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5614 - f1_perRow: 0.5882 - f1_perClass: 0.8604 - acc: 0.8477     \n",
      "Epoch 88/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5663 - f1_perRow: 0.5801 - f1_perClass: 0.8598 - acc: 0.8475     \n",
      "Epoch 89/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5723 - f1_perRow: 0.5770 - f1_perClass: 0.8587 - acc: 0.8483     \n",
      "Epoch 90/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5604 - f1_perRow: 0.5827 - f1_perClass: 0.8605 - acc: 0.8476     \n",
      "Epoch 91/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5786 - f1_perClass: 0.8607 - acc: 0.8478     \n",
      "Epoch 92/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5825 - f1_perClass: 0.8603 - acc: 0.8475     \n",
      "Epoch 93/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5637 - f1_perRow: 0.5846 - f1_perClass: 0.8602 - acc: 0.8478     \n",
      "Epoch 94/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5576 - f1_perRow: 0.5883 - f1_perClass: 0.8609 - acc: 0.8476     \n",
      "Epoch 95/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5776 - f1_perRow: 0.5758 - f1_perClass: 0.8575 - acc: 0.8467     \n",
      "Epoch 96/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5633 - f1_perRow: 0.5825 - f1_perClass: 0.8601 - acc: 0.8481     \n",
      "Epoch 97/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5730 - f1_perRow: 0.5792 - f1_perClass: 0.8587 - acc: 0.8472     \n",
      "Epoch 98/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5678 - f1_perRow: 0.5813 - f1_perClass: 0.8596 - acc: 0.8474     \n",
      "Epoch 99/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5578 - f1_perRow: 0.5892 - f1_perClass: 0.8608 - acc: 0.8484     \n",
      "Epoch 100/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5747 - f1_perRow: 0.5797 - f1_perClass: 0.8582 - acc: 0.8472     \n",
      "Epoch 101/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5650 - f1_perRow: 0.5852 - f1_perClass: 0.8601 - acc: 0.8484     \n",
      "Epoch 102/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5693 - f1_perRow: 0.5825 - f1_perClass: 0.8592 - acc: 0.8474     \n",
      "Epoch 103/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5670 - f1_perRow: 0.5759 - f1_perClass: 0.8597 - acc: 0.8483     \n",
      "Epoch 104/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5557 - f1_perRow: 0.5842 - f1_perClass: 0.8612 - acc: 0.8480     \n",
      "Epoch 105/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5627 - f1_perRow: 0.5839 - f1_perClass: 0.8603 - acc: 0.8476     \n",
      "Epoch 106/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5644 - f1_perRow: 0.5808 - f1_perClass: 0.8597 - acc: 0.8472     \n",
      "Epoch 107/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5672 - f1_perRow: 0.5845 - f1_perClass: 0.8594 - acc: 0.8483     \n",
      "Epoch 108/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5562 - f1_perRow: 0.5823 - f1_perClass: 0.8610 - acc: 0.8478     \n",
      "Epoch 109/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5556 - f1_perRow: 0.5826 - f1_perClass: 0.8615 - acc: 0.8479     \n",
      "Epoch 110/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5600 - f1_perRow: 0.5875 - f1_perClass: 0.8605 - acc: 0.8480     \n",
      "Epoch 111/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5685 - f1_perRow: 0.5804 - f1_perClass: 0.8590 - acc: 0.8474     \n",
      "Epoch 112/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5663 - f1_perRow: 0.5825 - f1_perClass: 0.8598 - acc: 0.8479     \n",
      "Epoch 113/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5623 - f1_perRow: 0.5831 - f1_perClass: 0.8602 - acc: 0.8480     \n",
      "Epoch 114/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5569 - f1_perRow: 0.5892 - f1_perClass: 0.8612 - acc: 0.8474     \n",
      "Epoch 115/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5878 - f1_perRow: 0.5642 - f1_perClass: 0.8553 - acc: 0.8457     \n",
      "Epoch 116/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5696 - f1_perRow: 0.5785 - f1_perClass: 0.8589 - acc: 0.8472     \n",
      "Epoch 117/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5556 - f1_perRow: 0.5913 - f1_perClass: 0.8613 - acc: 0.8482     \n",
      "Epoch 118/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5748 - f1_perRow: 0.5765 - f1_perClass: 0.8585 - acc: 0.8472     \n",
      "Epoch 119/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5591 - f1_perRow: 0.5874 - f1_perClass: 0.8606 - acc: 0.8479     \n",
      "Epoch 120/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5706 - f1_perRow: 0.5836 - f1_perClass: 0.8591 - acc: 0.8477     \n",
      "Epoch 121/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5661 - f1_perRow: 0.5886 - f1_perClass: 0.8600 - acc: 0.8484     \n",
      "Epoch 122/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5580 - f1_perRow: 0.5905 - f1_perClass: 0.8609 - acc: 0.8474     \n",
      "Epoch 123/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5760 - f1_perRow: 0.5704 - f1_perClass: 0.8581 - acc: 0.8473     \n",
      "Epoch 124/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5654 - f1_perRow: 0.5740 - f1_perClass: 0.8597 - acc: 0.8475     \n",
      "Epoch 125/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5735 - f1_perRow: 0.5764 - f1_perClass: 0.8588 - acc: 0.8479     \n",
      "Epoch 126/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5572 - f1_perRow: 0.5857 - f1_perClass: 0.8612 - acc: 0.8485     \n",
      "Epoch 127/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5687 - f1_perRow: 0.5810 - f1_perClass: 0.8593 - acc: 0.8483     \n",
      "Epoch 128/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5723 - f1_perRow: 0.5802 - f1_perClass: 0.8586 - acc: 0.8470     \n",
      "Epoch 129/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5551 - f1_perRow: 0.5883 - f1_perClass: 0.8613 - acc: 0.8483     \n",
      "Epoch 130/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5605 - f1_perRow: 0.5798 - f1_perClass: 0.8606 - acc: 0.8482     \n",
      "Epoch 131/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5801 - f1_perRow: 0.5662 - f1_perClass: 0.8576 - acc: 0.8475     \n",
      "Epoch 132/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5729 - f1_perRow: 0.5782 - f1_perClass: 0.8590 - acc: 0.8468     \n",
      "Epoch 133/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5651 - f1_perRow: 0.5775 - f1_perClass: 0.8595 - acc: 0.8482     \n",
      "Epoch 134/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5581 - f1_perRow: 0.5833 - f1_perClass: 0.8611 - acc: 0.8475     \n",
      "Epoch 135/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5727 - f1_perRow: 0.5809 - f1_perClass: 0.8589 - acc: 0.8474     \n",
      "Epoch 136/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5593 - f1_perRow: 0.5801 - f1_perClass: 0.8606 - acc: 0.8473     \n",
      "Epoch 137/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5885 - f1_perRow: 0.5774 - f1_perClass: 0.8559 - acc: 0.8477     \n",
      "Epoch 138/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5686 - f1_perRow: 0.5825 - f1_perClass: 0.8592 - acc: 0.8480     \n",
      "Epoch 139/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5611 - f1_perRow: 0.5870 - f1_perClass: 0.8605 - acc: 0.8486     \n",
      "Epoch 140/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5571 - f1_perRow: 0.5849 - f1_perClass: 0.8609 - acc: 0.8483     \n",
      "Epoch 141/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5670 - f1_perRow: 0.5738 - f1_perClass: 0.8593 - acc: 0.8478     \n",
      "Epoch 142/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5669 - f1_perRow: 0.5792 - f1_perClass: 0.8599 - acc: 0.8479     \n",
      "Epoch 143/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5761 - f1_perRow: 0.5733 - f1_perClass: 0.8579 - acc: 0.8466     \n",
      "Epoch 144/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5725 - f1_perRow: 0.5767 - f1_perClass: 0.8586 - acc: 0.8476     \n",
      "Epoch 145/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5866 - f1_perRow: 0.5776 - f1_perClass: 0.8559 - acc: 0.8459     \n",
      "Epoch 146/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5775 - f1_perRow: 0.5658 - f1_perClass: 0.8575 - acc: 0.8477     \n",
      "Epoch 147/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5586 - f1_perRow: 0.5862 - f1_perClass: 0.8610 - acc: 0.8479     \n",
      "Epoch 148/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5649 - f1_perRow: 0.5768 - f1_perClass: 0.8600 - acc: 0.8481     \n",
      "Epoch 149/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5719 - f1_perRow: 0.5752 - f1_perClass: 0.8589 - acc: 0.8482     \n",
      "Epoch 150/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5551 - f1_perRow: 0.5844 - f1_perClass: 0.8614 - acc: 0.8483     \n",
      "Epoch 151/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5608 - f1_perRow: 0.5880 - f1_perClass: 0.8603 - acc: 0.8484     \n",
      "Epoch 152/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5616 - f1_perRow: 0.5795 - f1_perClass: 0.8603 - acc: 0.8470     \n",
      "Epoch 153/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5638 - f1_perRow: 0.5819 - f1_perClass: 0.8602 - acc: 0.8478     \n",
      "Epoch 154/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5606 - f1_perRow: 0.5867 - f1_perClass: 0.8606 - acc: 0.8479     \n",
      "Epoch 155/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5593 - f1_perRow: 0.5828 - f1_perClass: 0.8608 - acc: 0.8477     \n",
      "Epoch 156/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5558 - f1_perRow: 0.5960 - f1_perClass: 0.8613 - acc: 0.8478     \n",
      "Epoch 157/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5668 - f1_perRow: 0.5768 - f1_perClass: 0.8593 - acc: 0.8473     \n",
      "Epoch 158/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5575 - f1_perRow: 0.5811 - f1_perClass: 0.8609 - acc: 0.8486     \n",
      "Epoch 159/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5574 - f1_perRow: 0.5913 - f1_perClass: 0.8610 - acc: 0.8476     \n",
      "Epoch 160/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5601 - f1_perRow: 0.5867 - f1_perClass: 0.8607 - acc: 0.8470     \n",
      "Epoch 161/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5835 - f1_perRow: 0.5761 - f1_perClass: 0.8571 - acc: 0.8478     \n",
      "Epoch 162/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5578 - f1_perRow: 0.5848 - f1_perClass: 0.8609 - acc: 0.8475     \n",
      "Epoch 163/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5817 - f1_perClass: 0.8600 - acc: 0.8476     \n",
      "Epoch 164/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5803 - f1_perRow: 0.5719 - f1_perClass: 0.8572 - acc: 0.8474     \n",
      "Epoch 165/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5634 - f1_perRow: 0.5803 - f1_perClass: 0.8598 - acc: 0.8472     \n",
      "Epoch 166/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5706 - f1_perRow: 0.5762 - f1_perClass: 0.8592 - acc: 0.8474     \n",
      "Epoch 167/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5577 - f1_perRow: 0.5903 - f1_perClass: 0.8610 - acc: 0.8470     \n",
      "Epoch 168/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5614 - f1_perRow: 0.5842 - f1_perClass: 0.8603 - acc: 0.8487     \n",
      "Epoch 169/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5700 - f1_perRow: 0.5808 - f1_perClass: 0.8590 - acc: 0.8474     \n",
      "Epoch 170/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5674 - f1_perRow: 0.5805 - f1_perClass: 0.8596 - acc: 0.8476     \n",
      "Epoch 171/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5619 - f1_perRow: 0.5815 - f1_perClass: 0.8601 - acc: 0.8471     \n",
      "Epoch 172/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5882 - f1_perRow: 0.5652 - f1_perClass: 0.8558 - acc: 0.8477     \n",
      "Epoch 173/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5806 - f1_perRow: 0.5731 - f1_perClass: 0.8572 - acc: 0.8476     \n",
      "Epoch 174/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5738 - f1_perRow: 0.5781 - f1_perClass: 0.8587 - acc: 0.8483     \n",
      "Epoch 175/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5581 - f1_perRow: 0.5881 - f1_perClass: 0.8607 - acc: 0.8486     \n",
      "Epoch 176/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5651 - f1_perRow: 0.5837 - f1_perClass: 0.8599 - acc: 0.8474     \n",
      "Epoch 177/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5708 - f1_perRow: 0.5798 - f1_perClass: 0.8591 - acc: 0.8480     \n",
      "Epoch 178/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5880 - f1_perClass: 0.8600 - acc: 0.8472     \n",
      "Epoch 179/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5759 - f1_perRow: 0.5735 - f1_perClass: 0.8581 - acc: 0.8475     \n",
      "Epoch 180/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5610 - f1_perRow: 0.5843 - f1_perClass: 0.8604 - acc: 0.8477     \n",
      "Epoch 181/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5636 - f1_perRow: 0.5833 - f1_perClass: 0.8602 - acc: 0.8486     \n",
      "Epoch 182/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5828 - f1_perRow: 0.5708 - f1_perClass: 0.8573 - acc: 0.8477     \n",
      "Epoch 183/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5744 - f1_perRow: 0.5725 - f1_perClass: 0.8586 - acc: 0.8482     \n",
      "Epoch 184/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5628 - f1_perRow: 0.5769 - f1_perClass: 0.8603 - acc: 0.8473     \n",
      "Epoch 185/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5567 - f1_perRow: 0.5893 - f1_perClass: 0.8610 - acc: 0.8483     \n",
      "Epoch 186/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5689 - f1_perRow: 0.5786 - f1_perClass: 0.8596 - acc: 0.8477     \n",
      "Epoch 187/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5831 - f1_perRow: 0.5671 - f1_perClass: 0.8576 - acc: 0.8468     \n",
      "Epoch 188/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5603 - f1_perRow: 0.5880 - f1_perClass: 0.8604 - acc: 0.8479     \n",
      "Epoch 189/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5734 - f1_perRow: 0.5803 - f1_perClass: 0.8588 - acc: 0.8477     \n",
      "Epoch 190/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5604 - f1_perRow: 0.5882 - f1_perClass: 0.8606 - acc: 0.8476     \n",
      "Epoch 191/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5621 - f1_perRow: 0.5857 - f1_perClass: 0.8603 - acc: 0.8482     \n",
      "Epoch 192/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5616 - f1_perRow: 0.5820 - f1_perClass: 0.8606 - acc: 0.8481     \n",
      "Epoch 193/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5892 - f1_perClass: 0.8604 - acc: 0.8481     \n",
      "Epoch 194/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5555 - f1_perRow: 0.5871 - f1_perClass: 0.8613 - acc: 0.8484     \n",
      "Epoch 195/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5667 - f1_perRow: 0.5826 - f1_perClass: 0.8597 - acc: 0.8478     \n",
      "Epoch 196/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5748 - f1_perRow: 0.5758 - f1_perClass: 0.8585 - acc: 0.8483     \n",
      "Epoch 197/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5761 - f1_perRow: 0.5752 - f1_perClass: 0.8579 - acc: 0.8473     \n",
      "Epoch 198/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5790 - f1_perClass: 0.8602 - acc: 0.8481     \n",
      "Epoch 199/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5611 - f1_perRow: 0.5852 - f1_perClass: 0.8602 - acc: 0.8475     \n",
      "Epoch 200/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5731 - f1_perRow: 0.5758 - f1_perClass: 0.8584 - acc: 0.8478     \n",
      "Epoch 201/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5721 - f1_perRow: 0.5841 - f1_perClass: 0.8590 - acc: 0.8482     \n",
      "Epoch 202/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5550 - f1_perRow: 0.5879 - f1_perClass: 0.8613 - acc: 0.8473     \n",
      "Epoch 203/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5681 - f1_perRow: 0.5861 - f1_perClass: 0.8592 - acc: 0.8479     \n",
      "Epoch 204/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5772 - f1_perRow: 0.5776 - f1_perClass: 0.8583 - acc: 0.8477     \n",
      "Epoch 205/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5579 - f1_perRow: 0.5828 - f1_perClass: 0.8607 - acc: 0.8475     \n",
      "Epoch 206/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5737 - f1_perRow: 0.5765 - f1_perClass: 0.8585 - acc: 0.8479     \n",
      "Epoch 207/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5669 - f1_perRow: 0.5791 - f1_perClass: 0.8596 - acc: 0.8474     \n",
      "Epoch 208/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5845 - f1_perRow: 0.5689 - f1_perClass: 0.8571 - acc: 0.8479     \n",
      "Epoch 209/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5846 - f1_perClass: 0.8601 - acc: 0.8476     \n",
      "Epoch 210/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5809 - f1_perRow: 0.5775 - f1_perClass: 0.8580 - acc: 0.8478     \n",
      "Epoch 211/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5575 - f1_perRow: 0.5921 - f1_perClass: 0.8612 - acc: 0.8479     \n",
      "Epoch 212/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5621 - f1_perRow: 0.5851 - f1_perClass: 0.8605 - acc: 0.8489     \n",
      "Epoch 213/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5646 - f1_perRow: 0.5799 - f1_perClass: 0.8598 - acc: 0.8474     \n",
      "Epoch 214/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5862 - f1_perClass: 0.8595 - acc: 0.8475     \n",
      "Epoch 215/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5885 - f1_perRow: 0.5716 - f1_perClass: 0.8562 - acc: 0.8475     \n",
      "Epoch 216/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5707 - f1_perRow: 0.5806 - f1_perClass: 0.8588 - acc: 0.8481     \n",
      "Epoch 217/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5737 - f1_perRow: 0.5730 - f1_perClass: 0.8584 - acc: 0.8479     \n",
      "Epoch 218/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5598 - f1_perRow: 0.5834 - f1_perClass: 0.8607 - acc: 0.8481     \n",
      "Epoch 219/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5563 - f1_perRow: 0.5890 - f1_perClass: 0.8611 - acc: 0.8476     \n",
      "Epoch 220/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5617 - f1_perRow: 0.5836 - f1_perClass: 0.8601 - acc: 0.8476     \n",
      "Epoch 221/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5559 - f1_perRow: 0.5815 - f1_perClass: 0.8614 - acc: 0.8480     \n",
      "Epoch 222/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5640 - f1_perRow: 0.5825 - f1_perClass: 0.8604 - acc: 0.8477     \n",
      "Epoch 223/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5593 - f1_perRow: 0.5840 - f1_perClass: 0.8607 - acc: 0.8474     \n",
      "Epoch 224/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5736 - f1_perRow: 0.5839 - f1_perClass: 0.8586 - acc: 0.8479     \n",
      "Epoch 225/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5765 - f1_perRow: 0.5717 - f1_perClass: 0.8583 - acc: 0.8478     \n",
      "Epoch 226/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5664 - f1_perRow: 0.5809 - f1_perClass: 0.8596 - acc: 0.8482     \n",
      "Epoch 227/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5874 - f1_perClass: 0.8607 - acc: 0.8483     \n",
      "Epoch 228/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5649 - f1_perRow: 0.5810 - f1_perClass: 0.8600 - acc: 0.8474     \n",
      "Epoch 229/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5673 - f1_perRow: 0.5851 - f1_perClass: 0.8597 - acc: 0.8482     \n",
      "Epoch 230/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5558 - f1_perRow: 0.5890 - f1_perClass: 0.8612 - acc: 0.8477     \n",
      "Epoch 231/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5649 - f1_perRow: 0.5826 - f1_perClass: 0.8595 - acc: 0.8482     \n",
      "Epoch 232/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5742 - f1_perRow: 0.5725 - f1_perClass: 0.8587 - acc: 0.8476     \n",
      "Epoch 233/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5614 - f1_perRow: 0.5897 - f1_perClass: 0.8607 - acc: 0.8483     \n",
      "Epoch 234/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5682 - f1_perRow: 0.5730 - f1_perClass: 0.8591 - acc: 0.8473     \n",
      "Epoch 235/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5894 - f1_perRow: 0.5744 - f1_perClass: 0.8560 - acc: 0.8472     \n",
      "Epoch 236/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5555 - f1_perRow: 0.5902 - f1_perClass: 0.8617 - acc: 0.8478     \n",
      "Epoch 237/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5630 - f1_perRow: 0.5877 - f1_perClass: 0.8603 - acc: 0.8483     \n",
      "Epoch 238/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5642 - f1_perRow: 0.5866 - f1_perClass: 0.8599 - acc: 0.8477     \n",
      "Epoch 239/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5589 - f1_perRow: 0.5816 - f1_perClass: 0.8609 - acc: 0.8477     \n",
      "Epoch 240/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5702 - f1_perRow: 0.5795 - f1_perClass: 0.8593 - acc: 0.8487     \n",
      "Epoch 241/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5582 - f1_perRow: 0.5835 - f1_perClass: 0.8609 - acc: 0.8492     \n",
      "Epoch 242/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5613 - f1_perRow: 0.5842 - f1_perClass: 0.8603 - acc: 0.8478     \n",
      "Epoch 243/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5722 - f1_perRow: 0.5844 - f1_perClass: 0.8589 - acc: 0.8479     \n",
      "Epoch 244/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5663 - f1_perRow: 0.5820 - f1_perClass: 0.8598 - acc: 0.8481     \n",
      "Epoch 245/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5612 - f1_perRow: 0.5886 - f1_perClass: 0.8604 - acc: 0.8484     \n",
      "Epoch 246/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5668 - f1_perRow: 0.5776 - f1_perClass: 0.8594 - acc: 0.8473     \n",
      "Epoch 247/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5561 - f1_perRow: 0.5857 - f1_perClass: 0.8613 - acc: 0.8486     \n",
      "Epoch 248/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5628 - f1_perRow: 0.5821 - f1_perClass: 0.8601 - acc: 0.8476     \n",
      "Epoch 249/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5784 - f1_perRow: 0.5757 - f1_perClass: 0.8579 - acc: 0.8469     \n",
      "Epoch 250/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5612 - f1_perRow: 0.5856 - f1_perClass: 0.8606 - acc: 0.8479     \n",
      "Epoch 251/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5611 - f1_perRow: 0.5905 - f1_perClass: 0.8605 - acc: 0.8473     \n",
      "Epoch 252/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5639 - f1_perRow: 0.5787 - f1_perClass: 0.8601 - acc: 0.8481     \n",
      "Epoch 253/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5586 - f1_perRow: 0.5848 - f1_perClass: 0.8608 - acc: 0.8478     \n",
      "Epoch 254/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5578 - f1_perRow: 0.5907 - f1_perClass: 0.8610 - acc: 0.8485     \n",
      "Epoch 255/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5655 - f1_perRow: 0.5847 - f1_perClass: 0.8599 - acc: 0.8476     \n",
      "Epoch 256/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5661 - f1_perRow: 0.5800 - f1_perClass: 0.8596 - acc: 0.8479     \n",
      "Epoch 257/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5606 - f1_perRow: 0.5874 - f1_perClass: 0.8607 - acc: 0.8487     \n",
      "Epoch 258/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5619 - f1_perRow: 0.5860 - f1_perClass: 0.8602 - acc: 0.8487     \n",
      "Epoch 259/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5566 - f1_perRow: 0.5861 - f1_perClass: 0.8611 - acc: 0.8475     \n",
      "Epoch 260/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5758 - f1_perRow: 0.5750 - f1_perClass: 0.8582 - acc: 0.8470     \n",
      "Epoch 261/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5564 - f1_perRow: 0.5896 - f1_perClass: 0.8612 - acc: 0.8481     \n",
      "Epoch 262/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5731 - f1_perRow: 0.5783 - f1_perClass: 0.8588 - acc: 0.8483     \n",
      "Epoch 263/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5625 - f1_perRow: 0.5746 - f1_perClass: 0.8601 - acc: 0.8472     \n",
      "Epoch 264/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5704 - f1_perRow: 0.5795 - f1_perClass: 0.8593 - acc: 0.8473     \n",
      "Epoch 265/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5623 - f1_perRow: 0.5842 - f1_perClass: 0.8604 - acc: 0.8479     \n",
      "Epoch 266/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5741 - f1_perRow: 0.5706 - f1_perClass: 0.8588 - acc: 0.8480     \n",
      "Epoch 267/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5673 - f1_perRow: 0.5840 - f1_perClass: 0.8594 - acc: 0.8482     \n",
      "Epoch 268/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5578 - f1_perRow: 0.5879 - f1_perClass: 0.8608 - acc: 0.8478     \n",
      "Epoch 269/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5736 - f1_perRow: 0.5766 - f1_perClass: 0.8587 - acc: 0.8475     \n",
      "Epoch 270/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5552 - f1_perRow: 0.5878 - f1_perClass: 0.8614 - acc: 0.8479     \n",
      "Epoch 271/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5625 - f1_perRow: 0.5796 - f1_perClass: 0.8602 - acc: 0.8468     \n",
      "Epoch 272/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5692 - f1_perRow: 0.5759 - f1_perClass: 0.8589 - acc: 0.8481     \n",
      "Epoch 273/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5681 - f1_perRow: 0.5858 - f1_perClass: 0.8597 - acc: 0.8473     \n",
      "Epoch 274/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5627 - f1_perRow: 0.5829 - f1_perClass: 0.8604 - acc: 0.8484     \n",
      "Epoch 275/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5561 - f1_perRow: 0.5773 - f1_perClass: 0.8611 - acc: 0.8484     \n",
      "Epoch 276/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5897 - f1_perClass: 0.8602 - acc: 0.8477     \n",
      "Epoch 277/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5583 - f1_perRow: 0.5889 - f1_perClass: 0.8610 - acc: 0.8479     \n",
      "Epoch 278/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5683 - f1_perRow: 0.5784 - f1_perClass: 0.8595 - acc: 0.8476     \n",
      "Epoch 279/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5606 - f1_perRow: 0.5813 - f1_perClass: 0.8605 - acc: 0.8471     \n",
      "Epoch 280/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5556 - f1_perRow: 0.5868 - f1_perClass: 0.8613 - acc: 0.8484     \n",
      "Epoch 281/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5549 - f1_perRow: 0.5893 - f1_perClass: 0.8615 - acc: 0.8483     \n",
      "Epoch 282/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5724 - f1_perRow: 0.5818 - f1_perClass: 0.8586 - acc: 0.8474     \n",
      "Epoch 283/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5572 - f1_perRow: 0.5877 - f1_perClass: 0.8613 - acc: 0.8485     \n",
      "Epoch 284/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5550 - f1_perRow: 0.5856 - f1_perClass: 0.8613 - acc: 0.8481     \n",
      "Epoch 285/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5672 - f1_perRow: 0.5843 - f1_perClass: 0.8593 - acc: 0.8475     \n",
      "Epoch 286/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5583 - f1_perRow: 0.5863 - f1_perClass: 0.8607 - acc: 0.8486     \n",
      "Epoch 287/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5634 - f1_perRow: 0.5867 - f1_perClass: 0.8598 - acc: 0.8484     \n",
      "Epoch 288/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5624 - f1_perRow: 0.5848 - f1_perClass: 0.8602 - acc: 0.8474     \n",
      "Epoch 289/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5560 - f1_perRow: 0.5897 - f1_perClass: 0.8612 - acc: 0.8471     \n",
      "Epoch 290/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5603 - f1_perRow: 0.5829 - f1_perClass: 0.8606 - acc: 0.8478     \n",
      "Epoch 291/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5689 - f1_perRow: 0.5800 - f1_perClass: 0.8593 - acc: 0.8478     \n",
      "Epoch 292/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5788 - f1_perRow: 0.5745 - f1_perClass: 0.8573 - acc: 0.8469     \n",
      "Epoch 293/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5591 - f1_perRow: 0.5872 - f1_perClass: 0.8608 - acc: 0.8487     \n",
      "Epoch 294/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5615 - f1_perRow: 0.5859 - f1_perClass: 0.8604 - acc: 0.8477     \n",
      "Epoch 295/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5601 - f1_perRow: 0.5806 - f1_perClass: 0.8606 - acc: 0.8479     \n",
      "Epoch 296/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5698 - f1_perRow: 0.5840 - f1_perClass: 0.8592 - acc: 0.8481     \n",
      "Epoch 297/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5681 - f1_perRow: 0.5767 - f1_perClass: 0.8593 - acc: 0.8476     \n",
      "Epoch 298/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5609 - f1_perRow: 0.5868 - f1_perClass: 0.8607 - acc: 0.8475     \n",
      "Epoch 299/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5630 - f1_perRow: 0.5851 - f1_perClass: 0.8602 - acc: 0.8480     \n",
      "Epoch 300/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5559 - f1_perRow: 0.5897 - f1_perClass: 0.8616 - acc: 0.8486     \n",
      "Epoch 301/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5744 - f1_perRow: 0.5795 - f1_perClass: 0.8588 - acc: 0.8475     \n",
      "Epoch 302/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5582 - f1_perRow: 0.5883 - f1_perClass: 0.8607 - acc: 0.8480     \n",
      "Epoch 303/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5584 - f1_perRow: 0.5848 - f1_perClass: 0.8609 - acc: 0.8478     \n",
      "Epoch 304/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5817 - f1_perRow: 0.5683 - f1_perClass: 0.8571 - acc: 0.8466     \n",
      "Epoch 305/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5583 - f1_perRow: 0.5892 - f1_perClass: 0.8609 - acc: 0.8482     \n",
      "Epoch 306/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5610 - f1_perRow: 0.5846 - f1_perClass: 0.8602 - acc: 0.8471     \n",
      "Epoch 307/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5650 - f1_perRow: 0.5773 - f1_perClass: 0.8600 - acc: 0.8486     \n",
      "Epoch 308/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5671 - f1_perRow: 0.5860 - f1_perClass: 0.8596 - acc: 0.8485     \n",
      "Epoch 309/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5575 - f1_perRow: 0.5890 - f1_perClass: 0.8610 - acc: 0.8484     \n",
      "Epoch 310/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5593 - f1_perRow: 0.5791 - f1_perClass: 0.8606 - acc: 0.8475     \n",
      "Epoch 311/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5698 - f1_perRow: 0.5755 - f1_perClass: 0.8587 - acc: 0.8479     \n",
      "Epoch 312/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5722 - f1_perRow: 0.5741 - f1_perClass: 0.8588 - acc: 0.8471     \n",
      "Epoch 313/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5852 - f1_perClass: 0.8603 - acc: 0.8487     \n",
      "Epoch 314/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5703 - f1_perRow: 0.5837 - f1_perClass: 0.8592 - acc: 0.8487     \n",
      "Epoch 315/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5662 - f1_perRow: 0.5796 - f1_perClass: 0.8601 - acc: 0.8481     \n",
      "Epoch 316/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5669 - f1_perRow: 0.5706 - f1_perClass: 0.8595 - acc: 0.8467     \n",
      "Epoch 317/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5825 - f1_perRow: 0.5699 - f1_perClass: 0.8573 - acc: 0.8471     \n",
      "Epoch 318/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5566 - f1_perRow: 0.5847 - f1_perClass: 0.8610 - acc: 0.8482     \n",
      "Epoch 319/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5628 - f1_perRow: 0.5844 - f1_perClass: 0.8601 - acc: 0.8482     \n",
      "Epoch 320/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5679 - f1_perRow: 0.5777 - f1_perClass: 0.8595 - acc: 0.8479     \n",
      "Epoch 321/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5698 - f1_perRow: 0.5819 - f1_perClass: 0.8592 - acc: 0.8470     \n",
      "Epoch 322/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5587 - f1_perRow: 0.5826 - f1_perClass: 0.8609 - acc: 0.8480     \n",
      "Epoch 323/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5604 - f1_perRow: 0.5862 - f1_perClass: 0.8606 - acc: 0.8473     \n",
      "Epoch 324/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5787 - f1_perClass: 0.8602 - acc: 0.8476     \n",
      "Epoch 325/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5685 - f1_perRow: 0.5778 - f1_perClass: 0.8594 - acc: 0.8479     \n",
      "Epoch 326/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5612 - f1_perRow: 0.5827 - f1_perClass: 0.8606 - acc: 0.8483     \n",
      "Epoch 327/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5656 - f1_perRow: 0.5769 - f1_perClass: 0.8597 - acc: 0.8488     \n",
      "Epoch 328/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5856 - f1_perClass: 0.8601 - acc: 0.8484     \n",
      "Epoch 329/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5728 - f1_perRow: 0.5851 - f1_perClass: 0.8593 - acc: 0.8492     \n",
      "Epoch 330/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5616 - f1_perRow: 0.5874 - f1_perClass: 0.8606 - acc: 0.8478     \n",
      "Epoch 331/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5547 - f1_perRow: 0.5884 - f1_perClass: 0.8613 - acc: 0.8482     \n",
      "Epoch 332/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5558 - f1_perRow: 0.5890 - f1_perClass: 0.8613 - acc: 0.8475     \n",
      "Epoch 333/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5621 - f1_perRow: 0.5878 - f1_perClass: 0.8602 - acc: 0.8475     \n",
      "Epoch 334/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5677 - f1_perRow: 0.5759 - f1_perClass: 0.8595 - acc: 0.8467     \n",
      "Epoch 335/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5764 - f1_perRow: 0.5703 - f1_perClass: 0.8582 - acc: 0.8477     \n",
      "Epoch 336/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5709 - f1_perRow: 0.5778 - f1_perClass: 0.8591 - acc: 0.8489     \n",
      "Epoch 337/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5664 - f1_perRow: 0.5826 - f1_perClass: 0.8595 - acc: 0.8479     \n",
      "Epoch 338/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5796 - f1_perRow: 0.5751 - f1_perClass: 0.8573 - acc: 0.8472     \n",
      "Epoch 339/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5695 - f1_perRow: 0.5742 - f1_perClass: 0.8592 - acc: 0.8480     \n",
      "Epoch 340/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5655 - f1_perRow: 0.5842 - f1_perClass: 0.8600 - acc: 0.8478     \n",
      "Epoch 341/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5585 - f1_perRow: 0.5918 - f1_perClass: 0.8608 - acc: 0.8484     \n",
      "Epoch 342/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5651 - f1_perRow: 0.5838 - f1_perClass: 0.8598 - acc: 0.8481     \n",
      "Epoch 343/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5678 - f1_perRow: 0.5749 - f1_perClass: 0.8593 - acc: 0.8481     \n",
      "Epoch 344/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5628 - f1_perRow: 0.5782 - f1_perClass: 0.8597 - acc: 0.8478     \n",
      "Epoch 345/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5839 - f1_perClass: 0.8604 - acc: 0.8482     \n",
      "Epoch 346/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5737 - f1_perRow: 0.5793 - f1_perClass: 0.8582 - acc: 0.8472     \n",
      "Epoch 347/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5690 - f1_perRow: 0.5772 - f1_perClass: 0.8595 - acc: 0.8485     \n",
      "Epoch 348/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5686 - f1_perRow: 0.5781 - f1_perClass: 0.8594 - acc: 0.8479     \n",
      "Epoch 349/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5615 - f1_perRow: 0.5887 - f1_perClass: 0.8605 - acc: 0.8478     \n",
      "Epoch 350/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5674 - f1_perRow: 0.5745 - f1_perClass: 0.8593 - acc: 0.8475     \n",
      "Epoch 351/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5613 - f1_perRow: 0.5875 - f1_perClass: 0.8605 - acc: 0.8480     \n",
      "Epoch 352/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5654 - f1_perRow: 0.5800 - f1_perClass: 0.8595 - acc: 0.8478     \n",
      "Epoch 353/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5569 - f1_perRow: 0.5809 - f1_perClass: 0.8609 - acc: 0.8479     \n",
      "Epoch 354/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5830 - f1_perRow: 0.5723 - f1_perClass: 0.8566 - acc: 0.8472     \n",
      "Epoch 355/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5920 - f1_perRow: 0.5721 - f1_perClass: 0.8560 - acc: 0.8477     \n",
      "Epoch 356/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5678 - f1_perRow: 0.5815 - f1_perClass: 0.8594 - acc: 0.8473     \n",
      "Epoch 357/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5854 - f1_perClass: 0.8600 - acc: 0.8483     \n",
      "Epoch 358/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5568 - f1_perRow: 0.5879 - f1_perClass: 0.8612 - acc: 0.8484     \n",
      "Epoch 359/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5714 - f1_perRow: 0.5755 - f1_perClass: 0.8590 - acc: 0.8474     \n",
      "Epoch 360/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5800 - f1_perRow: 0.5686 - f1_perClass: 0.8571 - acc: 0.8469     \n",
      "Epoch 361/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5779 - f1_perRow: 0.5704 - f1_perClass: 0.8578 - acc: 0.8473     \n",
      "Epoch 362/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5774 - f1_perRow: 0.5762 - f1_perClass: 0.8579 - acc: 0.8471     \n",
      "Epoch 363/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5758 - f1_perRow: 0.5766 - f1_perClass: 0.8583 - acc: 0.8485     \n",
      "Epoch 364/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5674 - f1_perRow: 0.5812 - f1_perClass: 0.8597 - acc: 0.8477     \n",
      "Epoch 365/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5687 - f1_perRow: 0.5860 - f1_perClass: 0.8593 - acc: 0.8475     \n",
      "Epoch 366/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5634 - f1_perRow: 0.5807 - f1_perClass: 0.8601 - acc: 0.8478     \n",
      "Epoch 367/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5592 - f1_perRow: 0.5863 - f1_perClass: 0.8608 - acc: 0.8476     \n",
      "Epoch 368/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5696 - f1_perRow: 0.5764 - f1_perClass: 0.8590 - acc: 0.8475     \n",
      "Epoch 369/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5571 - f1_perRow: 0.5916 - f1_perClass: 0.8611 - acc: 0.8486     \n",
      "Epoch 370/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5790 - f1_perClass: 0.8606 - acc: 0.8482     \n",
      "Epoch 371/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5712 - f1_perRow: 0.5814 - f1_perClass: 0.8586 - acc: 0.8471     \n",
      "Epoch 372/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5782 - f1_perRow: 0.5736 - f1_perClass: 0.8579 - acc: 0.8468     \n",
      "Epoch 373/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5671 - f1_perRow: 0.5775 - f1_perClass: 0.8597 - acc: 0.8478     \n",
      "Epoch 374/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5559 - f1_perRow: 0.5795 - f1_perClass: 0.8616 - acc: 0.8485     \n",
      "Epoch 375/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5634 - f1_perRow: 0.5838 - f1_perClass: 0.8601 - acc: 0.8482     \n",
      "Epoch 376/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5576 - f1_perRow: 0.5887 - f1_perClass: 0.8611 - acc: 0.8480     \n",
      "Epoch 377/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5554 - f1_perRow: 0.5899 - f1_perClass: 0.8611 - acc: 0.8483     \n",
      "Epoch 378/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5603 - f1_perRow: 0.5812 - f1_perClass: 0.8605 - acc: 0.8471     \n",
      "Epoch 379/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5607 - f1_perRow: 0.5835 - f1_perClass: 0.8604 - acc: 0.8473     \n",
      "Epoch 380/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5756 - f1_perRow: 0.5738 - f1_perClass: 0.8579 - acc: 0.8468     \n",
      "Epoch 381/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5743 - f1_perRow: 0.5733 - f1_perClass: 0.8584 - acc: 0.8473     \n",
      "Epoch 382/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5646 - f1_perRow: 0.5810 - f1_perClass: 0.8597 - acc: 0.8473     \n",
      "Epoch 383/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5896 - f1_perRow: 0.5678 - f1_perClass: 0.8558 - acc: 0.8463     \n",
      "Epoch 384/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5572 - f1_perRow: 0.5870 - f1_perClass: 0.8611 - acc: 0.8484     \n",
      "Epoch 385/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5684 - f1_perRow: 0.5848 - f1_perClass: 0.8592 - acc: 0.8481     \n",
      "Epoch 386/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5573 - f1_perRow: 0.5857 - f1_perClass: 0.8609 - acc: 0.8486     \n",
      "Epoch 387/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5582 - f1_perRow: 0.5891 - f1_perClass: 0.8612 - acc: 0.8487     \n",
      "Epoch 388/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5614 - f1_perRow: 0.5832 - f1_perClass: 0.8603 - acc: 0.8482     \n",
      "Epoch 389/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5732 - f1_perRow: 0.5809 - f1_perClass: 0.8588 - acc: 0.8482     \n",
      "Epoch 390/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5718 - f1_perRow: 0.5754 - f1_perClass: 0.8589 - acc: 0.8473     \n",
      "Epoch 391/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5668 - f1_perRow: 0.5858 - f1_perClass: 0.8598 - acc: 0.8482     \n",
      "Epoch 392/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5626 - f1_perRow: 0.5863 - f1_perClass: 0.8602 - acc: 0.8482     \n",
      "Epoch 393/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5627 - f1_perRow: 0.5739 - f1_perClass: 0.8604 - acc: 0.8485     \n",
      "Epoch 394/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5557 - f1_perRow: 0.5918 - f1_perClass: 0.8611 - acc: 0.8475     \n",
      "Epoch 395/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5764 - f1_perRow: 0.5774 - f1_perClass: 0.8585 - acc: 0.8484     \n",
      "Epoch 396/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5670 - f1_perRow: 0.5817 - f1_perClass: 0.8596 - acc: 0.8475     \n",
      "Epoch 397/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5640 - f1_perRow: 0.5810 - f1_perClass: 0.8599 - acc: 0.8467     \n",
      "Epoch 398/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5643 - f1_perRow: 0.5858 - f1_perClass: 0.8600 - acc: 0.8482     \n",
      "Epoch 399/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5646 - f1_perRow: 0.5734 - f1_perClass: 0.8598 - acc: 0.8475     \n",
      "Epoch 400/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5677 - f1_perRow: 0.5778 - f1_perClass: 0.8598 - acc: 0.8486     \n",
      "Epoch 401/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5805 - f1_perClass: 0.8596 - acc: 0.8475     \n",
      "Epoch 402/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5657 - f1_perRow: 0.5784 - f1_perClass: 0.8595 - acc: 0.8478     \n",
      "Epoch 403/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5725 - f1_perRow: 0.5769 - f1_perClass: 0.8585 - acc: 0.8475     \n",
      "Epoch 404/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5572 - f1_perRow: 0.5899 - f1_perClass: 0.8607 - acc: 0.8479     \n",
      "Epoch 405/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5773 - f1_perRow: 0.5789 - f1_perClass: 0.8583 - acc: 0.8478     \n",
      "Epoch 406/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5635 - f1_perRow: 0.5853 - f1_perClass: 0.8600 - acc: 0.8461     \n",
      "Epoch 407/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5623 - f1_perRow: 0.5875 - f1_perClass: 0.8604 - acc: 0.8482     \n",
      "Epoch 408/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5779 - f1_perRow: 0.5831 - f1_perClass: 0.8578 - acc: 0.8478     \n",
      "Epoch 409/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5590 - f1_perRow: 0.5840 - f1_perClass: 0.8607 - acc: 0.8477     \n",
      "Epoch 410/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5669 - f1_perRow: 0.5771 - f1_perClass: 0.8596 - acc: 0.8485     \n",
      "Epoch 411/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5650 - f1_perRow: 0.5864 - f1_perClass: 0.8599 - acc: 0.8479     \n",
      "Epoch 412/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5554 - f1_perRow: 0.5869 - f1_perClass: 0.8611 - acc: 0.8480     \n",
      "Epoch 413/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5525 - f1_perRow: 0.5945 - f1_perClass: 0.8619 - acc: 0.8481     \n",
      "Epoch 414/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5808 - f1_perRow: 0.5708 - f1_perClass: 0.8577 - acc: 0.8466     \n",
      "Epoch 415/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5757 - f1_perRow: 0.5780 - f1_perClass: 0.8583 - acc: 0.8470     \n",
      "Epoch 416/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5623 - f1_perRow: 0.5958 - f1_perClass: 0.8602 - acc: 0.8481     \n",
      "Epoch 417/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5668 - f1_perRow: 0.5777 - f1_perClass: 0.8596 - acc: 0.8475     \n",
      "Epoch 418/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5760 - f1_perRow: 0.5702 - f1_perClass: 0.8581 - acc: 0.8470     \n",
      "Epoch 419/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5635 - f1_perRow: 0.5841 - f1_perClass: 0.8601 - acc: 0.8481     \n",
      "Epoch 420/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5554 - f1_perRow: 0.5895 - f1_perClass: 0.8613 - acc: 0.8475     \n",
      "Epoch 421/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5702 - f1_perRow: 0.5762 - f1_perClass: 0.8590 - acc: 0.8481     \n",
      "Epoch 422/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5590 - f1_perRow: 0.5901 - f1_perClass: 0.8607 - acc: 0.8472     \n",
      "Epoch 423/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5566 - f1_perRow: 0.5868 - f1_perClass: 0.8611 - acc: 0.8481     \n",
      "Epoch 424/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5577 - f1_perRow: 0.5858 - f1_perClass: 0.8609 - acc: 0.8473     \n",
      "Epoch 425/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5603 - f1_perRow: 0.5929 - f1_perClass: 0.8607 - acc: 0.8475     \n",
      "Epoch 426/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5772 - f1_perRow: 0.5727 - f1_perClass: 0.8579 - acc: 0.8473     \n",
      "Epoch 427/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5644 - f1_perRow: 0.5856 - f1_perClass: 0.8600 - acc: 0.8478     \n",
      "Epoch 428/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5694 - f1_perRow: 0.5780 - f1_perClass: 0.8591 - acc: 0.8476     \n",
      "Epoch 429/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5593 - f1_perRow: 0.5906 - f1_perClass: 0.8607 - acc: 0.8484     \n",
      "Epoch 430/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5826 - f1_perRow: 0.5680 - f1_perClass: 0.8571 - acc: 0.8471     \n",
      "Epoch 431/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5792 - f1_perRow: 0.5762 - f1_perClass: 0.8580 - acc: 0.8479     \n",
      "Epoch 432/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5680 - f1_perRow: 0.5777 - f1_perClass: 0.8595 - acc: 0.8490     \n",
      "Epoch 433/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5632 - f1_perRow: 0.5847 - f1_perClass: 0.8605 - acc: 0.8481     \n",
      "Epoch 434/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5689 - f1_perRow: 0.5901 - f1_perClass: 0.8597 - acc: 0.8477     \n",
      "Epoch 435/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5832 - f1_perClass: 0.8605 - acc: 0.8476     \n",
      "Epoch 436/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5574 - f1_perRow: 0.5923 - f1_perClass: 0.8611 - acc: 0.8472     \n",
      "Epoch 437/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5634 - f1_perRow: 0.5824 - f1_perClass: 0.8603 - acc: 0.8478     \n",
      "Epoch 438/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5596 - f1_perRow: 0.5917 - f1_perClass: 0.8609 - acc: 0.8480     \n",
      "Epoch 439/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5598 - f1_perRow: 0.5827 - f1_perClass: 0.8605 - acc: 0.8478     \n",
      "Epoch 440/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5707 - f1_perRow: 0.5804 - f1_perClass: 0.8591 - acc: 0.8476     \n",
      "Epoch 441/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5682 - f1_perRow: 0.5851 - f1_perClass: 0.8597 - acc: 0.8480     \n",
      "Epoch 442/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5785 - f1_perClass: 0.8599 - acc: 0.8473     \n",
      "Epoch 443/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5683 - f1_perRow: 0.5764 - f1_perClass: 0.8592 - acc: 0.8477     \n",
      "Epoch 444/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5662 - f1_perRow: 0.5774 - f1_perClass: 0.8597 - acc: 0.8480     \n",
      "Epoch 445/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5933 - f1_perRow: 0.5708 - f1_perClass: 0.8562 - acc: 0.8471     \n",
      "Epoch 446/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5595 - f1_perRow: 0.5883 - f1_perClass: 0.8607 - acc: 0.8469     \n",
      "Epoch 447/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5801 - f1_perRow: 0.5771 - f1_perClass: 0.8578 - acc: 0.8473     \n",
      "Epoch 448/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5551 - f1_perRow: 0.5882 - f1_perClass: 0.8615 - acc: 0.8481     \n",
      "Epoch 449/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5741 - f1_perRow: 0.5690 - f1_perClass: 0.8586 - acc: 0.8479     \n",
      "Epoch 450/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5629 - f1_perRow: 0.5838 - f1_perClass: 0.8604 - acc: 0.8484     \n",
      "Epoch 451/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5633 - f1_perRow: 0.5853 - f1_perClass: 0.8602 - acc: 0.8478     \n",
      "Epoch 452/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5546 - f1_perRow: 0.5845 - f1_perClass: 0.8614 - acc: 0.8473     \n",
      "Epoch 453/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5876 - f1_perRow: 0.5697 - f1_perClass: 0.8563 - acc: 0.8468     \n",
      "Epoch 454/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5570 - f1_perRow: 0.5891 - f1_perClass: 0.8610 - acc: 0.8483     \n",
      "Epoch 455/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5603 - f1_perRow: 0.5877 - f1_perClass: 0.8608 - acc: 0.8480     \n",
      "Epoch 456/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5706 - f1_perRow: 0.5858 - f1_perClass: 0.8592 - acc: 0.8485     \n",
      "Epoch 457/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5650 - f1_perRow: 0.5839 - f1_perClass: 0.8599 - acc: 0.8477     \n",
      "Epoch 458/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5567 - f1_perRow: 0.5871 - f1_perClass: 0.8611 - acc: 0.8478     \n",
      "Epoch 459/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5588 - f1_perRow: 0.5871 - f1_perClass: 0.8610 - acc: 0.8475     \n",
      "Epoch 460/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5603 - f1_perRow: 0.5831 - f1_perClass: 0.8605 - acc: 0.8476     \n",
      "Epoch 461/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5589 - f1_perRow: 0.5849 - f1_perClass: 0.8608 - acc: 0.8477     \n",
      "Epoch 462/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5792 - f1_perRow: 0.5727 - f1_perClass: 0.8577 - acc: 0.8474     \n",
      "Epoch 463/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5826 - f1_perRow: 0.5735 - f1_perClass: 0.8570 - acc: 0.8472     \n",
      "Epoch 464/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5761 - f1_perRow: 0.5769 - f1_perClass: 0.8578 - acc: 0.8472     \n",
      "Epoch 465/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5735 - f1_perRow: 0.5805 - f1_perClass: 0.8585 - acc: 0.8471     \n",
      "Epoch 466/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5556 - f1_perRow: 0.5864 - f1_perClass: 0.8610 - acc: 0.8480     \n",
      "Epoch 467/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5556 - f1_perRow: 0.5932 - f1_perClass: 0.8613 - acc: 0.8480     \n",
      "Epoch 468/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5611 - f1_perRow: 0.5919 - f1_perClass: 0.8605 - acc: 0.8490     \n",
      "Epoch 469/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5797 - f1_perRow: 0.5725 - f1_perClass: 0.8578 - acc: 0.8476     \n",
      "Epoch 470/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5675 - f1_perRow: 0.5841 - f1_perClass: 0.8595 - acc: 0.8481     \n",
      "Epoch 471/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5591 - f1_perRow: 0.5907 - f1_perClass: 0.8608 - acc: 0.8473     \n",
      "Epoch 472/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5712 - f1_perRow: 0.5778 - f1_perClass: 0.8589 - acc: 0.8481     \n",
      "Epoch 473/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5563 - f1_perRow: 0.5988 - f1_perClass: 0.8612 - acc: 0.8481     \n",
      "Epoch 474/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5589 - f1_perRow: 0.5888 - f1_perClass: 0.8610 - acc: 0.8479     \n",
      "Epoch 475/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5573 - f1_perRow: 0.5943 - f1_perClass: 0.8610 - acc: 0.8479     \n",
      "Epoch 476/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5577 - f1_perRow: 0.5829 - f1_perClass: 0.8613 - acc: 0.8475     \n",
      "Epoch 477/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5627 - f1_perRow: 0.5786 - f1_perClass: 0.8600 - acc: 0.8475     \n",
      "Epoch 478/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5721 - f1_perRow: 0.5736 - f1_perClass: 0.8585 - acc: 0.8471     \n",
      "Epoch 479/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5751 - f1_perRow: 0.5758 - f1_perClass: 0.8585 - acc: 0.8477     \n",
      "Epoch 480/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5716 - f1_perRow: 0.5808 - f1_perClass: 0.8591 - acc: 0.8487     \n",
      "Epoch 481/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5712 - f1_perRow: 0.5817 - f1_perClass: 0.8590 - acc: 0.8482     \n",
      "Epoch 482/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5639 - f1_perRow: 0.5775 - f1_perClass: 0.8598 - acc: 0.8482     \n",
      "Epoch 483/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5691 - f1_perRow: 0.5853 - f1_perClass: 0.8594 - acc: 0.8475     \n",
      "Epoch 484/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5816 - f1_perRow: 0.5681 - f1_perClass: 0.8571 - acc: 0.8474     \n",
      "Epoch 485/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5584 - f1_perRow: 0.5796 - f1_perClass: 0.8609 - acc: 0.8475     \n",
      "Epoch 486/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5569 - f1_perRow: 0.5875 - f1_perClass: 0.8612 - acc: 0.8475     \n",
      "Epoch 487/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5798 - f1_perClass: 0.8603 - acc: 0.8483     \n",
      "Epoch 488/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5658 - f1_perRow: 0.5849 - f1_perClass: 0.8600 - acc: 0.8476     \n",
      "Epoch 489/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5619 - f1_perRow: 0.5872 - f1_perClass: 0.8601 - acc: 0.8484     \n",
      "Epoch 490/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5557 - f1_perRow: 0.5892 - f1_perClass: 0.8612 - acc: 0.8490     \n",
      "Epoch 491/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5822 - f1_perClass: 0.8600 - acc: 0.8481     \n",
      "Epoch 492/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5802 - f1_perRow: 0.5740 - f1_perClass: 0.8580 - acc: 0.8473     \n",
      "Epoch 493/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5706 - f1_perRow: 0.5808 - f1_perClass: 0.8592 - acc: 0.8470     \n",
      "Epoch 494/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5646 - f1_perRow: 0.5826 - f1_perClass: 0.8598 - acc: 0.8478     \n",
      "Epoch 495/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5813 - f1_perRow: 0.5683 - f1_perClass: 0.8574 - acc: 0.8474     \n",
      "Epoch 496/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5610 - f1_perRow: 0.5851 - f1_perClass: 0.8606 - acc: 0.8487     \n",
      "Epoch 497/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5646 - f1_perRow: 0.5761 - f1_perClass: 0.8598 - acc: 0.8473     \n",
      "Epoch 498/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5690 - f1_perRow: 0.5819 - f1_perClass: 0.8591 - acc: 0.8475     \n",
      "Epoch 499/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5622 - f1_perRow: 0.5883 - f1_perClass: 0.8601 - acc: 0.8484     \n",
      "Epoch 500/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5821 - f1_perRow: 0.5690 - f1_perClass: 0.8577 - acc: 0.8471     \n",
      "Epoch 501/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5583 - f1_perRow: 0.5899 - f1_perClass: 0.8609 - acc: 0.8472     \n",
      "Epoch 502/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5577 - f1_perRow: 0.5889 - f1_perClass: 0.8609 - acc: 0.8475     \n",
      "Epoch 503/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5610 - f1_perRow: 0.5891 - f1_perClass: 0.8607 - acc: 0.8475     \n",
      "Epoch 504/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5908 - f1_perRow: 0.5695 - f1_perClass: 0.8562 - acc: 0.8477     \n",
      "Epoch 505/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5671 - f1_perRow: 0.5843 - f1_perClass: 0.8594 - acc: 0.8478     \n",
      "Epoch 506/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5665 - f1_perRow: 0.5811 - f1_perClass: 0.8596 - acc: 0.8482     \n",
      "Epoch 507/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5591 - f1_perRow: 0.5880 - f1_perClass: 0.8609 - acc: 0.8483     \n",
      "Epoch 508/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5668 - f1_perRow: 0.5873 - f1_perClass: 0.8594 - acc: 0.8476     \n",
      "Epoch 509/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5598 - f1_perRow: 0.5769 - f1_perClass: 0.8606 - acc: 0.8478     \n",
      "Epoch 510/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5577 - f1_perRow: 0.5912 - f1_perClass: 0.8609 - acc: 0.8477     \n",
      "Epoch 511/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5654 - f1_perRow: 0.5745 - f1_perClass: 0.8598 - acc: 0.8477     \n",
      "Epoch 512/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5727 - f1_perRow: 0.5762 - f1_perClass: 0.8585 - acc: 0.8474     \n",
      "Epoch 513/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5640 - f1_perRow: 0.5884 - f1_perClass: 0.8598 - acc: 0.8483     \n",
      "Epoch 514/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5576 - f1_perRow: 0.5832 - f1_perClass: 0.8609 - acc: 0.8489     \n",
      "Epoch 515/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5620 - f1_perRow: 0.5867 - f1_perClass: 0.8605 - acc: 0.8480     \n",
      "Epoch 516/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5588 - f1_perRow: 0.5820 - f1_perClass: 0.8609 - acc: 0.8479     \n",
      "Epoch 517/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5858 - f1_perRow: 0.5720 - f1_perClass: 0.8565 - acc: 0.8483     \n",
      "Epoch 518/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5827 - f1_perRow: 0.5752 - f1_perClass: 0.8571 - acc: 0.8467     \n",
      "Epoch 519/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5647 - f1_perRow: 0.5794 - f1_perClass: 0.8597 - acc: 0.8479     \n",
      "Epoch 520/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5696 - f1_perRow: 0.5781 - f1_perClass: 0.8588 - acc: 0.8480     \n",
      "Epoch 521/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5737 - f1_perRow: 0.5686 - f1_perClass: 0.8584 - acc: 0.8474     \n",
      "Epoch 522/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5662 - f1_perRow: 0.5845 - f1_perClass: 0.8600 - acc: 0.8481     \n",
      "Epoch 523/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5620 - f1_perRow: 0.5844 - f1_perClass: 0.8603 - acc: 0.8477     \n",
      "Epoch 524/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5735 - f1_perRow: 0.5823 - f1_perClass: 0.8588 - acc: 0.8475     \n",
      "Epoch 525/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5574 - f1_perRow: 0.5907 - f1_perClass: 0.8609 - acc: 0.8475     \n",
      "Epoch 526/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5699 - f1_perRow: 0.5795 - f1_perClass: 0.8594 - acc: 0.8477     \n",
      "Epoch 527/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5633 - f1_perRow: 0.5900 - f1_perClass: 0.8600 - acc: 0.8479     \n",
      "Epoch 528/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5583 - f1_perRow: 0.5864 - f1_perClass: 0.8608 - acc: 0.8478     \n",
      "Epoch 529/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5639 - f1_perRow: 0.5821 - f1_perClass: 0.8601 - acc: 0.8478     \n",
      "Epoch 530/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5645 - f1_perRow: 0.5812 - f1_perClass: 0.8600 - acc: 0.8477     \n",
      "Epoch 531/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5704 - f1_perRow: 0.5839 - f1_perClass: 0.8591 - acc: 0.8474     \n",
      "Epoch 532/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5633 - f1_perRow: 0.5751 - f1_perClass: 0.8601 - acc: 0.8478     \n",
      "Epoch 533/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5823 - f1_perRow: 0.5695 - f1_perClass: 0.8573 - acc: 0.8472     \n",
      "Epoch 534/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5608 - f1_perRow: 0.5851 - f1_perClass: 0.8605 - acc: 0.8477     \n",
      "Epoch 535/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5720 - f1_perRow: 0.5772 - f1_perClass: 0.8587 - acc: 0.8481     \n",
      "Epoch 536/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5873 - f1_perClass: 0.8606 - acc: 0.8478     \n",
      "Epoch 537/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5711 - f1_perRow: 0.5842 - f1_perClass: 0.8590 - acc: 0.8479     \n",
      "Epoch 538/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5555 - f1_perRow: 0.5894 - f1_perClass: 0.8612 - acc: 0.8477     \n",
      "Epoch 539/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5582 - f1_perRow: 0.5870 - f1_perClass: 0.8609 - acc: 0.8475     \n",
      "Epoch 540/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5548 - f1_perRow: 0.5945 - f1_perClass: 0.8615 - acc: 0.8480     \n",
      "Epoch 541/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5620 - f1_perRow: 0.5875 - f1_perClass: 0.8603 - acc: 0.8473     \n",
      "Epoch 542/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5786 - f1_perRow: 0.5705 - f1_perClass: 0.8575 - acc: 0.8474     \n",
      "Epoch 543/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5659 - f1_perRow: 0.5833 - f1_perClass: 0.8597 - acc: 0.8479     \n",
      "Epoch 544/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5674 - f1_perRow: 0.5798 - f1_perClass: 0.8597 - acc: 0.8473     \n",
      "Epoch 545/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5698 - f1_perRow: 0.5780 - f1_perClass: 0.8592 - acc: 0.8479     \n",
      "Epoch 546/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5663 - f1_perRow: 0.5827 - f1_perClass: 0.8597 - acc: 0.8481     \n",
      "Epoch 547/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5642 - f1_perRow: 0.5839 - f1_perClass: 0.8600 - acc: 0.8484     \n",
      "Epoch 548/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5885 - f1_perRow: 0.5716 - f1_perClass: 0.8561 - acc: 0.8477     \n",
      "Epoch 549/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5561 - f1_perRow: 0.5887 - f1_perClass: 0.8613 - acc: 0.8484     \n",
      "Epoch 550/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5734 - f1_perRow: 0.5768 - f1_perClass: 0.8588 - acc: 0.8481     \n",
      "Epoch 551/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5697 - f1_perRow: 0.5833 - f1_perClass: 0.8592 - acc: 0.8482     \n",
      "Epoch 552/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5698 - f1_perRow: 0.5812 - f1_perClass: 0.8591 - acc: 0.8476     \n",
      "Epoch 553/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5710 - f1_perRow: 0.5732 - f1_perClass: 0.8586 - acc: 0.8477     \n",
      "Epoch 554/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5677 - f1_perRow: 0.5812 - f1_perClass: 0.8594 - acc: 0.8478     \n",
      "Epoch 555/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5599 - f1_perRow: 0.5832 - f1_perClass: 0.8606 - acc: 0.8478     \n",
      "Epoch 556/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5799 - f1_perClass: 0.8601 - acc: 0.8480     \n",
      "Epoch 557/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5614 - f1_perRow: 0.5819 - f1_perClass: 0.8605 - acc: 0.8482     \n",
      "Epoch 558/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5711 - f1_perRow: 0.5755 - f1_perClass: 0.8584 - acc: 0.8479     \n",
      "Epoch 559/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5649 - f1_perRow: 0.5789 - f1_perClass: 0.8597 - acc: 0.8483     \n",
      "Epoch 560/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5675 - f1_perRow: 0.5822 - f1_perClass: 0.8594 - acc: 0.8480     \n",
      "Epoch 561/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5607 - f1_perRow: 0.5784 - f1_perClass: 0.8603 - acc: 0.8468     \n",
      "Epoch 562/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5607 - f1_perRow: 0.5830 - f1_perClass: 0.8605 - acc: 0.8474     \n",
      "Epoch 563/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5573 - f1_perRow: 0.5882 - f1_perClass: 0.8610 - acc: 0.8488     \n",
      "Epoch 564/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5606 - f1_perRow: 0.5882 - f1_perClass: 0.8606 - acc: 0.8483     \n",
      "Epoch 565/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5759 - f1_perRow: 0.5808 - f1_perClass: 0.8578 - acc: 0.8477     \n",
      "Epoch 566/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5618 - f1_perRow: 0.5820 - f1_perClass: 0.8604 - acc: 0.8477     \n",
      "Epoch 567/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5651 - f1_perRow: 0.5825 - f1_perClass: 0.8599 - acc: 0.8472     \n",
      "Epoch 568/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5586 - f1_perRow: 0.5842 - f1_perClass: 0.8610 - acc: 0.8484     \n",
      "Epoch 569/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5821 - f1_perClass: 0.8600 - acc: 0.8473     \n",
      "Epoch 570/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5627 - f1_perRow: 0.5820 - f1_perClass: 0.8603 - acc: 0.8482     \n",
      "Epoch 571/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5782 - f1_perRow: 0.5684 - f1_perClass: 0.8577 - acc: 0.8475     \n",
      "Epoch 572/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5646 - f1_perRow: 0.5801 - f1_perClass: 0.8598 - acc: 0.8479     \n",
      "Epoch 573/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5705 - f1_perRow: 0.5782 - f1_perClass: 0.8591 - acc: 0.8477     \n",
      "Epoch 574/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5664 - f1_perRow: 0.5859 - f1_perClass: 0.8596 - acc: 0.8473     \n",
      "Epoch 575/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5965 - f1_perRow: 0.5632 - f1_perClass: 0.8547 - acc: 0.8471     \n",
      "Epoch 576/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.6008 - f1_perRow: 0.5702 - f1_perClass: 0.8532 - acc: 0.8442     \n",
      "Epoch 577/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5778 - f1_perClass: 0.8598 - acc: 0.8476     \n",
      "Epoch 578/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5759 - f1_perRow: 0.5849 - f1_perClass: 0.8583 - acc: 0.8474     \n",
      "Epoch 579/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5645 - f1_perRow: 0.5815 - f1_perClass: 0.8599 - acc: 0.8477     \n",
      "Epoch 580/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5634 - f1_perRow: 0.5837 - f1_perClass: 0.8600 - acc: 0.8483     \n",
      "Epoch 581/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5600 - f1_perRow: 0.5878 - f1_perClass: 0.8608 - acc: 0.8476     \n",
      "Epoch 582/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5658 - f1_perRow: 0.5764 - f1_perClass: 0.8596 - acc: 0.8472     \n",
      "Epoch 583/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5609 - f1_perRow: 0.5837 - f1_perClass: 0.8604 - acc: 0.8479     \n",
      "Epoch 584/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5702 - f1_perRow: 0.5821 - f1_perClass: 0.8593 - acc: 0.8482     \n",
      "Epoch 585/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5661 - f1_perRow: 0.5835 - f1_perClass: 0.8596 - acc: 0.8476     \n",
      "Epoch 586/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5637 - f1_perRow: 0.5847 - f1_perClass: 0.8598 - acc: 0.8473     \n",
      "Epoch 587/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5617 - f1_perRow: 0.5830 - f1_perClass: 0.8604 - acc: 0.8484     \n",
      "Epoch 588/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5753 - f1_perRow: 0.5766 - f1_perClass: 0.8582 - acc: 0.8477     \n",
      "Epoch 589/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5734 - f1_perRow: 0.5765 - f1_perClass: 0.8583 - acc: 0.8471     \n",
      "Epoch 590/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5685 - f1_perRow: 0.5738 - f1_perClass: 0.8593 - acc: 0.8481     \n",
      "Epoch 591/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5621 - f1_perRow: 0.5830 - f1_perClass: 0.8603 - acc: 0.8479     \n",
      "Epoch 592/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5648 - f1_perRow: 0.5758 - f1_perClass: 0.8598 - acc: 0.8470     \n",
      "Epoch 593/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5678 - f1_perRow: 0.5762 - f1_perClass: 0.8592 - acc: 0.8468     \n",
      "Epoch 594/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5620 - f1_perRow: 0.5787 - f1_perClass: 0.8602 - acc: 0.8486     \n",
      "Epoch 595/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5665 - f1_perRow: 0.5828 - f1_perClass: 0.8596 - acc: 0.8479     \n",
      "Epoch 596/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5681 - f1_perRow: 0.5825 - f1_perClass: 0.8592 - acc: 0.8476     \n",
      "Epoch 597/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5619 - f1_perRow: 0.5828 - f1_perClass: 0.8606 - acc: 0.8482     \n",
      "Epoch 598/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5717 - f1_perRow: 0.5712 - f1_perClass: 0.8590 - acc: 0.8477     \n",
      "Epoch 599/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5541 - f1_perRow: 0.5898 - f1_perClass: 0.8615 - acc: 0.8473     \n",
      "Epoch 600/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5638 - f1_perRow: 0.5896 - f1_perClass: 0.8601 - acc: 0.8484     \n",
      "Epoch 601/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5553 - f1_perRow: 0.5859 - f1_perClass: 0.8613 - acc: 0.8477     \n",
      "Epoch 602/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5684 - f1_perRow: 0.5856 - f1_perClass: 0.8589 - acc: 0.8472     \n",
      "Epoch 603/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5643 - f1_perRow: 0.5804 - f1_perClass: 0.8602 - acc: 0.8474     \n",
      "Epoch 604/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5677 - f1_perRow: 0.5814 - f1_perClass: 0.8593 - acc: 0.8474     \n",
      "Epoch 605/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5555 - f1_perRow: 0.5867 - f1_perClass: 0.8616 - acc: 0.8475     \n",
      "Epoch 606/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5908 - f1_perRow: 0.5709 - f1_perClass: 0.8556 - acc: 0.8476     \n",
      "Epoch 607/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5575 - f1_perRow: 0.5906 - f1_perClass: 0.8611 - acc: 0.8482     \n",
      "Epoch 608/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5568 - f1_perRow: 0.5875 - f1_perClass: 0.8611 - acc: 0.8484     \n",
      "Epoch 609/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5716 - f1_perRow: 0.5839 - f1_perClass: 0.8590 - acc: 0.8481     \n",
      "Epoch 610/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5590 - f1_perRow: 0.5839 - f1_perClass: 0.8610 - acc: 0.8484     \n",
      "Epoch 611/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5754 - f1_perRow: 0.5760 - f1_perClass: 0.8584 - acc: 0.8483     \n",
      "Epoch 612/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5799 - f1_perRow: 0.5712 - f1_perClass: 0.8577 - acc: 0.8475     \n",
      "Epoch 613/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5601 - f1_perRow: 0.5844 - f1_perClass: 0.8606 - acc: 0.8482     \n",
      "Epoch 614/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5693 - f1_perRow: 0.5723 - f1_perClass: 0.8592 - acc: 0.8478     \n",
      "Epoch 615/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5631 - f1_perRow: 0.5819 - f1_perClass: 0.8604 - acc: 0.8486     \n",
      "Epoch 616/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5785 - f1_perClass: 0.8596 - acc: 0.8477     \n",
      "Epoch 617/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5730 - f1_perRow: 0.5785 - f1_perClass: 0.8586 - acc: 0.8472     \n",
      "Epoch 618/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5675 - f1_perRow: 0.5804 - f1_perClass: 0.8592 - acc: 0.8474     \n",
      "Epoch 619/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5727 - f1_perRow: 0.5797 - f1_perClass: 0.8587 - acc: 0.8476     \n",
      "Epoch 620/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5591 - f1_perRow: 0.5784 - f1_perClass: 0.8606 - acc: 0.8474     \n",
      "Epoch 621/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5673 - f1_perRow: 0.5843 - f1_perClass: 0.8594 - acc: 0.8484     \n",
      "Epoch 622/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5584 - f1_perRow: 0.5838 - f1_perClass: 0.8607 - acc: 0.8479     \n",
      "Epoch 623/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5617 - f1_perRow: 0.5853 - f1_perClass: 0.8603 - acc: 0.8473     \n",
      "Epoch 624/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5718 - f1_perRow: 0.5729 - f1_perClass: 0.8587 - acc: 0.8479     \n",
      "Epoch 625/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5597 - f1_perRow: 0.5884 - f1_perClass: 0.8610 - acc: 0.8481     \n",
      "Epoch 626/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5685 - f1_perRow: 0.5741 - f1_perClass: 0.8592 - acc: 0.8472     \n",
      "Epoch 627/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5636 - f1_perRow: 0.5875 - f1_perClass: 0.8599 - acc: 0.8475     \n",
      "Epoch 628/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5684 - f1_perRow: 0.5794 - f1_perClass: 0.8593 - acc: 0.8482     \n",
      "Epoch 629/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5637 - f1_perRow: 0.5842 - f1_perClass: 0.8596 - acc: 0.8478     \n",
      "Epoch 630/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5606 - f1_perRow: 0.5839 - f1_perClass: 0.8604 - acc: 0.8478     \n",
      "Epoch 631/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5638 - f1_perRow: 0.5859 - f1_perClass: 0.8602 - acc: 0.8475     \n",
      "Epoch 632/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5595 - f1_perRow: 0.5879 - f1_perClass: 0.8608 - acc: 0.8473     \n",
      "Epoch 633/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5676 - f1_perRow: 0.5759 - f1_perClass: 0.8596 - acc: 0.8483     \n",
      "Epoch 634/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5949 - f1_perRow: 0.5702 - f1_perClass: 0.8548 - acc: 0.8469     \n",
      "Epoch 635/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5695 - f1_perRow: 0.5786 - f1_perClass: 0.8594 - acc: 0.8475     \n",
      "Epoch 636/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5768 - f1_perRow: 0.5766 - f1_perClass: 0.8585 - acc: 0.8474     \n",
      "Epoch 637/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5643 - f1_perRow: 0.5841 - f1_perClass: 0.8599 - acc: 0.8484     \n",
      "Epoch 638/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5659 - f1_perRow: 0.5794 - f1_perClass: 0.8595 - acc: 0.8475     \n",
      "Epoch 639/3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65266/65266 [==============================] - 3s - loss: 0.5566 - f1_perRow: 0.5875 - f1_perClass: 0.8612 - acc: 0.8478     \n",
      "Epoch 640/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5742 - f1_perRow: 0.5808 - f1_perClass: 0.8587 - acc: 0.8485     \n",
      "Epoch 641/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5680 - f1_perRow: 0.5802 - f1_perClass: 0.8592 - acc: 0.8482     \n",
      "Epoch 642/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5612 - f1_perRow: 0.5890 - f1_perClass: 0.8604 - acc: 0.8475     \n",
      "Epoch 643/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5728 - f1_perRow: 0.5743 - f1_perClass: 0.8587 - acc: 0.8475     \n",
      "Epoch 644/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5686 - f1_perRow: 0.5784 - f1_perClass: 0.8592 - acc: 0.8474     \n",
      "Epoch 645/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5754 - f1_perRow: 0.5733 - f1_perClass: 0.8585 - acc: 0.8475     \n",
      "Epoch 646/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5660 - f1_perRow: 0.5842 - f1_perClass: 0.8595 - acc: 0.8481     \n",
      "Epoch 647/3300\n",
      "65266/65266 [==============================] - 3s - loss: 0.5742 - f1_perRow: 0.5746 - f1_perClass: 0.8583 - acc: 0.8476     \n",
      "Epoch 648/3300\n",
      "57500/65266 [=========================>....] - ETA: 0s - loss: 0.5597 - f1_perRow: 0.5794 - f1_perClass: 0.8603 - acc: 0.8477"
     ]
    }
   ],
   "source": [
    "model2.compile(loss=losses, optimizer=keras.optimizers.Adam(lr=1e-8  ), metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=3300, batch_size=11500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=3500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM-sigmoid-withRemovedClasses\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "# plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model2, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2=load_model( \"LSTM_withSigmoid_LargeData_F1_E100_B500_MSE_False\"  \n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    makeReadable( classes=classes, confidance=0.7,data=lstm_tests[i][0],gt=lstm_tests[i][1],model=model2,path=test_names[i],x=lstm_tests[i][0])\n",
    "    \n",
    "#     lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[1][0])\n",
    "lstm_pred__ = np.array(list(lstm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot_new/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.997      0.351     0.541     0.426        57 20/19894/17/37\n",
      "                      activity     0.999      0.684     0.684     0.684        19 13/19943/6/6\n",
      "                       battery     0.999      0.000     0.000     0.000         7 0/19951/10/7\n",
      "                        button     1.000      0.500     0.875     0.636        14 7/19953/1/7\n",
      "              colorTemperature     1.000      0.667     0.800     0.727         6 4/19961/1/2\n",
      "                       contact     0.996      0.636     0.848     0.727       176 112/19772/20/64\n",
      "                         level     0.999      0.333     0.500     0.400        27 9/19932/9/18\n",
      "                          lock     0.996      0.771     0.287     0.419        35 27/19866/67/8\n",
      "                        motion     0.981      0.000     0.000     0.000       371 0/19591/6/371\n",
      "                          ping     0.996      0.999     0.985     0.992      4842 4836/15052/74/6\n",
      "                        status     0.997      0.790     0.783     0.787       119 94/19823/26/25\n",
      "                        switch     1.000      0.762     0.941     0.842        21 16/19946/1/5\n",
      "                   temperature     0.941      0.000     0.000     0.000      1168 0/18795/5/1168\n",
      "                     threeAxis     0.997      0.492     0.633     0.554        63 31/19887/18/32\n",
      "                       unknown     0.892      0.992     0.866     0.925     13305 13204/4617/2046/101\n",
      "                         water     1.000        nan     0.000     0.000         0 0/19960/8/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.987      0.499     0.546     0.507     19968 0/0/0/0\n",
      "Exact Match ACC : 0.88567 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 452 (0.023)%\n",
      "Total ZXeros in Test : 65 (0.003)%\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "lstm_pred__ = np.array(list(lstm_pred))\n",
    "print_info( lstm_tests[1][1], lstm_pred__, classes , confidance=0.59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1] :\n",
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [1] :\n",
    "# for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.992)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_train, y_lstm_prossed_train, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x  in lstm_pred  if  np.sum(x) > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM_withSigmoid_LargeData_F%s_E%d_B%d_M%s_%r\" %\n",
    "            (\n",
    "            FoldID,\n",
    "                Epoch_count,\n",
    "                Batch_size,\n",
    "                Mapper,\n",
    "                IgnoreEmpty\n",
    "            ) \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_muhammed,y_train_muhammed, classes = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False)\n",
    "# x_test_muhammed,y_test_muhammed, classes = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( y_lstm_prossed_train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size =160\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1]), len(y_lstm_prossed_test  ) , len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1])/len(y_lstm_prossed_test  ) *1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x  in  pred if np.sum(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_lstm_prossed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_for_raun( pred   ):\n",
    "    pp = pred\n",
    "    pp[pp>=0.5] = 1\n",
    "    pp[pp<0.5] = 0\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if np.sum( do_for_raun(x) )==0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if  do_for_raun(x)[20] ==1 or do_for_raun(x)[21] ==1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.save(\"../files/muhammed/x_train.json\" , x_train_muhammed)\n",
    "# np.save(\"../files/muhammed/y_train.json\", )\n",
    "\n",
    "\n",
    "# np.save( \"../files/muhammed/x_train.json\", x_train_muhammed )\n",
    "# np.save(\"../files/muhammed/y_train.json\",  y_train_muhammed )\n",
    "# np.save( \"../files/muhammed/x_test.json\",x_test_muhammed )\n",
    "# np.save( \"../files/muhammed/y_test.json\",y_test_muhammed )\n",
    "# np.save( \"../files/muhammed/classes.json\",  classes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_lstm_prossed_test) + len(x_lstm_prossed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(new_iot)",
   "language": "python",
   "name": "iot_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
