{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import keras \n",
    "\n",
    "# import numpy\n",
    "# import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFromMerged=True\n",
    "loadFromIndexes= False\n",
    "Mapper='S'\n",
    "IgnoreEmpty= True\n",
    "FoldID =\"1\"\n",
    "Epoch_count=100\n",
    "Batch_size=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data the old way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data= [] \n",
    "# y_data= [] \n",
    "\n",
    "\n",
    "# with open( '../files/txt/seq_mapping_large.txt' ) as f:\n",
    "#     x_data =   f.readlines()\n",
    "\n",
    "# with open( '../files/txt/command_mapping_large.txt' ) as f:\n",
    "#     y_data = f.readlines()\n",
    "    \n",
    "    \n",
    "# x_data =[ np.array([ int(y) for y in x.strip().split( ' ') ])   for x in  x_data ] \n",
    "# y_data =[ x.strip().split(' ') for x in  y_data ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load The Data The New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mapps the input records to a integer array for the input\n",
    "def mapping_x( inp, includeDirection = False , TrimAt= 15 ):\n",
    "    if includeDirection:\n",
    "        return np.array([ int(x[\"packet_length\"]) * (1 if x['packet_source']=='hub' else -1)  for x in inp ][:15])\n",
    "    else:\n",
    "        return np.array([ int(x[\"packet_length\"])  for x in inp ][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_y_service(inp):\n",
    "    return np.array(  list(set([x[\"event\"] for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_service_event(inp):\n",
    "    return np.array(  list(set([ \"%s-%s\"%( x[\"event\"] ,x[\"val\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_device_service(inp):\n",
    "    return np.array(  list(set([ \"%s & %s\"%( x[\"device\"] ,x[\"event\"] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n",
    "\n",
    "def mapping_y_full(inp):\n",
    "    return np.array(  list(set([ \"%s & %s & %s\"%( x[\"device\"] ,x[\"event\"], x['val'] ) for x in inp])) if (len(inp )>0) else [\"none\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cleans the data removing emply nodes and turning the nodes into sarrays by calling the mapping function \n",
    "def clean_data( x_data, y_data , removeempty=True, Mapping='S'):\n",
    "    cleans = [] \n",
    "    cleans = (sorted([ x for x in y_data if (removeempty and len(y_data[x]) > 0) or not removeempty  ] ))\n",
    "    \n",
    "    ret_x  = [x_data[x] for x in cleans]\n",
    "    ret_y  = [y_data[x] for x in cleans] \n",
    "    \n",
    "    print( len(y_data), len(cleans) )\n",
    "    \n",
    "    ret_x  = [ mapping_x(x) for x in ret_x ] \n",
    "    ret_y_s = [ mapping_y_service(y) for y in ret_y ]\n",
    "    if Mapping=='S':\n",
    "        ret_y  = [ mapping_y_service(y) for y in ret_y ]\n",
    "    elif Mapping=='SE':\n",
    "        ret_y  = [ mapping_y_service_event(y) for y in ret_y ]\n",
    "    elif Mapping=='DS':\n",
    "        ret_y  = [ mapping_y_device_service(y) for y in ret_y ]\n",
    "    elif Mapping=='F':\n",
    "        ret_y  = [ mapping_y_full(y) for y in ret_y ]\n",
    "    return ret_x, ret_y, ret_y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in load from merged\n",
      "58958 57867\n",
      "loading from test files\n",
      "found files :  4\n",
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57867, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= []\n",
    "y= []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "y_test_service= []\n",
    "\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "test_names = []\n",
    "\n",
    "add_to_trainig = [0,2]\n",
    "\n",
    "if loadFromMerged:\n",
    "    print(\"in load from merged\")\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_hub_segments_final.json'  ) as f:\n",
    "        y_data = json.load(f)\n",
    "\n",
    "    with open(  '../files/train/test/test_homes/final_upload/Merged_final_with_home/merged_pcap_segments_final.json'  ) as f:\n",
    "        x_data = json.load(f)\n",
    "        \n",
    "#     with open(  '../files/train/merged/hub_segments_2.json'  ) as f:\n",
    "#         y_data = json.load(f)\n",
    "\n",
    "#     with open(  '../files/train/merged/pcap_segments_2.json'  ) as f:\n",
    "#         x_data = json.load(f)\n",
    "  \n",
    "    if len( y_data ) != len(x_data) :\n",
    "        print( pick )\n",
    "        \n",
    "    \n",
    "    x_train,y_train, y_train_service= clean_data( x_data, y_data, IgnoreEmpty , Mapping=Mapper )\n",
    "    \n",
    "    #     continue\n",
    "#     if loadFromIndexes:\n",
    "#         print(\"load from indexes\")\n",
    "#         with open(\"../files/train/merged/items_2_test-train_indexes.json\")  as f:\n",
    "#             index_info = json.load(f)\n",
    "\n",
    "\n",
    "#         for i in index_info[FoldID][\"test\"]:\n",
    "#             x_test[str(i)]=(x_data[str(i)] )\n",
    "#             y_test[str(i)]=(y_data[str(i)] )\n",
    "\n",
    "#         for i in index_info[FoldID][\"train\"]:\n",
    "#             x_train[str(i)]=(  x_data[str(i)] )\n",
    "#             y_train[str(i)]=(  y_data[str(i)] )\n",
    "        \n",
    "#         x_test_t,y_test_t= clean_data( x_test, y_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#         x_test.append(x_test_t)\n",
    "#         y_test.append(y_test_t)\n",
    "    #     else :\n",
    "    print(\"loading from test files\")\n",
    "    test_files = sorted(glob.glob( '../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/home*.json' ))\n",
    "    print( \"found files : \" , len(test_files) )\n",
    "    for pick  in test_files:\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/test/test_homes/final_upload/usecases/hub_segments_final_final/', fname) ) as f:\n",
    "            y_data_test = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/test/test_homes/final_upload/usecases/pcap_segments_final_final/', fname) ) as f:\n",
    "            x_data_test = json.load(f)\n",
    "\n",
    "\n",
    "        t_x,t_y, t_z= clean_data( x_data_test, y_data_test, False , Mapping=Mapper )\n",
    "\n",
    "#         if test_files.index(pick) in add_to_trainig:\n",
    "#             x_test_t,y_test_t, y_test_service_t= clean_data( x_data_test, y_data_test, IgnoreEmpty , Mapping=Mapper)\n",
    "#             x_train.extend(x_test_t)\n",
    "#             y_train.extend(y_test_t)\n",
    "#             y_train_service.extend(y_test_service_t)\n",
    "\n",
    "\n",
    "        x_test.append(t_x)\n",
    "        y_test.append(t_y)\n",
    "        y_test_service.append(t_z)\n",
    "            \n",
    "#     x_test = x_data[ index_info[\"1\"][\"test\"]  ]\n",
    "#     y_test = y_data[ index_info[\"1\"][\"test\"]  ]\n",
    "    \n",
    "#     x_train = x_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     y_train = y_data[ index_info[\"1\"][\"train\"]  ]\n",
    "#     x.extend(t_x)\n",
    "#     y.extend(t_y)\n",
    "else:\n",
    "    for pick in sorted(glob.glob( '../files/train/hub_segments/*.json' )):\n",
    "        fname  = os.path.basename(pick)\n",
    "        test_names.append( fname )\n",
    "        with open( os.path.join( '../files/train/hub_segments/', fname) ) as f:\n",
    "            y_data = json.load(f)\n",
    "\n",
    "        with open( os.path.join('../files/train/pcap_segments/', fname) ) as f:\n",
    "            x_data = json.load(f)\n",
    "\n",
    "        if len( y_data ) != len(x_data) :\n",
    "            print( pick )\n",
    "            continue\n",
    "\n",
    "        t_x,t_y= clean_data( x_data, y_data, True )\n",
    "\n",
    "        x.extend( t_x)\n",
    "        y.extend(t_y)\n",
    "\n",
    "x= np.array(x)\n",
    "y= np.array(y)\n",
    "\n",
    "# x_train = np.append( x_train, x_test[0] , axis=0)\n",
    "# x_train = np.append( x_train, x_test[2] , axis=0)\n",
    "\n",
    "# y_train = np.append( y_train, y_test[0] , axis=0)\n",
    "# y_train = np.append( y_train, y_test[2] , axis=0)\n",
    "\n",
    "\n",
    "len(x_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Mittigation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# for ii in range(len(x_test)):\n",
    "#     p = x_test[ii]\n",
    "#     sizes = np.unique(np.concatenate(p), return_counts=True)\n",
    "#     sums= 0 \n",
    "#     for i in range(len(sizes[0])):\n",
    "# #         print( \"%d--> %d\" % ( sizes[0][i], sizes[1][i] ) )\n",
    "#         if  sizes[0][i] < 1000:\n",
    "#             sums+= (1000-sizes[0][i] )* sizes[1][i]\n",
    "#     t_sum +=(sums / days[ii] )/1000000 \n",
    "#     print ( (sums / days[ii] )/1000000)\n",
    "# print('--------')\n",
    "# print(t_sum/4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# d_sum = 0\n",
    "# import math\n",
    "# for ii in range(len(x_test)):\n",
    "\n",
    "#     p = x_test[ii]\n",
    "#     for i in p : \n",
    "#         t_sum += math.ceil(np.sum(i) / 2000)\n",
    "#         d_sum+= np.sum( i )\n",
    "# total_fixed  =  t_sum* 2000 / 15\n",
    "\n",
    "# print ( total_fixed , d_sum, d_sum-t_sum*2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days =[7,4,2,2]\n",
    "# t_sum =0\n",
    "# for ii in range(len(x_test)):\n",
    "#     p = x_test[ii]\n",
    "#     sizes = np.unique(np.concatenate(p), return_counts=True)\n",
    "#     sums= 0 \n",
    "#     for i in range(len(sizes[0])):\n",
    "# #         print( \"%d--> %d\" % ( sizes[0][i], sizes[1][i] ) )\n",
    "#         if  sizes[0][i] < 1000:\n",
    "#             sums+= (1000-sizes[0][i] )* sizes[1][i]\n",
    "#     t_sum +=(sums / days[ii] )/1000000 \n",
    "#     print ( (sums / days[ii] )/1000000)\n",
    "# print('--------')\n",
    "# print(t_sum/4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sets the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'acceleration'), (1, 'activity'), (2, 'battery'), (3, 'button'), (4, 'colorTemperature'), (5, 'contact'), (6, 'level'), (7, 'lock'), (8, 'motion'), (9, 'ping'), (10, 'status'), (11, 'switch'), (12, 'temperature'), (13, 'threeAxis'), (14, 'unknown'), (15, 'water')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'acceleration'),\n",
       " (1, 'activity'),\n",
       " (2, 'battery'),\n",
       " (3, 'button'),\n",
       " (4, 'colorTemperature'),\n",
       " (5, 'contact'),\n",
       " (6, 'level'),\n",
       " (7, 'lock'),\n",
       " (8, 'motion'),\n",
       " (9, 'ping'),\n",
       " (10, 'status'),\n",
       " (11, 'switch'),\n",
       " (12, 'temperature'),\n",
       " (13, 'threeAxis'),\n",
       " (14, 'unknown'),\n",
       " (15, 'water')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(list(np.unique(  np.concatenate( y_train  ))))\n",
    "print([ (i , classes[i]) for i in range( len(classes) ) ])\n",
    "\n",
    "service_classes = sorted(list(np.unique(  np.concatenate( y_train_service  ))))\n",
    "[ (i , service_classes[i]) for i in range( len(service_classes) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section controls wheather the unknown packets are removed or not, this should be tested with and without removed unknowns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean_event( inp, return_clean= True  ):\n",
    "    if return_clean:\n",
    "        return  'no_logs' not in inp and 'lock-unlocked' not in inp and 'on/off-XXX' not in inp and 'raw-XXX' not in inp and 'read_attr_-_raw-XXX' not in inp\n",
    "    else:\n",
    "        return  'lock-locked' in inp or 'lock-unlocked'  in inp or 'on/off-XXX' in inp or  'raw-XXX' in inp  or 'read_attr_-_raw-XXX' in inp \n",
    "    \n",
    "def is_clean_service( inp, return_clean= True  ):\n",
    "    \n",
    "    if return_clean:\n",
    "        return  'no_logs' not in inp and 'unknown' not in inp and 'read_attr_-_raw' not in inp #and 'ping' not in inp \n",
    "    else:\n",
    "        return  'no_logs' in inp or  'unknown' in inp  or 'read_attr_-_raw' in inp #or 'ping' in inp \n",
    "#     return  \"contact-open\" not in inp and 'contact-closed' not in inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toKeep = [ i for i in range(len(y_train)) if is_clean_event( y_train[i]) ] if Mapper=='SE' else [ i for i in range(len(y_train)) if is_clean_service( y_train[i]) ]\n",
    "x_train= [ x_train[i] for i in toKeep ]\n",
    "y_train= [ y_train[i] for i in toKeep ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(x_test)):\n",
    "    toChange= [ i for i in range(len(y_test[j])) if is_clean_service( y_test[j][i], False) ]\n",
    "    y_test[j] = [ (y_test[j][i] if i not in toChange else np.array( ['none'])) for i in range(len(y_test[j])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes.remove('read_attr_-_raw-XXX')\n",
    "# classes.remove('on/off-XXX')\n",
    "# classes.remove('raw-XXX')\n",
    "# classes.remove('lock-unlocked')\n",
    "# classes.remove('lock-locked')\n",
    "\n",
    "\n",
    "# classes.remove('read_attr_-_raw')\n",
    "# classes.remove('on/off')\n",
    "# classes.remove('raw')\n",
    "classes.remove('unknown')\n",
    "\n",
    "# classes.remove('lock')\n",
    "# # classes.remove('lock')\n",
    "\n",
    "\n",
    "# classes.remove('switch-on')\n",
    "\n",
    "\n",
    "\n",
    "service_classes= [\"\",\"\",\"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== end of unknown packet control====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_raw( x_data,y_data, dim_size = 128, zero_pad = False, normalize = False ,classes=None, twoD= False ):\n",
    "#  y data \n",
    "# \"\"\"\n",
    "# this functino is in charge of preprocessing the records , the sourc e json contains a lot of extra stuff, this function tailors\n",
    "# the data and it fixes their lenghth\n",
    "# \"\"\"\n",
    "    if classes is None:\n",
    "        classes  = sorted(list(np.unique(  np.concatenate( y_data  ))))\n",
    "    else :\n",
    "        classes = sorted(classes)\n",
    "    y_data_categorical = []  \n",
    "\n",
    "    for x in y_data:\n",
    "        temp = np.zeros( len(classes) )\n",
    "        for y in x : \n",
    "            if y in classes:\n",
    "                temp[ classes.index( y ) ] = 1\n",
    "        y_data_categorical.append( temp )\n",
    "    y_data_categorical = np.vstack(y_data_categorical)\n",
    "\n",
    "#     x_data = np.array( x_data) / 1500.0\n",
    "    \n",
    "    x_data_temp = [] \n",
    "    \n",
    "    if not zero_pad:\n",
    "        if twoD:\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size**2 - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size**2:\n",
    "                    temp.append( 0 )\n",
    "\n",
    "                x_data_temp.append(np.array( temp).reshape(dim_size,dim_size))\n",
    "\n",
    "\n",
    "            x_data_temp = np.array( x_data_temp )\n",
    "            x_data_temp=x_data_temp.reshape(x_data_temp.shape+(1,))\n",
    "        else: \n",
    "            temp = [] \n",
    "            lst = list(x)\n",
    "            for x in x_data:\n",
    "                temp = [] #list(x)\n",
    "                lst = list(x)\n",
    "                while dim_size - len(temp )   > len(lst):\n",
    "                    temp.extend(lst)\n",
    "\n",
    "                while len(temp) < dim_size:\n",
    "                    temp.append( 0 )\n",
    "                \n",
    "                x_data_temp.append(np.array( temp))\n",
    "            \n",
    "    else :\n",
    "        x_data_temp = sequence.pad_sequences(x_data, maxlen=dim_size)\n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        x_data_temp = np.array( x_data_temp) / 1500.0\n",
    "    else :\n",
    "        x_data_temp = np.array(x_data_temp)\n",
    "    \n",
    "    \n",
    "    return x_data_temp ,y_data_categorical , classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recall_shit( inp ):\n",
    "    tp = inp[1][1]\n",
    "    tn = inp[0][0]\n",
    "    fp = inp[0][1] \n",
    "    fn = inp[1][0]\n",
    "    \n",
    "    acc = (tp+tn)*1.0 / ( tp+tn+fp+fn)*1.0\n",
    "    recall = tp*1.0/ ( tp+fn ) *1.0\n",
    "    prec = tp*1.0 / ( tp+fp )*1.0\n",
    "    \n",
    "#     F= 2.0*( prec* recall )/ (prec+recall)\n",
    "    F= 2.0*( tp)/ (2*tp + fp + fn)\n",
    "    \n",
    "    return acc, recall, prec, F\n",
    "\n",
    "def acc_match( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    " \n",
    "    return (len( [ x  for x  in  [np.sum(np.abs( true[i]- pred[i] )) for i in range(len(true))] if x  == 0]))*1.0 / len(true)\n",
    "\n",
    "\n",
    "# def acc_none_zero ( true, pred ):\n",
    "    \n",
    "\n",
    "def acc_match_wierd( true, pred ):\n",
    "    \"\"\"\n",
    "    returns exact mathc accuracy\n",
    "    \"\"\"\n",
    "    level = 6 \n",
    "    switch = 11\n",
    "    threeAxis=13\n",
    "    accel = 0 \n",
    "    status=10\n",
    "    contact=5\n",
    "    \n",
    "    counter  = 0 \n",
    "    for i in range( len (true) ):\n",
    "        if np.sum(np.abs( true[i]- pred[i] ))==0 :\n",
    "            counter+=1\n",
    "        else : \n",
    "            t_rec = np.array(list( pred[i]))\n",
    "            \n",
    "            if true[i][level]==1 and true[i][switch]==1 and t_rec[level]==1 :\n",
    "                t_rec[switch]=1\n",
    "            \n",
    "            if true[i][threeAxis]==1 and true[i][accel]==1 and t_rec[threeAxis]==1:\n",
    "                t_rec[accel] =1\n",
    "            \n",
    "            if true[i][status]==1 and true[i][contact]==1 and t_rec[status]==1:\n",
    "                t_rec[contact]=1\n",
    "#             print(t_rec , true[i])    \n",
    "            if np.sum(np.abs( true[i]- t_rec ))==0 :\n",
    "                counter+=1   \n",
    "            \n",
    "             \n",
    "            \n",
    "    \n",
    "    return counter*1.0 / len(true)\n",
    "\n",
    "\n",
    "def print_info(y_test, pred , classes , confidance=0.5 ):\n",
    "    \n",
    "    counts = np.sum( y_test.astype(int) , axis=0)\n",
    "    \n",
    "    pred[pred>=confidance] = 1\n",
    "    pred[pred<confidance] = 0\n",
    "    \n",
    "#     acc_wierd  =acc_match_wierd(y_test, pred)\n",
    "    \n",
    "    conf= multilabel_confusion_matrix( y_test , pred.astype(int), labels= range(len(classes)))\n",
    "    accs = [make_recall_shit(x) for x in conf]\n",
    "    print( \"%30s  %8s   %8s  %8s  %8s %8s %16s\"  %( \"Class\",\"Accuracy\", \"Recall\",\"Precision\",\"F Score\" , \"Count\", \"TP/TN/FP/FN\"))\n",
    "    print( \"------------------------------------------------------------------------\" )\n",
    "    \n",
    "    for index in range(len(classes)):\n",
    "        tp = conf[index][1][1]\n",
    "        tn = conf[index][0][0]\n",
    "        fp = conf[index][0][1] \n",
    "        fn = conf[index][1][0]\n",
    "        print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %d/%d/%d/%d\"  %\n",
    "             (classes[index],\n",
    "              accs[index][0],\n",
    "              accs[index][1],\n",
    "              accs[index][2],\n",
    "              accs[index][3],\n",
    "              counts[index],\n",
    "                  tp ,\n",
    "                tn ,\n",
    "                fp ,\n",
    "                fn ))\n",
    "    n_zeros_true = len([ x  for x  in  [np.sum(np.abs( y_test[i] )) for i in range(len(y_test))] if x  == 0]  )\n",
    "    n_zeros_pred = len([ x  for x  in  [np.sum(np.abs( pred[i] )) for i in range(len(pred))] if x  == 0]  )\n",
    "    \n",
    "    accs = np.nan_to_num(accs)\n",
    "    \n",
    "    print (\"------------------------------------------------------------------------\")\n",
    "    print( \"%30s  %8.3f   %8.3f  %8.3f  %8.3f  %8d %d/%d/%d/%d\"  %\n",
    "             (\"AVERAGES\",\n",
    "              np.average( accs, axis=0)[0],\n",
    "              np.average( accs, axis=0)[1],\n",
    "              np.average( accs, axis=0)[2],\n",
    "              np.average( accs, axis=0)[3],\n",
    "              len(y_test),\n",
    "                  0 ,\n",
    "                0,\n",
    "                0 ,\n",
    "                0 ))\n",
    "    \n",
    "    print ( \"Exact Match ACC : %.5f \" % acc_match( y_test, pred )  )\n",
    "#     print ( \"Wierd Exact Match ACC : %.5f\" % acc_wierd)\n",
    "    print ( \"Total Records : %d \" % len(y_test)  )\n",
    "    print ( \"Total ZXeros in True : %d (%.3f)%%\" % (n_zeros_true ,  n_zeros_true * 1.0/ len(y_test)  ))\n",
    "    print ( \"Total ZXeros in Test : %d (%.3f)%%\" % (n_zeros_pred ,  n_zeros_pred * 1.0/ len(y_test)  ) )\n",
    "    print ('=============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_readable_results ( inp , classes , conffidance=True):\n",
    "    ret = [] \n",
    "    inp =inp.astype(int)\n",
    "    for xx in range(len(inp)) :\n",
    "        u = inp[xx]\n",
    "        temp = []\n",
    "        for j in range(len(u)) : \n",
    "            if u[j] >0:\n",
    "                temp.append(classes[j])\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def makeReadable( model , data, gt, path , classes, x, confidance=0.7):\n",
    "    pred_temp = model.predict(data)\n",
    "    print_info(gt, pred_temp, classes , confidance=confidance)\n",
    "    print( len(classes ), len( pred_temp[0] ) )\n",
    "    xcc= make_readable_results(pred_temp , classes)\n",
    "    y_gt = make_readable_results( gt, classes )\n",
    "    temp_dic = {} \n",
    "    for pick in range(len(xcc)): \n",
    "        temp_dic[ pick +1 ] =  { 'seq': str(data[pick]),\n",
    "                               'pred': xcc[pick],\n",
    "                                'true':y_gt[pick]\n",
    "                               }   \n",
    "\n",
    "    with open(path , 'w') as f:\n",
    "        json.dump(temp_dic , f, indent=4)\n",
    "\n",
    "\n",
    "# def makeReadable( model , data, gt, path , classes, confidance=0.7, x):\n",
    "#     pred_temp = model.predict( data)\n",
    "#     print_info(gt, pred_temp, classes , confidance=confidance)\n",
    "#     xcc= make_readable_results( pred_temp , classes )\n",
    "#     temp_dic = {} \n",
    "#     for pick in range(len(xcc)): \n",
    "#         temp_dic[ pick +1 ] = xcc[pick]  \n",
    "\n",
    "#     with open(path , 'w') as f:\n",
    "#         json.dump(temp_dic , f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcualte per class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest baseline calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size= 50\n",
    "x_random_forest_train,y_random_forest_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "# x_random_forest_test,y_random_forest_test, _ = pre_process_raw( x_test[0], y_test[0] , dim_size, zero_pad=True, normalize=False, classes=classes)\n",
    "rf_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=True, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "# x,y, classes = pre_process_raw( x_data, y_data , dim_size, zero_pad=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=960, max_depth=9050,\n",
    "                             random_state=0 )\n",
    "t_hist = clf.fit(x_random_forest_train, y_random_forest_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(clf.feature_importances_)\n",
    "\n",
    "# print(clf.predict([[0, 0, 0, 0]]))\n",
    "# from sklearn import metrics\n",
    "# scores = cross_val_score(clf, x_random_forest_train, y_random_forest_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(rf_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[ i] )\n",
    "    rf_pred= clf.predict( rf_tests[i][0])\n",
    "    print_info( rf_tests[i][1], rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makeReadable( data=rf_tests[0][0], model=clf, classes=classes, confidance=0.7,gt=rf_tests[0][1], path='sk_home_out.json' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(y_random_forest_test, rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(y_random_forest_test, rf_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ( \"mean : %f \\nstd: %f\\nmax:%f\" %( scores.mean(), scores.std(), scores.max()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnts  = np.unique(np.array([ len(x) for x  in x_train ]), return_counts=True)\n",
    "# # np.sort( cnts[1] )\n",
    "# cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dim_size =15\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=False, normalize=False,classes=classes)\n",
    "_, y_s_lstm_processed_train ,_ =  pre_process_raw( x_train, y_train_service , dim_size, zero_pad=False, normalize=False,classes=service_classes)\n",
    "# x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test_2 , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "lstm_tests  = [ pre_process_raw( x_test[i], y_test[i] , dim_size, zero_pad=False, normalize=False, classes=classes) for i in range(len(x_test)) ] \n",
    "lstm_tests_services  = [ pre_process_raw( x_test[i], y_test_service[i] , dim_size, zero_pad=False, normalize=False, classes=service_classes) for i in range(len(x_test)) ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32069 32069\n",
      "19968 19968\n",
      "9109 9109\n",
      "6404 6404\n"
     ]
    }
   ],
   "source": [
    "for i in range( len(lstm_tests) ):\n",
    "    print( len( lstm_tests[i][0] ), len( lstm_tests_services[i][0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lstm_prossed_test2 = np.expand_dims(x_lstm_prossed_test,axis=1)\n",
    "# x_lstm_prossed_train2 =np.expand_dims(x_lstm_prossed_train,axis=1)\n",
    "\n",
    "for tt  in range( len(lstm_tests) ):\n",
    "    lstm_tests[tt]= (lstm_tests[tt][0].reshape(len(lstm_tests[tt][0]),dim_size,1) , lstm_tests[tt][1],lstm_tests_services[tt][1] )\n",
    "# x_lstm_prossed_test2 = x_lstm_prossed_test.reshape(len(x_lstm_prossed_test),dim_size,1)\n",
    "x_lstm_prossed_train2 =x_lstm_prossed_train.reshape(len(x_lstm_prossed_train),dim_size,1)\n",
    "\n",
    "# y_lstm_prossed_test2 = y_lstm_prossed_test.reshape(len(y_lstm_prossed_test),len(classes),1)\n",
    "# y_lstm_prossed_train2 =y_lstm_prossed_train.reshape(len(y_lstm_prossed_train),len(classes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60562.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_s_lstm_processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 15, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 15, 128)      512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 128)      512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 15, 128)      512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 15, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 128)      512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 15, 128)      49280       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 128)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 15, 128)      49280       conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 15, 128)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 15, 128)      49280       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 15, 100)      91600       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 15, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 15, 100)      40800       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 15, 128)      12928       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 15, 128)      49280       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 15, 128)      12928       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 15, 128)      16512       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 15, 128)      49280       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 15, 128)      16512       dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 15, 128)      16512       dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 128)      512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 15, 128)      16512       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 15, 128)      0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 128)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 15, 128)      0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1920)         0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 15, 128)      0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1920)         0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          245888      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 15, 128)      49280       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          245888      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1920)         0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mergerguy (Concatenate)         (None, 2176)         0           dropout_11[0][0]                 \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          278656      mergerguy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          16512       dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          16512       dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service1 (Dense)             (None, 130)          16770       dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service2 (Dense)             (None, 130)          17030       to_service1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "service_output (Dense)          (None, 16)           2096        to_service2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,362,408\n",
      "Trainable params: 1,361,384\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57867/57867 [==============================] - 7s 125us/step - loss: 45.3149 - f1_perRow: 0.0991 - f1_perClass: 0.2685 - acc: 0.3673\n",
      "Epoch 2/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 24.9812 - f1_perRow: 0.1479 - f1_perClass: 0.5986 - acc: 0.4668\n",
      "Epoch 3/100\n",
      "57867/57867 [==============================] - 2s 35us/step - loss: 22.5274 - f1_perRow: 0.1954 - f1_perClass: 0.6255 - acc: 0.4668\n",
      "Epoch 4/100\n",
      "57867/57867 [==============================] - 2s 35us/step - loss: 21.4959 - f1_perRow: 0.2246 - f1_perClass: 0.6300 - acc: 0.4668\n",
      "Epoch 5/100\n",
      "57867/57867 [==============================] - 2s 35us/step - loss: 20.6836 - f1_perRow: 0.2561 - f1_perClass: 0.6411 - acc: 0.4668\n",
      "Epoch 6/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 20.1946 - f1_perRow: 0.2781 - f1_perClass: 0.6454 - acc: 0.4668\n",
      "Epoch 7/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 19.8484 - f1_perRow: 0.2945 - f1_perClass: 0.6506 - acc: 0.4668\n",
      "Epoch 8/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 19.4163 - f1_perRow: 0.3085 - f1_perClass: 0.6552 - acc: 0.4668\n",
      "Epoch 9/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 18.8491 - f1_perRow: 0.3239 - f1_perClass: 0.6707 - acc: 0.4668\n",
      "Epoch 10/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 18.4979 - f1_perRow: 0.3354 - f1_perClass: 0.6784 - acc: 0.4668\n",
      "Epoch 11/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 18.3655 - f1_perRow: 0.3396 - f1_perClass: 0.6798 - acc: 0.4668\n",
      "Epoch 12/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 17.9842 - f1_perRow: 0.3474 - f1_perClass: 0.6847 - acc: 0.4668\n",
      "Epoch 13/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 17.7274 - f1_perRow: 0.3536 - f1_perClass: 0.6863 - acc: 0.4668\n",
      "Epoch 14/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 17.4772 - f1_perRow: 0.3571 - f1_perClass: 0.6899 - acc: 0.4668\n",
      "Epoch 15/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 17.2880 - f1_perRow: 0.3606 - f1_perClass: 0.6943 - acc: 0.4668\n",
      "Epoch 16/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.9719 - f1_perRow: 0.3665 - f1_perClass: 0.6996 - acc: 0.4667\n",
      "Epoch 17/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.7791 - f1_perRow: 0.3720 - f1_perClass: 0.7025 - acc: 0.4667\n",
      "Epoch 18/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.7080 - f1_perRow: 0.3760 - f1_perClass: 0.7053 - acc: 0.4668\n",
      "Epoch 19/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.7198 - f1_perRow: 0.3798 - f1_perClass: 0.7075 - acc: 0.4668\n",
      "Epoch 20/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.5287 - f1_perRow: 0.3796 - f1_perClass: 0.7068 - acc: 0.4667\n",
      "Epoch 21/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.4115 - f1_perRow: 0.3840 - f1_perClass: 0.7081 - acc: 0.4668\n",
      "Epoch 22/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.3149 - f1_perRow: 0.3863 - f1_perClass: 0.7090 - acc: 0.4667\n",
      "Epoch 23/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.5249 - f1_perRow: 0.3931 - f1_perClass: 0.7093 - acc: 0.4667\n",
      "Epoch 24/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.4379 - f1_perRow: 0.3871 - f1_perClass: 0.7090 - acc: 0.4667\n",
      "Epoch 25/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.5476 - f1_perRow: 0.3881 - f1_perClass: 0.7054 - acc: 0.4667\n",
      "Epoch 26/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.3854 - f1_perRow: 0.3865 - f1_perClass: 0.7088 - acc: 0.4667\n",
      "Epoch 27/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.2913 - f1_perRow: 0.3957 - f1_perClass: 0.7115 - acc: 0.4667\n",
      "Epoch 28/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.1644 - f1_perRow: 0.3985 - f1_perClass: 0.7117 - acc: 0.4668\n",
      "Epoch 29/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.2965 - f1_perRow: 0.3989 - f1_perClass: 0.7084 - acc: 0.4668\n",
      "Epoch 30/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.1720 - f1_perRow: 0.3984 - f1_perClass: 0.7086 - acc: 0.4668\n",
      "Epoch 31/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.4999 - f1_perRow: 0.4061 - f1_perClass: 0.7132 - acc: 0.4668\n",
      "Epoch 32/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.2540 - f1_perRow: 0.3939 - f1_perClass: 0.7106 - acc: 0.4668\n",
      "Epoch 33/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.1327 - f1_perRow: 0.4054 - f1_perClass: 0.7131 - acc: 0.4668\n",
      "Epoch 34/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.9745 - f1_perRow: 0.4103 - f1_perClass: 0.7136 - acc: 0.4667\n",
      "Epoch 35/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0627 - f1_perRow: 0.4077 - f1_perClass: 0.7137 - acc: 0.4667\n",
      "Epoch 36/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.9334 - f1_perRow: 0.4100 - f1_perClass: 0.7137 - acc: 0.4668\n",
      "Epoch 37/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.9765 - f1_perRow: 0.4124 - f1_perClass: 0.7153 - acc: 0.4668\n",
      "Epoch 38/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0211 - f1_perRow: 0.4153 - f1_perClass: 0.7133 - acc: 0.4668\n",
      "Epoch 39/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.1458 - f1_perRow: 0.4072 - f1_perClass: 0.7135 - acc: 0.4668\n",
      "Epoch 40/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0642 - f1_perRow: 0.4169 - f1_perClass: 0.7137 - acc: 0.4668\n",
      "Epoch 41/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0241 - f1_perRow: 0.4109 - f1_perClass: 0.7142 - acc: 0.4668\n",
      "Epoch 42/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.9099 - f1_perRow: 0.4158 - f1_perClass: 0.7134 - acc: 0.4668\n",
      "Epoch 43/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0884 - f1_perRow: 0.4198 - f1_perClass: 0.7179 - acc: 0.4668\n",
      "Epoch 44/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.1955 - f1_perRow: 0.4056 - f1_perClass: 0.7078 - acc: 0.4671\n",
      "Epoch 45/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0711 - f1_perRow: 0.4119 - f1_perClass: 0.7155 - acc: 0.4668\n",
      "Epoch 46/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.8976 - f1_perRow: 0.4185 - f1_perClass: 0.7166 - acc: 0.4668\n",
      "Epoch 47/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 16.0068 - f1_perRow: 0.4242 - f1_perClass: 0.7141 - acc: 0.4668\n",
      "Epoch 48/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.8148 - f1_perRow: 0.4180 - f1_perClass: 0.7154 - acc: 0.4668\n",
      "Epoch 49/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.7654 - f1_perRow: 0.4271 - f1_perClass: 0.7199 - acc: 0.4668\n",
      "Epoch 50/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.7571 - f1_perRow: 0.4288 - f1_perClass: 0.7161 - acc: 0.4668\n",
      "Epoch 51/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.6599 - f1_perRow: 0.4274 - f1_perClass: 0.7162 - acc: 0.4668\n",
      "Epoch 52/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 15.8840 - f1_perRow: 0.4308 - f1_perClass: 0.7173 - acc: 0.4668\n",
      "Epoch 53/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 16.1727 - f1_perRow: 0.4157 - f1_perClass: 0.7108 - acc: 0.4668\n",
      "Epoch 54/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.9952 - f1_perRow: 0.4148 - f1_perClass: 0.7135 - acc: 0.4669\n",
      "Epoch 55/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 15.7713 - f1_perRow: 0.4299 - f1_perClass: 0.7189 - acc: 0.4669\n",
      "Epoch 56/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.7616 - f1_perRow: 0.4388 - f1_perClass: 0.7178 - acc: 0.4668\n",
      "Epoch 57/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.8497 - f1_perRow: 0.4290 - f1_perClass: 0.7149 - acc: 0.4669\n",
      "Epoch 58/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.9007 - f1_perRow: 0.4251 - f1_perClass: 0.7185 - acc: 0.4668\n",
      "Epoch 59/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.7962 - f1_perRow: 0.4237 - f1_perClass: 0.7136 - acc: 0.4670\n",
      "Epoch 60/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.5395 - f1_perRow: 0.4400 - f1_perClass: 0.7202 - acc: 0.4668\n",
      "Epoch 61/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 15.6828 - f1_perRow: 0.4380 - f1_perClass: 0.7163 - acc: 0.4668\n",
      "Epoch 62/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.8767 - f1_perRow: 0.4389 - f1_perClass: 0.7177 - acc: 0.4675\n",
      "Epoch 63/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.5859 - f1_perRow: 0.4358 - f1_perClass: 0.7200 - acc: 0.4668\n",
      "Epoch 64/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.9982 - f1_perRow: 0.4366 - f1_perClass: 0.7127 - acc: 0.4670\n",
      "Epoch 65/100\n",
      "57867/57867 [==============================] - 2s 36us/step - loss: 15.7518 - f1_perRow: 0.4320 - f1_perClass: 0.7172 - acc: 0.4669\n",
      "Epoch 66/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.8006 - f1_perRow: 0.4314 - f1_perClass: 0.7189 - acc: 0.4668\n",
      "Epoch 67/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.9371 - f1_perRow: 0.4292 - f1_perClass: 0.7103 - acc: 0.4945\n",
      "Epoch 68/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.7118 - f1_perRow: 0.4305 - f1_perClass: 0.7181 - acc: 0.4672\n",
      "Epoch 69/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.6180 - f1_perRow: 0.4451 - f1_perClass: 0.7196 - acc: 0.4668\n",
      "Epoch 70/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 16.1346 - f1_perRow: 0.4392 - f1_perClass: 0.7129 - acc: 0.4670\n",
      "Epoch 71/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 16.6918 - f1_perRow: 0.4181 - f1_perClass: 0.7065 - acc: 0.4668\n",
      "Epoch 72/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 16.1496 - f1_perRow: 0.4025 - f1_perClass: 0.7138 - acc: 0.4675\n",
      "Epoch 73/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.8798 - f1_perRow: 0.4226 - f1_perClass: 0.7172 - acc: 0.4668\n",
      "Epoch 74/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 15.7938 - f1_perRow: 0.4365 - f1_perClass: 0.7164 - acc: 0.4670\n",
      "Epoch 75/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 15.7937 - f1_perRow: 0.4299 - f1_perClass: 0.7164 - acc: 0.4668\n",
      "Epoch 76/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.8686 - f1_perRow: 0.4312 - f1_perClass: 0.7160 - acc: 0.4668\n",
      "Epoch 77/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.7412 - f1_perRow: 0.4345 - f1_perClass: 0.7166 - acc: 0.4668\n",
      "Epoch 78/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.5408 - f1_perRow: 0.4363 - f1_perClass: 0.7226 - acc: 0.4668\n",
      "Epoch 79/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.6350 - f1_perRow: 0.4471 - f1_perClass: 0.7154 - acc: 0.4668\n",
      "Epoch 80/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 15.6516 - f1_perRow: 0.4522 - f1_perClass: 0.7211 - acc: 0.4669\n",
      "Epoch 81/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 15.5889 - f1_perRow: 0.4330 - f1_perClass: 0.7167 - acc: 0.4678\n",
      "Epoch 82/100\n",
      "57867/57867 [==============================] - 2s 39us/step - loss: 15.6172 - f1_perRow: 0.4507 - f1_perClass: 0.7201 - acc: 0.4668\n",
      "Epoch 83/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.4726 - f1_perRow: 0.4545 - f1_perClass: 0.7196 - acc: 0.4676\n",
      "Epoch 84/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.5481 - f1_perRow: 0.4463 - f1_perClass: 0.7167 - acc: 0.4703\n",
      "Epoch 85/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.5508 - f1_perRow: 0.4578 - f1_perClass: 0.7223 - acc: 0.4668\n",
      "Epoch 86/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 16.2263 - f1_perRow: 0.4360 - f1_perClass: 0.7120 - acc: 0.4668\n",
      "Epoch 87/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.7736 - f1_perRow: 0.4327 - f1_perClass: 0.7154 - acc: 0.4680\n",
      "Epoch 88/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.5438 - f1_perRow: 0.4434 - f1_perClass: 0.7218 - acc: 0.4691\n",
      "Epoch 89/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.5094 - f1_perRow: 0.4522 - f1_perClass: 0.7185 - acc: 0.4679\n",
      "Epoch 90/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.4129 - f1_perRow: 0.4557 - f1_perClass: 0.7206 - acc: 0.4672\n",
      "Epoch 91/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.6057 - f1_perRow: 0.4501 - f1_perClass: 0.7209 - acc: 0.4679\n",
      "Epoch 92/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.3598 - f1_perRow: 0.4540 - f1_perClass: 0.7190 - acc: 0.4678\n",
      "Epoch 93/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.3350 - f1_perRow: 0.4526 - f1_perClass: 0.7216 - acc: 0.4683\n",
      "Epoch 94/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 15.9671 - f1_perRow: 0.4510 - f1_perClass: 0.7172 - acc: 0.4670\n",
      "Epoch 95/100\n",
      "57867/57867 [==============================] - 2s 40us/step - loss: 15.6178 - f1_perRow: 0.4401 - f1_perClass: 0.7164 - acc: 0.4697\n",
      "Epoch 96/100\n",
      "57867/57867 [==============================] - 2s 41us/step - loss: 15.4884 - f1_perRow: 0.4458 - f1_perClass: 0.7208 - acc: 0.4693\n",
      "Epoch 97/100\n",
      "57867/57867 [==============================] - 2s 38us/step - loss: 15.5455 - f1_perRow: 0.4515 - f1_perClass: 0.7207 - acc: 0.4684\n",
      "Epoch 98/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.6654 - f1_perRow: 0.4583 - f1_perClass: 0.7179 - acc: 0.4685\n",
      "Epoch 99/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.3463 - f1_perRow: 0.4519 - f1_perClass: 0.7204 - acc: 0.4676\n",
      "Epoch 100/100\n",
      "57867/57867 [==============================] - 2s 37us/step - loss: 15.3397 - f1_perRow: 0.4577 - f1_perClass: 0.7212 - acc: 0.4676\n"
     ]
    }
   ],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(out)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [dout_2, fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    " \n",
    "weights = [\n",
    "1.0/(57.0 / len(y_train)),\n",
    "1.0/(19.0 / len(y_train)),\n",
    "1.0/(7.0 / len(y_train)),\n",
    "1.0/(14.0 / len(y_train)),\n",
    "1.0/(6.0 / len(y_train)),\n",
    "1.0/(176.0 / len(y_train)),\n",
    "1.0/(27.0 / len(y_train)),\n",
    "1.0/(35.0 / len(y_train)),\n",
    "1.0/(371.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "1.0/(4842.0 / len(y_train)),\n",
    "1.0/(119.0 / len(y_train)),\n",
    "1.0/(21.0 / len(y_train)),\n",
    "1.0/(1168.0 / len(y_train)),\n",
    "1.0/(63.0 / len(y_train)),\n",
    "1.0/(13305.0 / len(y_train)),\n",
    "1.0/(11111.0 / len(y_train)),\n",
    "]\n",
    "    \n",
    "\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lstm_prossed_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3393c128e8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# model2.add(Dense(len(classes), activation='sigmoid'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweighted_categorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_2 = LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_1 =  Dense(128, activation='relu')(bi_d_1)\n",
    "lstm_2 = Dense(128, activation='relu')(lstm_1)\n",
    "\n",
    "\n",
    "\n",
    "# td_1    = TimeDistributed(Dense(256, activation='relu'))(lstm_2)\n",
    "# dout_1  = Dropout(0.1)(td_1)\n",
    "dout_1  = Dropout(0.1)(lstm_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_2  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dout_2)\n",
    "\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=1000, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "\n",
    "dens_out_3 = Dense( 128, activation='relu' )(fl_out_cnn)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 15, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 15, 128)      512         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 15, 128)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 15, 128)      0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 15, 128)      49280       dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 15, 128)      49280       conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 15, 128)      512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 15, 128)      0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 15, 128)      0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 15, 100)      40800       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 15, 128)      49280       dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 15, 128)      12928       lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 15, 128)      49280       conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 15, 128)      16512       dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 15, 128)      512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 15, 128)      16512       dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 15, 128)      0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 15, 128)      0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 15, 128)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1920)         0           dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 15, 128)      49280       dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 128)          245888      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 1920)         0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 128)          0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mergerguy (Concatenate)         (None, 2048)         0           flatten_10[0][0]                 \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          262272      mergerguy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 128)          16512       dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 128)          16512       dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service1 (Dense)             (None, 130)          16770       dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "to_service2 (Dense)             (None, 130)          17030       to_service1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "service_output (Dense)          (None, 16)           2096        to_service2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 912,280\n",
      "Trainable params: 911,512\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "57867/57867 [==============================] - 6s 100us/step - loss: 46.8265 - f1_perRow: 0.0982 - f1_perClass: 0.2258 - acc: 0.4425\n",
      "Epoch 2/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 28.4893 - f1_perRow: 0.1412 - f1_perClass: 0.5087 - acc: 0.5061\n",
      "Epoch 3/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 26.1877 - f1_perRow: 0.1950 - f1_perClass: 0.5362 - acc: 0.4661\n",
      "Epoch 4/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 22.5012 - f1_perRow: 0.2438 - f1_perClass: 0.6218 - acc: 0.4668\n",
      "Epoch 5/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 21.2603 - f1_perRow: 0.2583 - f1_perClass: 0.6308 - acc: 0.4640\n",
      "Epoch 6/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 20.6388 - f1_perRow: 0.2777 - f1_perClass: 0.6441 - acc: 0.4616\n",
      "Epoch 7/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 20.0653 - f1_perRow: 0.3008 - f1_perClass: 0.6531 - acc: 0.4629\n",
      "Epoch 8/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 19.6142 - f1_perRow: 0.3145 - f1_perClass: 0.6517 - acc: 0.4668\n",
      "Epoch 9/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 19.3665 - f1_perRow: 0.3244 - f1_perClass: 0.6560 - acc: 0.4668\n",
      "Epoch 10/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 19.1740 - f1_perRow: 0.3401 - f1_perClass: 0.6603 - acc: 0.4668\n",
      "Epoch 11/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 18.9573 - f1_perRow: 0.3383 - f1_perClass: 0.6627 - acc: 0.4668\n",
      "Epoch 12/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 18.7398 - f1_perRow: 0.3408 - f1_perClass: 0.6638 - acc: 0.4668\n",
      "Epoch 13/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 18.6073 - f1_perRow: 0.3475 - f1_perClass: 0.6669 - acc: 0.4668\n",
      "Epoch 14/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 18.4585 - f1_perRow: 0.3471 - f1_perClass: 0.6677 - acc: 0.4668\n",
      "Epoch 15/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 18.0239 - f1_perRow: 0.3576 - f1_perClass: 0.6806 - acc: 0.4668\n",
      "Epoch 16/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 17.7495 - f1_perRow: 0.3611 - f1_perClass: 0.6869 - acc: 0.4668\n",
      "Epoch 17/100\n",
      "57867/57867 [==============================] - 1s 22us/step - loss: 17.4290 - f1_perRow: 0.3654 - f1_perClass: 0.6886 - acc: 0.4668\n",
      "Epoch 18/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 17.2280 - f1_perRow: 0.3676 - f1_perClass: 0.6930 - acc: 0.4668\n",
      "Epoch 19/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.9887 - f1_perRow: 0.3725 - f1_perClass: 0.6993 - acc: 0.4668\n",
      "Epoch 20/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.8374 - f1_perRow: 0.3777 - f1_perClass: 0.7060 - acc: 0.4668\n",
      "Epoch 21/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.6944 - f1_perRow: 0.3802 - f1_perClass: 0.7030 - acc: 0.4668\n",
      "Epoch 22/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.5660 - f1_perRow: 0.3786 - f1_perClass: 0.7040 - acc: 0.4668\n",
      "Epoch 23/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.4528 - f1_perRow: 0.3927 - f1_perClass: 0.7070 - acc: 0.4668\n",
      "Epoch 24/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.5617 - f1_perRow: 0.3876 - f1_perClass: 0.7091 - acc: 0.4668\n",
      "Epoch 25/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.4607 - f1_perRow: 0.3894 - f1_perClass: 0.7095 - acc: 0.4668\n",
      "Epoch 26/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.4821 - f1_perRow: 0.3935 - f1_perClass: 0.7077 - acc: 0.4668\n",
      "Epoch 27/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.4098 - f1_perRow: 0.3880 - f1_perClass: 0.7089 - acc: 0.4668\n",
      "Epoch 28/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.3186 - f1_perRow: 0.3989 - f1_perClass: 0.7116 - acc: 0.4668\n",
      "Epoch 29/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2183 - f1_perRow: 0.3973 - f1_perClass: 0.7115 - acc: 0.4668\n",
      "Epoch 30/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2005 - f1_perRow: 0.3985 - f1_perClass: 0.7101 - acc: 0.4668\n",
      "Epoch 31/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2359 - f1_perRow: 0.3982 - f1_perClass: 0.7100 - acc: 0.4668\n",
      "Epoch 32/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2341 - f1_perRow: 0.4031 - f1_perClass: 0.7139 - acc: 0.4668\n",
      "Epoch 33/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2454 - f1_perRow: 0.4020 - f1_perClass: 0.7127 - acc: 0.4668\n",
      "Epoch 34/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2509 - f1_perRow: 0.4009 - f1_perClass: 0.7097 - acc: 0.4668\n",
      "Epoch 35/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.2022 - f1_perRow: 0.4016 - f1_perClass: 0.7117 - acc: 0.4668\n",
      "Epoch 36/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.1832 - f1_perRow: 0.4083 - f1_perClass: 0.7147 - acc: 0.4668\n",
      "Epoch 37/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.1445 - f1_perRow: 0.4071 - f1_perClass: 0.7115 - acc: 0.4668\n",
      "Epoch 38/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.0797 - f1_perRow: 0.3998 - f1_perClass: 0.7115 - acc: 0.4676\n",
      "Epoch 39/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.0477 - f1_perRow: 0.4106 - f1_perClass: 0.7122 - acc: 0.4668\n",
      "Epoch 40/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.1676 - f1_perRow: 0.4089 - f1_perClass: 0.7140 - acc: 0.4668\n",
      "Epoch 41/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.1037 - f1_perRow: 0.4057 - f1_perClass: 0.7139 - acc: 0.4668\n",
      "Epoch 42/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9223 - f1_perRow: 0.4130 - f1_perClass: 0.7140 - acc: 0.4668\n",
      "Epoch 43/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.8813 - f1_perRow: 0.4182 - f1_perClass: 0.7153 - acc: 0.4668\n",
      "Epoch 44/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9975 - f1_perRow: 0.4180 - f1_perClass: 0.7143 - acc: 0.4668\n",
      "Epoch 45/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.1982 - f1_perRow: 0.4128 - f1_perClass: 0.7143 - acc: 0.4668\n",
      "Epoch 46/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.1792 - f1_perRow: 0.4058 - f1_perClass: 0.7115 - acc: 0.4668\n",
      "Epoch 47/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.0158 - f1_perRow: 0.4136 - f1_perClass: 0.7138 - acc: 0.4671\n",
      "Epoch 48/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9984 - f1_perRow: 0.4163 - f1_perClass: 0.7156 - acc: 0.4669\n",
      "Epoch 49/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9822 - f1_perRow: 0.4135 - f1_perClass: 0.7115 - acc: 0.4677\n",
      "Epoch 50/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.8361 - f1_perRow: 0.4227 - f1_perClass: 0.7159 - acc: 0.4672\n",
      "Epoch 51/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.7734 - f1_perRow: 0.4279 - f1_perClass: 0.7190 - acc: 0.4669\n",
      "Epoch 52/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6905 - f1_perRow: 0.4298 - f1_perClass: 0.7155 - acc: 0.4668\n",
      "Epoch 53/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 16.3404 - f1_perRow: 0.4227 - f1_perClass: 0.7108 - acc: 0.4669\n",
      "Epoch 54/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9493 - f1_perRow: 0.4160 - f1_perClass: 0.7166 - acc: 0.4668\n",
      "Epoch 55/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9098 - f1_perRow: 0.4176 - f1_perClass: 0.7157 - acc: 0.4668\n",
      "Epoch 56/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9280 - f1_perRow: 0.4243 - f1_perClass: 0.7134 - acc: 0.4669\n",
      "Epoch 57/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9515 - f1_perRow: 0.4234 - f1_perClass: 0.7158 - acc: 0.4673\n",
      "Epoch 58/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.8950 - f1_perRow: 0.4204 - f1_perClass: 0.7160 - acc: 0.4709\n",
      "Epoch 59/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.8827 - f1_perRow: 0.4262 - f1_perClass: 0.7137 - acc: 0.4671\n",
      "Epoch 60/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6620 - f1_perRow: 0.4292 - f1_perClass: 0.7197 - acc: 0.4683\n",
      "Epoch 61/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.8048 - f1_perRow: 0.4319 - f1_perClass: 0.7142 - acc: 0.4715\n",
      "Epoch 62/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.9465 - f1_perRow: 0.4346 - f1_perClass: 0.7177 - acc: 0.4670\n",
      "Epoch 63/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.7607 - f1_perRow: 0.4270 - f1_perClass: 0.7185 - acc: 0.4680\n",
      "Epoch 64/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6419 - f1_perRow: 0.4339 - f1_perClass: 0.7138 - acc: 0.4671\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.8029 - f1_perRow: 0.4410 - f1_perClass: 0.7181 - acc: 0.4714\n",
      "Epoch 66/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6709 - f1_perRow: 0.4387 - f1_perClass: 0.7192 - acc: 0.4670\n",
      "Epoch 67/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6630 - f1_perRow: 0.4429 - f1_perClass: 0.7181 - acc: 0.4671\n",
      "Epoch 68/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.7833 - f1_perRow: 0.4375 - f1_perClass: 0.7170 - acc: 0.4672\n",
      "Epoch 69/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.7421 - f1_perRow: 0.4370 - f1_perClass: 0.7150 - acc: 0.4684\n",
      "Epoch 70/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6950 - f1_perRow: 0.4403 - f1_perClass: 0.7198 - acc: 0.4682\n",
      "Epoch 71/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5313 - f1_perRow: 0.4511 - f1_perClass: 0.7185 - acc: 0.4867\n",
      "Epoch 72/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6143 - f1_perRow: 0.4451 - f1_perClass: 0.7175 - acc: 0.4680\n",
      "Epoch 73/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5600 - f1_perRow: 0.4476 - f1_perClass: 0.7182 - acc: 0.6030\n",
      "Epoch 74/100\n",
      "57867/57867 [==============================] - 1s 25us/step - loss: 16.0705 - f1_perRow: 0.4341 - f1_perClass: 0.7123 - acc: 0.4798\n",
      "Epoch 75/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.6273 - f1_perRow: 0.4414 - f1_perClass: 0.7183 - acc: 0.4700\n",
      "Epoch 76/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6301 - f1_perRow: 0.4466 - f1_perClass: 0.7206 - acc: 0.4681\n",
      "Epoch 77/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.5096 - f1_perRow: 0.4500 - f1_perClass: 0.7169 - acc: 0.4878\n",
      "Epoch 78/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5880 - f1_perRow: 0.4513 - f1_perClass: 0.7198 - acc: 0.4748\n",
      "Epoch 79/100\n",
      "57867/57867 [==============================] - 1s 25us/step - loss: 15.4566 - f1_perRow: 0.4508 - f1_perClass: 0.7196 - acc: 0.4931\n",
      "Epoch 80/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6410 - f1_perRow: 0.4467 - f1_perClass: 0.7185 - acc: 0.4729\n",
      "Epoch 81/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5018 - f1_perRow: 0.4482 - f1_perClass: 0.7190 - acc: 0.4773\n",
      "Epoch 82/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.4138 - f1_perRow: 0.4569 - f1_perClass: 0.7195 - acc: 0.4687\n",
      "Epoch 83/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5207 - f1_perRow: 0.4541 - f1_perClass: 0.7213 - acc: 0.4731\n",
      "Epoch 84/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.4485 - f1_perRow: 0.4605 - f1_perClass: 0.7209 - acc: 0.4676\n",
      "Epoch 85/100\n",
      "57867/57867 [==============================] - 1s 25us/step - loss: 15.6227 - f1_perRow: 0.4462 - f1_perClass: 0.7172 - acc: 0.4678\n",
      "Epoch 86/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6381 - f1_perRow: 0.4586 - f1_perClass: 0.7224 - acc: 0.4697\n",
      "Epoch 87/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5446 - f1_perRow: 0.4456 - f1_perClass: 0.7175 - acc: 0.4686\n",
      "Epoch 88/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.3921 - f1_perRow: 0.4551 - f1_perClass: 0.7200 - acc: 0.4673\n",
      "Epoch 89/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6076 - f1_perRow: 0.4588 - f1_perClass: 0.7209 - acc: 0.4671\n",
      "Epoch 90/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.4162 - f1_perRow: 0.4561 - f1_perClass: 0.7187 - acc: 0.4693\n",
      "Epoch 91/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.3312 - f1_perRow: 0.4619 - f1_perClass: 0.7230 - acc: 0.4724\n",
      "Epoch 92/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5848 - f1_perRow: 0.4607 - f1_perClass: 0.7196 - acc: 0.4700\n",
      "Epoch 93/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.5402 - f1_perRow: 0.4573 - f1_perClass: 0.7205 - acc: 0.4671\n",
      "Epoch 94/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.5165 - f1_perRow: 0.4532 - f1_perClass: 0.7206 - acc: 0.4678\n",
      "Epoch 95/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.3909 - f1_perRow: 0.4619 - f1_perClass: 0.7205 - acc: 0.4678\n",
      "Epoch 96/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.6637 - f1_perRow: 0.4575 - f1_perClass: 0.7190 - acc: 0.4673\n",
      "Epoch 97/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.6184 - f1_perRow: 0.4486 - f1_perClass: 0.7172 - acc: 0.4692\n",
      "Epoch 98/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.4438 - f1_perRow: 0.4605 - f1_perClass: 0.7218 - acc: 0.4683\n",
      "Epoch 99/100\n",
      "57867/57867 [==============================] - 1s 24us/step - loss: 15.5121 - f1_perRow: 0.4541 - f1_perClass: 0.7179 - acc: 0.4679\n",
      "Epoch 100/100\n",
      "57867/57867 [==============================] - 1s 23us/step - loss: 15.3897 - f1_perRow: 0.4609 - f1_perClass: 0.7239 - acc: 0.4696\n"
     ]
    }
   ],
   "source": [
    "## Bidirectional LSTM :D \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense,Conv1D,Dropout,Activation,BatchNormalization,MaxPooling1D,Flatten,Masking,TimeDistributed\n",
    "from keras.layers.recurrent import LSTM,GRU,SimpleRNN\n",
    "from keras.models import Input,Sequential,Model\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MSE,MSLE\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1_perRow(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def f1_perClass(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss_perClass(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "def f1_loss_perRow(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1))**2\n",
    "\n",
    "\n",
    "\n",
    "inputs  = Input( ( dim_size,1 ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bi_d_1 = Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat') (inputs)\n",
    "lstm_1 =  LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True)(inputs)\n",
    "# lstm_1 = LSTM(40 ,  recurrent_dropout=0.14, return_sequences=True)(lstm_1)\n",
    "\n",
    "bi_d_raw_1 =Dense(128, activation='relu')  (lstm_1)\n",
    "lstm_raw_1 =  Dense(128, activation='relu')(bi_d_raw_1)\n",
    "lstm_raw_2 = Dense(128, activation='relu')(lstm_raw_1)\n",
    "\n",
    "dout_1  = Dropout(0.1)(lstm_raw_2)\n",
    "flt_1   = Flatten()(dout_1)\n",
    "dense_1 = Dense(128, activation='relu')(flt_1)\n",
    "dout_3  = Dropout(0.2)(dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(inputs)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling2')(out)\n",
    "\n",
    "\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Conv1D(128,3,padding='same')(out)\n",
    "# out = Flatten()(out)\n",
    "# out = MaxPooling1D(2,padding='same', name ='pooling')(out)\n",
    "\n",
    "\n",
    "\n",
    "# fl_out_1 = Flatten()(dout_2)\n",
    "\n",
    "fl_out_cnn = Flatten()(out)\n",
    "\n",
    "# out_new = concatenate( [fl_out_1, fl_out_cnn] , name='mergerguy')\n",
    "out_new = concatenate( [ fl_out_cnn,dout_3] , name='mergerguy')\n",
    "\n",
    "dens_out_1 = Dense( 128, activation='relu' )(out_new)\n",
    "dens_out_2 = Dense( 128, activation='relu' )(dens_out_1)\n",
    "dens_out_3 = Dense( 128, activation='relu' )(dens_out_2)\n",
    "\n",
    "# fl2  = Flatten()(out_new)\n",
    "\n",
    "out_put_final = Dense(len(classes), activation='sigmoid', name='Event_output')(dens_out_3)\n",
    "\n",
    "toService_1 = Dense( 130, name=\"to_service1\" )(dens_out_3)\n",
    "toService_1 = Dense( 130, name=\"to_service2\" )(toService_1)\n",
    "\n",
    "service_output = Dense(len(classes  ), activation=\"sigmoid\", name = 'service_output')(toService_1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[inputs], outputs=[service_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model2.add(Bidirectional( LSTM(200 ,  recurrent_dropout=0.12, return_sequences=True),input_shape=[dim_size,1],merge_mode='concat'))\n",
    "# model2.add( LSTM(60 ,  recurrent_dropout=0.04, return_sequences=True))\n",
    "# model2.add( LSTM(30 ,  recurrent_dropout=0.14, return_sequences=True))\n",
    "# # model2.add(Bidirectional( LSTM(100 ,  recurrent_dropout=0.04, return_sequences=True),merge_mode='concat'))\n",
    "# model2.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(128, activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "# model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perRow ,\n",
    "    \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "    \"service_output\": 20}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses,loss_weights=lossWeights, optimizer='adam', metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "# model2.compile(loss=losses, loss_weights=lossWeights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('IoTDownNet', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model2.summary())\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=100, batch_size=12500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "#     \"service_output\": f1_loss_perClass ,\n",
    "    \"service_output\": f1_loss_perClass ,\n",
    "#     \"service_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {#\"service_output\": 20,\n",
    "               \"service_output\": 30.0 ,\n",
    "#     \"service_output\": 20\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "model2.compile(loss=losses, loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=1e-6  ), metrics=[f1_perRow,f1_perClass,'acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.compile(loss=losses,loss_weights=lossWeights, optimizer=keras.optimizers.Adam(lr=5e-5  ), metrics=[f1_perRow,f1_perClass,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=16500, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=f1_loss, optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=[f1,'acc'])\n",
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=7500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=8e-5  ), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=weighted_categorical_crossentropy(weights=weights), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(x_lstm_prossed_train2, y_lstm_prossed_train, epochs=300, batch_size=3500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM-sigmoid-withRemovedClasses\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4aa1e57ef0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'], c='red')\n",
    "# plt.plot(hist2.history['acc'], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model2, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2=load_model( \"LSTM_withSigmoid_LargeData_F1_E100_B500_MSE_False\"  \n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================HOME Case : home_muhammed_final.json =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/home/omid/.conda/envs/iot/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     1.000      0.333     0.154     0.211         6 2/32052/11/4\n",
      "                      activity     0.999      0.500     0.889     0.640        48 24/32018/3/24\n",
      "                       battery     1.000        nan       nan       nan         0 0/32069/0/0\n",
      "                        button     0.579      0.571     0.000     0.001         7 4/18550/13512/3\n",
      "              colorTemperature     1.000        nan     0.000     0.000         0 0/32065/4/0\n",
      "                       contact     0.998      0.628     0.576     0.601        78 49/31955/36/29\n",
      "                         level     0.999      0.444     0.500     0.471        27 12/32030/12/15\n",
      "                          lock     0.997      0.571     0.070     0.125        14 8/31949/106/6\n",
      "                        motion     0.966      0.001     1.000     0.002      1099 1/30970/0/1098\n",
      "                          ping     0.993      0.997     0.969     0.983      6975 6957/24873/221/18\n",
      "                        status     0.999      0.840     0.568     0.677        50 42/31987/32/8\n",
      "                        switch     1.000      0.826     0.760     0.792        23 19/32040/6/4\n",
      "                   temperature     0.953      0.000       nan     0.000      1512 0/30557/0/1512\n",
      "                     threeAxis     0.999      0.500     0.154     0.235         8 4/32039/22/4\n",
      "                       unknown     0.697      1.000     0.697     0.821     22337 22337/0/9732/0\n",
      "                         water     1.000        nan       nan       nan         0 0/32069/0/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.949      0.451     0.396     0.347     32069 0/0/0/0\n",
      "Exact Match ACC : 0.33712 \n",
      "Total Records : 32069 \n",
      "Total ZXeros in True : 811 (0.025)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "16 16\n",
      "==================HOME Case : home_os_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.997      0.368     0.447     0.404        57 21/19885/26/36\n",
      "                      activity     0.999      0.632     0.800     0.706        19 12/19946/3/7\n",
      "                       battery     1.000      0.000     0.000     0.000         7 0/19959/2/7\n",
      "                        button     0.544      0.071     0.000     0.000        14 1/10853/9101/13\n",
      "              colorTemperature     1.000      0.667     0.800     0.727         6 4/19961/1/2\n",
      "                       contact     0.995      0.602     0.791     0.684       176 106/19764/28/70\n",
      "                         level     0.998      0.222     0.300     0.255        27 6/19927/14/21\n",
      "                          lock     0.996      0.771     0.260     0.388        35 27/19856/77/8\n",
      "                        motion     0.981      0.000       nan     0.000       371 0/19597/0/371\n",
      "                          ping     0.995      0.998     0.982     0.990      4842 4832/15038/88/10\n",
      "                        status     0.997      0.798     0.736     0.766       119 95/19815/34/24\n",
      "                        switch     0.999      0.714     0.750     0.732        21 15/19942/5/6\n",
      "                   temperature     0.942      0.000       nan     0.000      1168 0/18800/0/1168\n",
      "                     threeAxis     0.996      0.492     0.397     0.440        63 31/19858/47/32\n",
      "                       unknown     0.666      1.000     0.666     0.800     13305 13305/0/6663/0\n",
      "                         water     1.000        nan     0.000     0.000         0 0/19966/2/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.944      0.459     0.433     0.431     19968 0/0/0/0\n",
      "Exact Match ACC : 0.27654 \n",
      "Total Records : 19968 \n",
      "Total ZXeros in True : 452 (0.023)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "16 16\n",
      "==================HOME Case : home_os_final_reduced.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.997      0.280     0.350     0.311        25 7/9071/13/18\n",
      "                      activity     0.998      0.381     0.889     0.533        21 8/9087/1/13\n",
      "                       battery     0.999      0.000     0.000     0.000         4 0/9103/2/4\n",
      "                        button     0.549      0.000     0.000     0.000         3 0/5005/4101/3\n",
      "              colorTemperature     1.000        nan       nan       nan         0 0/9109/0/0\n",
      "                       contact     0.995      0.564     0.830     0.672        78 44/9022/9/34\n",
      "                         level     0.998      0.250     0.375     0.300        12 3/9092/5/9\n",
      "                          lock     0.995      0.545     0.250     0.343        22 12/9051/36/10\n",
      "                        motion     0.976      0.000       nan     0.000       218 0/8891/0/218\n",
      "                          ping     0.993      0.997     0.976     0.987      2237 2231/6818/54/6\n",
      "                        status     0.996      0.660     0.623     0.641        50 33/9039/20/17\n",
      "                        switch     0.999      0.500     0.500     0.500         6 3/9100/3/3\n",
      "                   temperature     0.931      0.000       nan     0.000       630 0/8479/0/630\n",
      "                     threeAxis     0.996      0.440     0.324     0.373        25 11/9061/23/14\n",
      "                       unknown     0.653      1.000     0.653     0.790      5948 5948/0/3161/0\n",
      "                         water     1.000        nan     0.000     0.000         0 0/9107/2/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.942      0.351     0.361     0.341      9109 0/0/0/0\n",
      "Exact Match ACC : 0.27380 \n",
      "Total Records : 9109 \n",
      "Total ZXeros in True : 168 (0.018)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "16 16\n",
      "==================HOME Case : home_sk_final.json =============\n",
      "                         Class  Accuracy     Recall  Precision   F Score    Count      TP/TN/FP/FN\n",
      "------------------------------------------------------------------------\n",
      "                  acceleration     0.995      0.400     0.640     0.492        40 16/6355/9/24\n",
      "                      activity     1.000        nan       nan       nan         0 0/6404/0/0\n",
      "                       battery     1.000      0.000     0.000     0.000         2 0/6401/1/2\n",
      "                        button     0.628      0.150     0.004     0.008        60 9/4014/2330/51\n",
      "              colorTemperature     1.000      0.000       nan     0.000         2 0/6402/0/2\n",
      "                       contact     0.986      0.509     0.957     0.664       175 89/6225/4/86\n",
      "                         level     0.991      0.136     0.750     0.231        66 9/6335/3/57\n",
      "                          lock     0.998        nan     0.000     0.000         0 0/6391/13/0\n",
      "                        motion     0.977      0.000       nan     0.000       145 0/6259/0/145\n",
      "                          ping     0.995      0.998     0.989     0.993      2307 2302/4071/26/5\n",
      "                        status     0.998      0.864     0.978     0.918       103 89/6299/2/14\n",
      "                        switch     0.997      0.375     0.750     0.500        24 9/6377/3/15\n",
      "                   temperature     0.968      0.000       nan     0.000       203 0/6201/0/203\n",
      "                     threeAxis     0.993      0.457     0.553     0.500        46 21/6341/17/25\n",
      "                       unknown     0.556      1.000     0.556     0.714      3558 3558/0/2846/0\n",
      "                         water     1.000        nan     0.000     0.000         0 0/6402/2/0\n",
      "------------------------------------------------------------------------\n",
      "                      AVERAGES     0.943      0.306     0.386     0.314      6404 0/0/0/0\n",
      "Exact Match ACC : 0.24391 \n",
      "Total Records : 6404 \n",
      "Total ZXeros in True : 80 (0.012)%\n",
      "Total ZXeros in Test : 0 (0.000)%\n",
      "=============================================================================\n",
      "16 16\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    makeReadable( classes=classes, confidance=0.5,data=lstm_tests[i][0],gt=lstm_tests[i][1],model=model2,path=test_names[i],x=lstm_tests[i][0])\n",
    "    \n",
    "#     lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------------- do not go any further :) ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred= model2.predict( lstm_tests[1][0])\n",
    "lstm_pred__ = np.array(list(lstm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred__ = np.array(list(lstm_pred))\n",
    "print_info( lstm_tests[1][1], lstm_pred__, classes , confidance=0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1] :\n",
    "for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [1] :\n",
    "# for i in range(len(lstm_tests)) :\n",
    "    print( \"==================HOME Case : %s =============\" % test_names[i])\n",
    "    lstm_pred= model2.predict( lstm_tests[i][0])\n",
    "    \n",
    "#     print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.7)\n",
    "    print_info( lstm_tests[i][1], lstm_pred, classes , confidance=0.992)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred =model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test)\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes, confidance=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_train, y_lstm_prossed_train, classes, confidance=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x  in lstm_pred  if  np.sum(x) > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save( \"LSTM_withSigmoid_LargeData_F%s_E%d_B%d_M%s_%r\" %\n",
    "            (\n",
    "            FoldID,\n",
    "                Epoch_count,\n",
    "                Batch_size,\n",
    "                Mapper,\n",
    "                IgnoreEmpty\n",
    "            ) \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_muhammed,y_train_muhammed, classes = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False)\n",
    "# x_test_muhammed,y_test_muhammed, classes = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model2.predict( x_lstm_prossed_test )\n",
    "print_info(y_lstm_prossed_test, lstm_pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_info(y_lstm_prossed_test, pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( y_lstm_prossed_train[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size =160\n",
    "x_lstm_prossed_train,y_lstm_prossed_train, _ = pre_process_raw( x_train, y_train , dim_size, zero_pad=True, normalize=False,classes=classes)\n",
    "x_lstm_prossed_test,y_lstm_prossed_test, _ = pre_process_raw( x_test, y_test , dim_size, zero_pad=True, normalize=False,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1]), len(y_lstm_prossed_test  ) , len([x  for x  in y_lstm_prossed_test if x[21]==1 or x[20]==1])/len(y_lstm_prossed_test  ) *1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x  in  pred if np.sum(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_lstm_prossed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_for_raun( pred   ):\n",
    "    pp = pred\n",
    "    pp[pp>=0.5] = 1\n",
    "    pp[pp<0.5] = 0\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if np.sum( do_for_raun(x) )==0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in pred if  do_for_raun(x)[20] ==1 or do_for_raun(x)[21] ==1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.save(\"../files/muhammed/x_train.json\" , x_train_muhammed)\n",
    "# np.save(\"../files/muhammed/y_train.json\", )\n",
    "\n",
    "\n",
    "# np.save( \"../files/muhammed/x_train.json\", x_train_muhammed )\n",
    "# np.save(\"../files/muhammed/y_train.json\",  y_train_muhammed )\n",
    "# np.save( \"../files/muhammed/x_test.json\",x_test_muhammed )\n",
    "# np.save( \"../files/muhammed/y_test.json\",y_test_muhammed )\n",
    "# np.save( \"../files/muhammed/classes.json\",  classes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_lstm_prossed_test) + len(x_lstm_prossed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(iot)",
   "language": "python",
   "name": "iot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
